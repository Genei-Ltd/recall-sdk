// This file is auto-generated by @hey-api/openapi-ts

export type ClientOptions = {
    baseUrl: 'https://us-east-1.recall.ai' | 'https://eu-central-1.recall.ai' | 'https://ap-northeast-1.recall.ai' | 'https://us-west-2.recall.ai' | (string & {});
};

export type ArtifactStatus = {
    code: ArtifactStatusCodeEnum;
    sub_code: string | null;
    updated_at: string;
};

/**
 * * `processing` - Processing
 * * `done` - Done
 * * `failed` - Failed
 * * `deleted` - Deleted
 */
export type ArtifactStatusCodeEnum = 'processing' | 'done' | 'failed' | 'deleted';

export type AudioMixedArtifactData = {
    readonly download_url: string | null;
};

export type AudioMixedArtifactShortcut = {
    readonly id: string;
    readonly created_at: string;
    status: ArtifactStatus;
    metadata: {
        [key: string]: string | null;
    };
    data: AudioMixedArtifactData;
    /**
     * Format of the mixed audio file
     *
     * * `mp3` - Mp3
     * * `raw` - Raw
     */
    format: Format33bEnum;
};

export type AudioOutput = {
    data: AudioOutputDataWritable;
    /**
     * Specify this parameter if you want the audio to be replayed when additional participants join the call.
     */
    replay_on_participant_join?: AudioOutputReplay;
};

export type AudioOutputData = {
    /**
     * The kind of data encoded in b64_data
     *
     * * `mp3` - mp3
     */
    kind: AudioOutputDataKindEnum;
};

/**
 * * `mp3` - mp3
 */
export type AudioOutputDataKindEnum = 'mp3';

export type AudioOutputReplay = {
    /**
     * Leading: The debounce timer will start counting down when the first participant joins.
     * Trailing: The debounce timer will start counting down when the last participant joins.
     * For example with a debounce_interval of 10, if two participants join 9 seconds apart:
     * - In `Leading` mode the audio will play 10 seconds after the 1st participant joined (1 second after the 2nd participant)
     * - In `Trailing` mode the audio will play 10 seconds after the 2nd participant joined (19 seconds after the 1st participant)
     *
     * * `leading` - leading
     * * `trailing` - trailing
     */
    debounce_mode?: DebounceModeEnum;
    /**
     * The amount of time to wait for additional participants to join before replaying the audio.
     */
    debounce_interval: number;
    /**
     * The number of seconds after which the audio will no longer replay when new participants join. This parameter is useful to prevent the bot from interrupting a meeting, if a late participant joins.
     */
    disable_after?: number;
};

export type AutomaticAudioOutput = {
    in_call_recording: AudioOutput;
};

export type AutomaticLeave = {
    /**
     * The number of seconds after which the bot will automatically leave the call, if it has not been let in from the waiting room.Note that this has a max value of 600 seconds for Google Meet and 1800 for Microsoft Teams.
     */
    waiting_room_timeout?: number;
    /**
     * The number of seconds after which the bot will automatically leave the call, if it has joined the meeting but no other participant has joined.
     */
    noone_joined_timeout?: number;
    /**
     * The number of seconds after which the bot will automatically leave the call, if there were other participants in the call who have all left. Note: The everyone_left waiter remains active and will trigger even if recording is paused, unlike other automatic leave waiters which are paused when recording is paused.
     */
    everyone_left_timeout?: AutomaticLeaveEveryoneLeft;
    /**
     * The number of seconds after which the bot will automatically leave the call, if it has joined the call and is not recording.This includes all periods when the bot is not recording, even if the bot has recorded previously.
     */
    in_call_not_recording_timeout?: number;
    /**
     * The number of seconds after which the bot will automatically leave the call, if it has joined the call and started recording it. This can be used to enforce a maximum recording time limit for a bot. There is no default value for this parameter, meaning a bot will continue to record for as long as the meeting lasts.
     */
    in_call_recording_timeout?: number;
    /**
     * The number of seconds after which the bot will automatically leave the call, if it has joined the call but has not started recording. For e.g This can occur due to bot being denied permission to record(Zoom meetings).
     */
    recording_permission_denied_timeout?: number;
    silence_detection?: AutomaticLeaveSilenceDetection;
    bot_detection?: AutomaticLeaveBotDetection;
};

export type AutomaticLeaveBotDetection = {
    using_participant_events?: AutomaticLeaveBotDetectionUsingParticipantEvents;
    using_participant_names?: AutomaticLeaveBotDetectionUsingParticipantNames;
};

export type AutomaticLeaveBotDetectionUsingParticipantEvents = {
    /**
     * The number of seconds after which the bot will automatically leave the call if all other participants(in call) are detected as bots using this heuristic. A participant is considered a bot if they have no audio or screenshare activity for the duration of the call.
     */
    timeout?: number;
    /**
     * The number of seconds(post recording start) after which the bot will start detecting other participants as bots using this heuristic. This parameter can be helpful if the meeting tends to start late, so the bot does not preemptively leave.
     */
    activate_after?: number;
};

export type AutomaticLeaveBotDetectionUsingParticipantNames = {
    /**
     * The number of seconds after which the bot will automatically leave the call if all other participants(in call) are detected as bots using this heuristic. A participant is considered a bot if their name contains any of the strings specified in the `matches` parameter(case insensitive match).
     */
    timeout: number;
    /**
     * The number of seconds(post recording start) after which the bot will start detecting other participants as bots using this heuristic. This parameter can be helpful if the meeting tends to start late, so the bot does not preemptively leave.
     */
    activate_after: number;
    /**
     * A list of strings that the bot will use to detect other bots. If a participant's name contains any of these strings(case insensitive match), they will be considered a bot. If all remaining participants in the call are detected as bots(except the bot itself), the bot will automatically leave the call.For e.g ['notetaker', 'recorder', 'assistant']
     */
    matches: Array<string>;
};

export type AutomaticLeaveEveryoneLeft = {
    /**
     * The number of seconds after which the bot will automatically leave the call, if there were other participants in the call who have all left.
     */
    timeout?: number;
    /**
     * The number of seconds after which the bot will start detecting everyone left once it has started recording the call.
     */
    activate_after?: number | null;
};

export type AutomaticLeaveSilenceDetection = {
    /**
     * The number of seconds of continuous silence after which the bot will automatically leave the call.
     */
    timeout?: number;
    /**
     * The number of seconds after which the bot will start detecting silence once it has started recording the call. This parameter can be helpful if the meeting tends to start late, so the bot does not preemptively leave.
     */
    activate_after?: number;
};

export type AutomaticVideoOutput = {
    /**
     * The video that will be automatically output when the bot is in the **in_call_recording** state.
     */
    in_call_recording?: VideoOutputWritable;
    /**
     * The video that will be automatically output when the bot is in the **in_call_not_recording** state. If not specified the bot will fallback to in_call_recording output if available.
     */
    in_call_not_recording?: VideoOutputWritable;
};

export type Bot = {
    readonly id: string;
    /**
     * Structured meeting URL metadata for the associated platform.
     */
    meeting_url: MeetingUrl;
    /**
     * The name of the bot that will be displayed in the call.
     * *(Note: Authenticated Google Meet bots will use the Google account name and this field will be ignored.)*
     */
    bot_name?: string;
    /**
     * The time at which the bot will join the call, formatted in ISO 8601. This field can only be read from scheduled bots that have not yet joined a call. Once a bot has joined a call, it's join_at will be cleared.
     */
    join_at?: string | null;
    /**
     * Configure the recording generated by the bot. Includes options for getting meeting transcript,  the layout of the recorded video, when to start recording and more.
     */
    recording_config?: BotRecordingConfig | null;
    readonly status_changes: Array<BotEvent>;
    readonly recordings: Array<BotRecordingEmbed>;
    /**
     * Settings for the bot output media.
     */
    output_media?: OutputMedia | null;
    /**
     * Settings for the bot to output video. Image should be 16:9. Recommended resolution is 640x360.
     */
    automatic_video_output?: AutomaticVideoOutput | null;
    /**
     * Settings for the bot to output audio.
     */
    automatic_audio_output?: AutomaticAudioOutput | null;
    /**
     * Settings for the bot to send chat messages.
     * *(Note: Chat functionality is only supported for Zoom, Google Meet and Microsoft Teams currently.)*
     */
    chat?: Chat | null;
    automatic_leave?: AutomaticLeave | null;
    /**
     * Configure bot variants per meeting platforms, e.g. {"zoom": "web_4_core"}.
     */
    variant?: BotVariant | null;
    /**
     * The calendar meetings associated with this bot. This field is **populated only for bots that are dispatched via Calendar V1 API integration**.
     */
    readonly calendar_meetings: Array<BotCalendarMeeting>;
    /**
     * Zoom specific parameters
     */
    zoom?: Zoom | null;
    /**
     * Google Meet specific parameters
     */
    google_meet?: GoogleMeet | null;
    slack_team: SlackTeamIntegrationMinimal | null;
    /**
     * Configure how the bot handles breakout rooms. Currently, Zoom is supported.Examples: {"mode": "join_main_room"} | {"mode": "join_specific_room", "room_id": "<uuid>"} | {"mode": "auto_accept_all_invites"} (default).
     */
    breakout_room?: BreakoutRoom | null;
    metadata?: {
        [key: string]: string | null;
    };
};

export type BotCalendarMeeting = {
    readonly id: string;
    readonly start_time: string;
    readonly end_time: string;
    calendar_user: BotCalendarUser;
};

export type BotCalendarUser = {
    readonly id: string;
    readonly external_id: string;
};

export type BotEvent = {
    readonly code: string;
    readonly message: string;
    readonly created_at: string;
    readonly sub_code: string;
};

export type BotIncludeInRecording = {
    /**
     * Whether to record audio from the bot.
     */
    audio?: boolean;
};

export type BotRecordingConfig = {
    /**
     * Specify this to capture transcript of the meeting as part of the recording. Supports:
     * - [Recall.ai Transcription](https://docs.recall.ai/docs/recallai-transcription)
     * - [3rd-party AI transcription providers](https://docs.recall.ai/docs/ai-transcription)
     * - [meeting caption transcription](https://docs.recall.ai/docs/meeting-caption-transcription)
     *
     * **[Read more about real-time transcription.](https://docs.recall.ai/docs/bot-real-time-transcription)**
     *
     * Default: `null`
     *
     *
     */
    transcript?: BotRecordingConfigTranscriptArtifact | null;
    /**
     * Add endpoints here to receive data (e.g transcript, participant events) from the recording in realtime during the meeting.
     *
     * Default: `[]`
     *
     *
     */
    realtime_endpoints?: Array<BotRecordingConfigRealtimeEndpoint>;
    /**
     * Specify the retention policy for the recording. **[Read more in this guide](https://docs.recall.ai/docs/storage-and-playback)**
     */
    retention?: BotRecordingConfigRetention | null;
    /**
     * The layout of the mixed video output.
     *
     * * `speaker_view` - speaker_view
     * * `gallery_view` - gallery_view
     * * `gallery_view_v2` - gallery_view_v2
     * * `audio_only` - audio_only
     */
    video_mixed_layout?: VideoMixedLayoutEnum;
    /**
     * Capture mixed video of the meeting as part of the recording. **This is enabled by default**.
     *
     * Default: `{}`
     *
     *
     */
    video_mixed_mp4?: BotRecordingConfigBaseArtifact | null;
    /**
     * Capture participant events as part of the recording. **This is enabled by default**.
     *
     * Default: `{}`
     *
     *
     */
    participant_events?: BotRecordingConfigBaseArtifact | null;
    /**
     * Capture meeting metadata as part of the recording. **This is enabled by default**.
     *
     * Default: `{}`
     *
     *
     */
    meeting_metadata?: BotRecordingConfigBaseArtifact | null;
    /**
     * The layout of participant video streams in mixed video output when screen is being shared. More docs: https://docs.recall.ai/docs/video-layouts
     *
     * * `hide` - hide
     * * `beside` - beside
     * * `overlap` - overlap
     */
    video_mixed_participant_video_when_screenshare?: VideoMixedParticipantVideoWhenScreenshareEnum;
    /**
     * Specify when automatic recording should start.
     * *call_join* - Bot will start recording as soon as it joins the call.
     * *participant_join* - Bot will start recording when the first participant joins the call. If a participant is already present when the bot joins, the bot will automatically start recording.
     * *participant_speak* - Bot will start recording when the first participant starts talking.
     *
     *
     * * `call_join` - call_join
     * * `participant_join` - participant_join
     * * `participant_speak` - participant_speak
     * * `manual` - manual
     */
    start_recording_on?: StartRecordingOnEnum;
    include_bot_in_recording?: BotIncludeInRecording;
    metadata?: {
        [key: string]: string | null;
    };
    /**
     * Specify this to capture mixed audio in raw format of the meeting.
     *
     * Default: `null`
     *
     *
     */
    audio_mixed_raw?: BotRecordingConfigBaseArtifact | null;
    /**
     * Specify this to capture mixed audio in the mp3 format of the meeting
     *
     * Default: `null`
     *
     *
     */
    audio_mixed_mp3?: BotRecordingConfigBaseArtifact | null;
    /**
     * Specify this to capture separate video in mp4 format for each participant in the meeting. Only supported with video_mixed_layout='gallery_view_v2'. **[Read more in this guide](https://docs.recall.ai/docs/how-to-get-separate-videos-per-participant-async)**
     *
     * Default: `null`
     *
     *
     */
    video_separate_mp4?: BotRecordingConfigBaseArtifact | null;
    /**
     * Specify this to capture separate audio in raw format for each participant in the meeting(**limited support**).  **[Read more in this guide](https://docs.recall.ai/docs/how-to-get-separate-audio-per-participant-async)**
     *
     * Default: `null`
     *
     *
     */
    audio_separate_raw?: BotRecordingConfigBaseArtifact | null;
    /**
     * Specify this to capture separate audio in mp3 format for each participant in the meeting. **[Read more in this guide](https://docs.recall.ai/docs/how-to-get-separate-audio-per-participant-async)**
     *
     * Default: `null`
     *
     *
     */
    audio_separate_mp3?: BotRecordingConfigBaseArtifact | null;
    /**
     * Specify this to capture mixed video in flv format(**required for rtmp streaming).
     *
     * Default: `null`
     *
     *
     */
    video_mixed_flv?: BotRecordingConfigBaseArtifact | null;
    /**
     * Specify this to capture separate video in png format for each participant in the meeting. Only supported with video_mixed_layout='gallery_view_v2'. **[Read more in this guide](https://docs.recall.ai/docs/how-to-get-separate-videos-per-participant-realtime)**
     *
     * Default: `null`
     *
     *
     */
    video_separate_png?: BotRecordingConfigBaseArtifact | null;
    /**
     * Specify this to capture separate video in compressed h264 for each participant in the meeting. **[Read more in this guide](https://docs.recall.ai/docs/how-to-get-separate-videos-per-participant-realtime)**
     *
     * Default: `null`
     *
     *
     */
    video_separate_h264?: BotRecordingConfigBaseArtifact | null;
};

export type BotRecordingConfigBaseArtifact = {
    metadata?: {
        [key: string]: string | null;
    };
};

export type BotRecordingConfigRealtimeEndpoint = ({
    type: 'rtmp';
} & BotRecordingConfigRealtimeEndpointBotRecordingConfigRealtimeEndpointRtmp) | ({
    type: 'websocket';
} & BotRecordingConfigRealtimeEndpointBotRecordingConfigRealtimeEndpointWebsocket) | ({
    type: 'webhook';
} & BotRecordingConfigRealtimeEndpointBotRecordingConfigRealtimeEndpointWebhook) | ({
    type: 'desktop_sdk_callback';
} & BotRecordingConfigRealtimeEndpointBotRecordingConfigRealtimeEndpointDesktopSdkCallback);

export type BotRecordingConfigRealtimeEndpointBotRecordingConfigRealtimeEndpointDesktopSdkCallback = BotRecordingConfigRealtimeEndpointShared & BotRecordingConfigRealtimeEndpointDesktopSdkCallback;

export type BotRecordingConfigRealtimeEndpointBotRecordingConfigRealtimeEndpointRtmp = BotRecordingConfigRealtimeEndpointShared & BotRecordingConfigRealtimeEndpointRtmp;

export type BotRecordingConfigRealtimeEndpointBotRecordingConfigRealtimeEndpointWebhook = BotRecordingConfigRealtimeEndpointShared & BotRecordingConfigRealtimeEndpointWebhook;

export type BotRecordingConfigRealtimeEndpointBotRecordingConfigRealtimeEndpointWebsocket = BotRecordingConfigRealtimeEndpointShared & BotRecordingConfigRealtimeEndpointWebsocket;

export type BotRecordingConfigRealtimeEndpointDesktopSdkCallback = {
    /**
     * The events to send to the realtime endpoint.
     */
    events: Array<BotRecordingConfigRealtimeEndpointDesktopSdkCallbackEventsEnum>;
};

/**
 * * `participant_events.join` - participant_events.join
 * * `participant_events.leave` - participant_events.leave
 * * `participant_events.update` - participant_events.update
 * * `participant_events.speech_on` - participant_events.speech_on
 * * `participant_events.speech_off` - participant_events.speech_off
 * * `participant_events.webcam_on` - participant_events.webcam_on
 * * `participant_events.webcam_off` - participant_events.webcam_off
 * * `participant_events.screenshare_on` - participant_events.screenshare_on
 * * `participant_events.screenshare_off` - participant_events.screenshare_off
 * * `participant_events.chat_message` - participant_events.chat_message
 * * `transcript.data` - transcript.data
 * * `transcript.partial_data` - transcript.partial_data
 * * `transcript.provider_data` - transcript.provider_data
 * * `video_separate_png.data` - video_separate_png.data
 * * `video_separate_h264.data` - video_separate_h264.data
 * * `audio_mixed_raw.data` - audio_mixed_raw.data
 */
export type BotRecordingConfigRealtimeEndpointDesktopSdkCallbackEventsEnum = 'participant_events.join' | 'participant_events.leave' | 'participant_events.update' | 'participant_events.speech_on' | 'participant_events.speech_off' | 'participant_events.webcam_on' | 'participant_events.webcam_off' | 'participant_events.screenshare_on' | 'participant_events.screenshare_off' | 'participant_events.chat_message' | 'transcript.data' | 'transcript.partial_data' | 'transcript.provider_data' | 'video_separate_png.data' | 'video_separate_h264.data' | 'audio_mixed_raw.data';

export type BotRecordingConfigRealtimeEndpointRtmp = {
    /**
     * The URL of the realtime endpoint.
     */
    url: string;
    /**
     * The events to send to the realtime endpoint.
     */
    events: Array<BotRecordingConfigRealtimeEndpointRtmpEventsEnum>;
};

/**
 * * `video_mixed_flv.data` - video_mixed_flv.data
 */
export type BotRecordingConfigRealtimeEndpointRtmpEventsEnum = 'video_mixed_flv.data';

export type BotRecordingConfigRealtimeEndpointShared = {
    metadata?: {
        [key: string]: string | null;
    };
    /**
     * The type of the realtime endpoint.
     *
     * * `rtmp` - Rtmp
     * * `websocket` - Websocket
     * * `webhook` - Webhook
     * * `desktop_sdk_callback` - Desktop Sdk Callback
     */
    type: BotRecordingConfigRealtimeEndpointSharedTypeEnum;
};

/**
 * * `rtmp` - Rtmp
 * * `websocket` - Websocket
 * * `webhook` - Webhook
 * * `desktop_sdk_callback` - Desktop Sdk Callback
 */
export type BotRecordingConfigRealtimeEndpointSharedTypeEnum = 'rtmp' | 'websocket' | 'webhook' | 'desktop_sdk_callback';

export type BotRecordingConfigRealtimeEndpointWebhook = {
    /**
     * The URL of the realtime endpoint.
     */
    url: string;
    /**
     * The events to send to the realtime endpoint.
     */
    events: Array<BotRecordingConfigRealtimeEndpointWebhookEventsEnum>;
};

/**
 * * `participant_events.join` - participant_events.join
 * * `participant_events.leave` - participant_events.leave
 * * `participant_events.update` - participant_events.update
 * * `participant_events.speech_on` - participant_events.speech_on
 * * `participant_events.speech_off` - participant_events.speech_off
 * * `participant_events.webcam_on` - participant_events.webcam_on
 * * `participant_events.webcam_off` - participant_events.webcam_off
 * * `participant_events.screenshare_on` - participant_events.screenshare_on
 * * `participant_events.screenshare_off` - participant_events.screenshare_off
 * * `participant_events.chat_message` - participant_events.chat_message
 * * `transcript.data` - transcript.data
 * * `transcript.partial_data` - transcript.partial_data
 * * `transcript.provider_data` - transcript.provider_data
 */
export type BotRecordingConfigRealtimeEndpointWebhookEventsEnum = 'participant_events.join' | 'participant_events.leave' | 'participant_events.update' | 'participant_events.speech_on' | 'participant_events.speech_off' | 'participant_events.webcam_on' | 'participant_events.webcam_off' | 'participant_events.screenshare_on' | 'participant_events.screenshare_off' | 'participant_events.chat_message' | 'transcript.data' | 'transcript.partial_data' | 'transcript.provider_data';

export type BotRecordingConfigRealtimeEndpointWebsocket = {
    /**
     * The URL of the realtime endpoint.
     */
    url: string;
    /**
     * The events to send to the realtime endpoint.
     */
    events: Array<BotRecordingConfigRealtimeEndpointWebsocketEventsEnum>;
};

/**
 * * `participant_events.join` - participant_events.join
 * * `participant_events.leave` - participant_events.leave
 * * `participant_events.update` - participant_events.update
 * * `participant_events.speech_on` - participant_events.speech_on
 * * `participant_events.speech_off` - participant_events.speech_off
 * * `participant_events.webcam_on` - participant_events.webcam_on
 * * `participant_events.webcam_off` - participant_events.webcam_off
 * * `participant_events.screenshare_on` - participant_events.screenshare_on
 * * `participant_events.screenshare_off` - participant_events.screenshare_off
 * * `participant_events.chat_message` - participant_events.chat_message
 * * `audio_mixed_raw.data` - audio_mixed_raw.data
 * * `transcript.data` - transcript.data
 * * `transcript.partial_data` - transcript.partial_data
 * * `transcript.provider_data` - transcript.provider_data
 * * `audio_separate_raw.data` - audio_separate_raw.data
 * * `video_separate_png.data` - video_separate_png.data
 * * `video_separate_h264.data` - video_separate_h264.data
 */
export type BotRecordingConfigRealtimeEndpointWebsocketEventsEnum = 'participant_events.join' | 'participant_events.leave' | 'participant_events.update' | 'participant_events.speech_on' | 'participant_events.speech_off' | 'participant_events.webcam_on' | 'participant_events.webcam_off' | 'participant_events.screenshare_on' | 'participant_events.screenshare_off' | 'participant_events.chat_message' | 'audio_mixed_raw.data' | 'transcript.data' | 'transcript.partial_data' | 'transcript.provider_data' | 'audio_separate_raw.data' | 'video_separate_png.data' | 'video_separate_h264.data';

export type BotRecordingConfigRetention = ({
    type: 'timed';
} & BotRecordingConfigRetentionBotRecordingConfigRetentionTimed) | ({
    type: 'forever';
} & BotRecordingConfigRetentionGenericObject);

export type BotRecordingConfigRetentionBotRecordingConfigRetentionTimed = BotRecordingConfigRetentionShared & BotRecordingConfigRetentionTimed;

export type BotRecordingConfigRetentionGenericObject = BotRecordingConfigRetentionShared & GenericObject;

export type BotRecordingConfigRetentionShared = {
    /**
     * The type of the retention policy.
     *
     * * `timed` - Timed
     * * `forever` - Forever
     */
    type: BotRecordingConfigRetentionSharedTypeEnum;
};

/**
 * * `timed` - Timed
 * * `forever` - Forever
 */
export type BotRecordingConfigRetentionSharedTypeEnum = 'timed' | 'forever';

export type BotRecordingConfigRetentionTimed = {
    /**
     * The number of hours to retain the recording.
     */
    hours?: number;
};

export type BotRecordingConfigTranscriptArtifact = {
    metadata?: {
        [key: string]: string | null;
    };
    /**
     * Transcription Provider
     *
     * The **[transcription method](https://docs.recall.ai//docs/bot-transcription)** to get the transcript of the meeting.
     */
    provider: BotRecordingConfigTranscriptProvider;
    diarization?: BotRecordingConfigTranscriptDiarization;
};

export type BotRecordingConfigTranscriptDiarization = {
    /**
     * Use separate audio streams(a.k.a perfect diarization) when available. This feature has limted support. **[Read more in this guide](https://docs.recall.ai/docs/perfect-diarization)**
     */
    use_separate_streams_when_available?: boolean;
};

export type BotRecordingConfigTranscriptProvider = {
    /**
     * RecallAI Real-time Transcription Settings
     *
     *  **[Read more in this guide](https://docs.recall.ai/docs/recallai-transcription)**
     */
    recallai_streaming?: RecallaiStreamingTranscription;
    /**
     * AssemblyAi async chunked transcription Settings
     *
     * Docs: https://www.assemblyai.com/docs/api-reference/transcripts/submit
     */
    assembly_ai_async_chunked?: {
        /**
         * The language of your audio file. Possible values are found in [Supported Languages](https://www.assemblyai.com/docs/concepts/supported-languages).
         * The default value is 'en_us'.
         *
         */
        language_code?: 'en' | 'en_au' | 'en_uk' | 'en_us' | 'es' | 'fr' | 'de' | 'it' | 'pt' | 'nl' | 'af' | 'sq' | 'am' | 'ar' | 'hy' | 'as' | 'az' | 'ba' | 'eu' | 'be' | 'bn' | 'bs' | 'br' | 'bg' | 'my' | 'ca' | 'zh' | 'hr' | 'cs' | 'da' | 'et' | 'fo' | 'fi' | 'gl' | 'ka' | 'el' | 'gu' | 'ht' | 'ha' | 'haw' | 'he' | 'hi' | 'hu' | 'is' | 'id' | 'ja' | 'jw' | 'kn' | 'kk' | 'km' | 'ko' | 'lo' | 'la' | 'lv' | 'ln' | 'lt' | 'lb' | 'mk' | 'mg' | 'ms' | 'ml' | 'mt' | 'mi' | 'mr' | 'mn' | 'ne' | 'no' | 'nn' | 'oc' | 'pa' | 'ps' | 'fa' | 'pl' | 'ro' | 'ru' | 'sa' | 'sr' | 'sn' | 'sd' | 'si' | 'sk' | 'sl' | 'so' | 'su' | 'sw' | 'sv' | 'tl' | 'tg' | 'ta' | 'tt' | 'te' | 'th' | 'bo' | 'tr' | 'tk' | 'uk' | 'ur' | 'uz' | 'vi' | 'cy' | 'yi' | 'yo' | string | null;
        /**
         * Enable [Automatic language detection](https://www.assemblyai.com/docs/models/speech-recognition#automatic-language-detection), either true or false.
         */
        language_detection?: boolean;
        /**
         * The confidence threshold for the automatically detected language.
         * An error will be returned if the language confidence is below this threshold.
         * Defaults to 0.
         *
         */
        language_confidence_threshold?: number;
        /**
         * The speech model to use for the transcription. When `null`, the "best" model is used.
         */
        speech_model?: 'best' | 'nano' | 'slam-1' | 'universal' | string | null;
        /**
         * Enable Automatic Punctuation, can be true or false
         */
        punctuate?: boolean;
        /**
         * Enable Text Formatting, can be true or false
         */
        format_text?: boolean;
        /**
         * Transcribe Filler Words, like "umm", in your media file; can be true or false
         */
        disfluencies?: boolean;
        /**
         * Enable [Multichannel](https://www.assemblyai.com/docs/models/speech-recognition#multichannel-transcription) transcription, can be true or false.
         */
        multichannel?: boolean;
        /**
         * Enable [Dual Channel](https://www.assemblyai.com/docs/models/speech-recognition#dual-channel-transcription) transcription, can be true or false.
         *
         * @deprecated
         */
        dual_channel?: boolean;
        /**
         * Enable Key Phrases, either true or false
         */
        auto_highlights?: boolean;
        /**
         * The point in time, in milliseconds, to begin transcribing in your media file
         */
        audio_start_from?: number;
        /**
         * The point in time, in milliseconds, to stop transcribing in your media file
         */
        audio_end_at?: number;
        /**
         * The list of custom vocabulary to boost transcription probability for
         *
         * @deprecated
         */
        word_boost?: Array<string>;
        /**
         * How much to boost specified words
         */
        boost_param?: 'low' | 'default' | 'high';
        /**
         * Filter profanity from the transcribed text, can be true or false
         */
        filter_profanity?: boolean;
        /**
         * Redact PII from the transcribed text using the Redact PII model, can be true or false
         */
        redact_pii?: boolean;
        /**
         * Generate a copy of the original media file with spoken PII "beeped" out, can be true or false. See [PII redaction](https://www.assemblyai.com/docs/models/pii-redaction) for more details.
         */
        redact_pii_audio?: boolean;
        /**
         * Controls the filetype of the audio created by redact_pii_audio. Currently supports mp3 (default) and wav. See [PII redaction](https://www.assemblyai.com/docs/models/pii-redaction) for more details.
         */
        redact_pii_audio_quality?: 'mp3' | 'wav';
        /**
         * The list of PII Redaction policies to enable. See [PII redaction](https://www.assemblyai.com/docs/models/pii-redaction) for more details.
         */
        redact_pii_policies?: Array<'account_number' | 'banking_information' | 'blood_type' | 'credit_card_cvv' | 'credit_card_expiration' | 'credit_card_number' | 'date' | 'date_interval' | 'date_of_birth' | 'drivers_license' | 'drug' | 'duration' | 'email_address' | 'event' | 'filename' | 'gender_sexuality' | 'healthcare_number' | 'injury' | 'ip_address' | 'language' | 'location' | 'marital_status' | 'medical_condition' | 'medical_process' | 'money_amount' | 'nationality' | 'number_sequence' | 'occupation' | 'organization' | 'passport_number' | 'password' | 'person_age' | 'person_name' | 'phone_number' | 'physical_attribute' | 'political_affiliation' | 'religion' | 'statistics' | 'time' | 'url' | 'us_social_security_number' | 'username' | 'vehicle_id' | 'zodiac_sign'>;
        /**
         * The replacement logic for detected PII, can be "entity_type" or "hash". See [PII redaction](https://www.assemblyai.com/docs/models/pii-redaction) for more details.
         */
        redact_pii_sub?: 'entity_name' | 'hash' | string | null;
        /**
         * Enable [Speaker diarization](https://www.assemblyai.com/docs/models/speaker-diarization), can be true or false
         */
        speaker_labels?: boolean;
        /**
         * Tells the speaker label model how many speakers it should attempt to identify, up to 10. See [Speaker diarization](https://www.assemblyai.com/docs/models/speaker-diarization) for more details.
         */
        speakers_expected?: number | null;
        /**
         * Enable [Content Moderation](https://www.assemblyai.com/docs/models/content-moderation), can be true or false
         */
        content_safety?: boolean;
        /**
         * The confidence threshold for the Content Moderation model. Values must be between 25 and 100.
         */
        content_safety_confidence?: number;
        /**
         * Enable [Topic Detection](https://www.assemblyai.com/docs/models/topic-detection), can be true or false
         */
        iab_categories?: boolean;
        /**
         * Customize how words are spelled and formatted using to and from values
         */
        custom_spelling?: Array<{
            /**
             * Words or phrases to replace
             */
            from: Array<string>;
            /**
             * Word to replace with
             */
            to: string;
        }>;
        /**
         * <Warning>`keyterms_prompt` is only supported when the `speech_model` is specified as `slam-1`</Warning>
         * Improve accuracy with up to 1000 domain-specific words or phrases (maximum 6 words per phrase).
         *
         */
        keyterms_prompt?: Array<string>;
        /**
         * This parameter does not currently have any functionality attached to it.
         *
         * @deprecated
         */
        prompt?: string;
        /**
         * Enable [Sentiment Analysis](https://www.assemblyai.com/docs/models/sentiment-analysis), can be true or false
         */
        sentiment_analysis?: boolean;
        /**
         * Enable [Auto Chapters](https://www.assemblyai.com/docs/models/auto-chapters), can be true or false
         */
        auto_chapters?: boolean;
        /**
         * Enable [Entity Detection](https://www.assemblyai.com/docs/models/entity-detection), can be true or false
         */
        entity_detection?: boolean;
        /**
         * Reject audio files that contain less than this fraction of speech.
         * Valid values are in the range [0, 1] inclusive.
         *
         */
        speech_threshold?: number | null;
        /**
         * Enable [Summarization](https://www.assemblyai.com/docs/models/summarization), can be true or false
         */
        summarization?: boolean;
        /**
         * The model to summarize the transcript
         */
        summary_model?: 'informative' | 'conversational' | 'catchy';
        /**
         * The type of summary
         */
        summary_type?: 'bullets' | 'bullets_verbose' | 'gist' | 'headline' | 'paragraph';
        /**
         * Enable custom topics, either true or false
         *
         * @deprecated
         */
        custom_topics?: boolean;
        /**
         * The list of custom topics
         */
        topics?: Array<string>;
    };
    /**
     * AssemblyAi Real-time Transcription Settings
     *
     * Docs: https://www.assemblyai.com/docs/api-reference/streaming-api/streaming-api
     */
    assembly_ai_v3_streaming?: {
        /**
         * Whether to return formatted final transcripts
         */
        format_turns?: boolean;
        /**
         * The confidence threshold (0.0 to 1.0) to use when determining if the end of a turn has been reached
         */
        end_of_turn_confidence_threshold?: unknown;
        /**
         * The minimum amount of silence in milliseconds required to detect end of turn when confident
         */
        min_end_of_turn_silence_when_confident?: unknown;
        /**
         * The maximum amount of silence in milliseconds allowed in a turn before end of turn is triggered
         */
        max_turn_silence?: unknown;
    };
    /**
     * Deepgram Real-time Transcription Settings
     *
     * Docs: https://developers.deepgram.com/reference/streaming
     */
    deepgram_streaming?: {
        /**
         * Defaults to `false`. Recognize speaker changes. Each word in the transcript will be assigned a speaker number starting at 0
         */
        diarize?: 'true' | 'false';
        /**
         * Identify and extract key entities from content in submitted audio
         */
        dictation?: 'true' | 'false';
        /**
         * Indicates how long Deepgram will wait to detect whether a speaker has finished speaking or pauses for a significant period of time. When set to a value, the streaming endpoint immediately finalizes the transcription for the processed time range and returns the transcript with a speech_final parameter set to true. Can also be set to false to disable endpointing
         */
        endpointing?: unknown;
        /**
         * Arbitrary key-value pairs that are attached to the API response for usage in downstream processing
         */
        extra?: unknown;
        /**
         * Filler Words can help transcribe interruptions in your audio, like "uh" and "um"
         */
        filler_words?: 'true' | 'false';
        /**
         * Specifies whether the streaming endpoint should provide ongoing transcription updates as more audio is received. When set to true, the endpoint sends continuous updates, meaning transcription results may evolve over time
         */
        interim_results?: 'true' | 'false';
        /**
         * Key term prompting can boost or suppress specialized terminology and brands. Only compatible with Nova-3
         */
        keyterm?: unknown;
        /**
         * Keywords can boost or suppress specialized terminology and brands
         */
        keywords?: unknown;
        /**
         * The [BCP-47 language tag](https://tools.ietf.org/html/bcp47) that hints at the primary spoken language. Depending on the Model you choose only certain languages are available
         */
        language?: 'bg' | 'ca' | 'cs' | 'da' | 'da-DK' | 'de' | 'de-CH' | 'el' | 'en' | 'en-AU' | 'en-GB' | 'en-IN' | 'en-NZ' | 'en-US' | 'es' | 'es-419' | 'es-LATAM' | 'et' | 'fi' | 'fr' | 'fr-CA' | 'hi' | 'hi-Latn' | 'hu' | 'id' | 'it' | 'ja' | 'ko' | 'ko-KR' | 'lt' | 'lv' | 'ms' | 'multi' | 'nl' | 'nl-BE' | 'no' | 'pl' | 'pt' | 'pt-BR' | 'pt-PT' | 'ro' | 'ru' | 'sk' | 'sv' | 'sv-SE' | 'taq' | 'th' | 'th-TH' | 'tr' | 'uk' | 'vi' | 'zh' | 'zh-CN' | 'zh-HK' | 'zh-Hans' | 'zh-Hant' | 'zh-TW';
        /**
         * Opts out requests from the Deepgram Model Improvement Program. Refer to our Docs for pricing impacts before setting this to true. https://dpgr.am/deepgram-mip
         */
        mip_opt_out?: unknown;
        /**
         * AI model to use for the transcription
         */
        model?: 'nova-3' | 'nova-3-general' | 'nova-3-medical' | 'nova-2' | 'nova-2-general' | 'nova-2-meeting' | 'nova-2-finance' | 'nova-2-conversationalai' | 'nova-2-voicemail' | 'nova-2-video' | 'nova-2-medical' | 'nova-2-drivethru' | 'nova-2-automotive' | 'nova' | 'nova-general' | 'nova-phonecall' | 'nova-medical' | 'enhanced' | 'enhanced-general' | 'enhanced-meeting' | 'enhanced-phonecall' | 'enhanced-finance' | 'base' | 'meeting' | 'phonecall' | 'finance' | 'conversationalai' | 'voicemail' | 'video' | 'custom';
        /**
         * Transcribe each audio channel independently
         */
        multichannel?: 'true' | 'false';
        /**
         * Convert numbers from written format to numerical format
         */
        numerals?: 'true' | 'false';
        /**
         * Profanity Filter looks for recognized profanity and converts it to the nearest recognized non-profane word or removes it from the transcript completely
         */
        profanity_filter?: 'true' | 'false';
        /**
         * Add punctuation and capitalization to the transcript
         */
        punctuate?: 'true' | 'false';
        /**
         * Redaction removes sensitive information from your transcripts
         */
        redact?: 'true' | 'false' | 'pci' | 'numbers' | 'aggressive_numbers' | 'ssn';
        /**
         * Search for terms or phrases in submitted audio and replaces them
         */
        replace?: unknown;
        /**
         * Search for terms or phrases in submitted audio
         */
        search?: unknown;
        /**
         * Apply formatting to transcript output. When set to true, additional formatting will be applied to transcripts to improve readability
         */
        smart_format?: 'true' | 'false';
        /**
         * Label your requests for the purpose of identification during usage reporting
         */
        tag?: unknown;
        /**
         * Indicates how long Deepgram will wait to send an UtteranceEnd message after a word has been transcribed. Use with interim_results
         */
        utterance_end_ms?: unknown;
        /**
         * Indicates that speech has started. You'll begin receiving Speech Started messages upon speech starting
         */
        vad_events?: 'true' | 'false';
        /**
         * Version of an AI model to use
         */
        version?: unknown;
    };
    /**
     * Gladia Real-time Transcription Settings
     *
     * Docs: https://docs.gladia.io/reference/live-audio
     */
    gladia_v2_streaming?: {
        /**
         * Custom metadata you can attach to this live transcription
         */
        custom_metadata?: {
            [key: string]: unknown;
        };
        /**
         * The model used to process the audio. "solaria-1" is used by default.
         */
        model?: 'solaria-1';
        /**
         * The endpointing duration in seconds. Endpointing is the duration of silence which will cause an utterance to be considered as finished
         */
        endpointing?: number;
        /**
         * The maximum duration in seconds without endpointing. If endpointing is not detected after this duration, current utterance will be considered as finished
         */
        maximum_duration_without_endpointing?: number;
        /**
         * Specify the language configuration
         */
        language_config?: {
            /**
             * If one language is set, it will be used for the transcription. Otherwise, language will be auto-detected by the model.
             */
            languages?: Array<'af' | 'sq' | 'am' | 'ar' | 'hy' | 'as' | 'az' | 'ba' | 'eu' | 'be' | 'bn' | 'bs' | 'br' | 'bg' | 'ca' | 'zh' | 'hr' | 'cs' | 'da' | 'nl' | 'en' | 'et' | 'fo' | 'fi' | 'fr' | 'gl' | 'ka' | 'de' | 'el' | 'gu' | 'ht' | 'ha' | 'haw' | 'he' | 'hi' | 'hu' | 'is' | 'id' | 'it' | 'ja' | 'jv' | 'kn' | 'kk' | 'km' | 'ko' | 'lo' | 'la' | 'lv' | 'ln' | 'lt' | 'lb' | 'mk' | 'mg' | 'ms' | 'ml' | 'mt' | 'mi' | 'mr' | 'mn' | 'mymr' | 'ne' | 'no' | 'nn' | 'oc' | 'ps' | 'fa' | 'pl' | 'pt' | 'pa' | 'ro' | 'ru' | 'sa' | 'sr' | 'sn' | 'sd' | 'si' | 'sk' | 'sl' | 'so' | 'es' | 'su' | 'sw' | 'sv' | 'tl' | 'tg' | 'ta' | 'tt' | 'te' | 'th' | 'bo' | 'tr' | 'tk' | 'uk' | 'ur' | 'uz' | 'vi' | 'cy' | 'yi' | 'yo' | 'jp'>;
            /**
             * If true, language will be auto-detected on each utterance. Otherwise, language will be auto-detected on first utterance and then used for the rest of the transcription. If one language is set, this option will be ignored.
             */
            code_switching?: boolean;
        };
        /**
         * Specify the pre-processing configuration
         */
        pre_processing?: {
            /**
             * If true, apply pre-processing to the audio stream to enhance the quality.
             */
            audio_enhancer?: boolean;
            /**
             * Sensitivity configuration for Speech Threshold. A value close to 1 will apply stricter thresholds, making it less likely to detect background sounds as speech.
             */
            speech_threshold?: number;
        };
        /**
         * Specify the realtime processing configuration
         */
        realtime_processing?: {
            /**
             * If true, enable custom vocabulary for the transcription.
             */
            custom_vocabulary?: boolean;
            /**
             * Custom vocabulary configuration, if `custom_vocabulary` is enabled
             */
            custom_vocabulary_config?: {
                /**
                 * Specific vocabulary list to feed the transcription model with. Each item can be a string or an object with the following properties: value, intensity, pronunciations, language.
                 */
                vocabulary: Array<{
                    /**
                     * The text used to replace in the transcription.
                     */
                    value: string;
                    /**
                     * The global intensity of the feature.
                     */
                    intensity?: number;
                    /**
                     * The pronunciations used in the transcription.
                     */
                    pronunciations?: Array<string>;
                    /**
                     * Specify the language in which it will be pronounced when sound comparison occurs. Default to transcription language.
                     */
                    language?: string;
                } | string>;
                /**
                 * Default intensity for the custom vocabulary
                 */
                default_intensity?: number;
            };
            /**
             * If true, enable custom spelling for the transcription.
             */
            custom_spelling?: boolean;
            /**
             * Custom spelling configuration, if `custom_spelling` is enabled
             */
            custom_spelling_config?: {
                /**
                 * The list of spelling applied on the audio transcription
                 */
                spelling_dictionary: {
                    [key: string]: unknown;
                };
            };
            /**
             * If true, enable translation for the transcription
             */
            translation?: boolean;
            /**
             * Translation configuration, if `translation` is enabled
             */
            translation_config?: {
                target_languages: Array<'af' | 'sq' | 'am' | 'ar' | 'hy' | 'as' | 'ast' | 'az' | 'ba' | 'eu' | 'be' | 'bn' | 'bs' | 'br' | 'bg' | 'my' | 'ca' | 'ceb' | 'zh' | 'hr' | 'cs' | 'da' | 'nl' | 'en' | 'et' | 'fo' | 'fi' | 'fr' | 'fy' | 'ff' | 'gd' | 'gl' | 'lg' | 'ka' | 'de' | 'el' | 'gu' | 'ht' | 'ha' | 'haw' | 'he' | 'hi' | 'hu' | 'is' | 'ig' | 'ilo' | 'id' | 'ga' | 'it' | 'ja' | 'jp' | 'jv' | 'kn' | 'kk' | 'km' | 'ko' | 'lo' | 'la' | 'lv' | 'ln' | 'lt' | 'lb' | 'mk' | 'mg' | 'ms' | 'ml' | 'mt' | 'mi' | 'mr' | 'mo' | 'mn' | 'mymr' | 'ne' | 'no' | 'nn' | 'oc' | 'or' | 'pa' | 'ps' | 'fa' | 'pl' | 'pt' | 'ro' | 'ru' | 'sa' | 'sr' | 'sn' | 'sd' | 'si' | 'sk' | 'sl' | 'so' | 'es' | 'su' | 'sw' | 'ss' | 'sv' | 'tl' | 'tg' | 'ta' | 'tt' | 'te' | 'th' | 'bo' | 'tn' | 'tr' | 'tk' | 'uk' | 'ur' | 'uz' | 'vi' | 'cy' | 'wo' | 'xh' | 'yi' | 'yo' | 'zu'>;
                /**
                 * Model you want the translation model to use to translate
                 */
                model?: 'base' | 'enhanced';
                /**
                 * Align translated utterances with the original ones
                 */
                match_original_utterances?: boolean;
                /**
                 * Whether to apply lipsync to the translated transcription.
                 */
                lipsync?: boolean;
                /**
                 * Enables or disables context-aware translation features that allow the model to adapt translations based on provided context.
                 */
                context_adaptation?: boolean;
                /**
                 * Context information to improve translation accuracy
                 */
                context?: string;
                /**
                 * Forces the translation to use informal language forms when available in the target language.
                 */
                informal?: boolean;
            };
            /**
             * If true, enable named entity recognition for the transcription.
             */
            named_entity_recognition?: boolean;
            /**
             * If true, enable sentiment analysis for the transcription.
             */
            sentiment_analysis?: boolean;
        };
        /**
         * Specify the post-processing configuration
         */
        post_processing?: {
            /**
             * If true, generates summarization for the whole transcription.
             */
            summarization?: boolean;
            /**
             * Summarization configuration, if `summarization` is enabled
             */
            summarization_config?: {
                /**
                 * The type of summarization to apply
                 */
                type?: 'general' | 'bullet_points' | 'concise';
            };
            /**
             * If true, generates chapters for the whole transcription.
             */
            chapterization?: boolean;
        };
        /**
         * Specify the websocket messages configuration
         */
        messages_config?: {
            /**
             * If true, final utterance will be sent to websocket.
             */
            receive_final_transcripts?: boolean;
            /**
             * If true, begin and end speech events will be sent to websocket.
             */
            receive_speech_events?: boolean;
            /**
             * If true, pre-processing events will be sent to websocket.
             */
            receive_pre_processing_events?: boolean;
            /**
             * If true, realtime processing events will be sent to websocket.
             */
            receive_realtime_processing_events?: boolean;
            /**
             * If true, post-processing events will be sent to websocket.
             */
            receive_post_processing_events?: boolean;
            /**
             * If true, acknowledgments will be sent to websocket.
             */
            receive_acknowledgments?: boolean;
            /**
             * If true, errors will be sent to websocket.
             */
            receive_errors?: boolean;
            /**
             * If true, lifecycle events will be sent to websocket.
             */
            receive_lifecycle_events?: boolean;
        };
        /**
         * If true, messages will be sent to configured url.
         */
        callback?: boolean;
        /**
         * Specify the callback configuration
         */
        callback_config?: {
            /**
             * URL on which we will do a `POST` request with configured messages
             */
            url?: string;
            /**
             * If true, final utterance will be sent to the defined callback.
             */
            receive_final_transcripts?: boolean;
            /**
             * If true, begin and end speech events will be sent to the defined callback.
             */
            receive_speech_events?: boolean;
            /**
             * If true, pre-processing events will be sent to the defined callback.
             */
            receive_pre_processing_events?: boolean;
            /**
             * If true, realtime processing events will be sent to the defined callback.
             */
            receive_realtime_processing_events?: boolean;
            /**
             * If true, post-processing events will be sent to the defined callback.
             */
            receive_post_processing_events?: boolean;
            /**
             * If true, acknowledgments will be sent to the defined callback.
             */
            receive_acknowledgments?: boolean;
            /**
             * If true, errors will be sent to the defined callback.
             */
            receive_errors?: boolean;
            /**
             * If true, lifecycle events will be sent to the defined callback.
             */
            receive_lifecycle_events?: boolean;
        };
    };
    /**
     * Rev Real-time Transcription Settings
     *
     * Docs: https://docs.rev.ai/api/streaming/requests/
     */
    rev_streaming?: {
        obscure_expletives?: boolean;
        delete_after?: string;
        'audio_options._content_type'?: string;
        'audio_options._layout'?: string;
        'audio_options._rate'?: number;
        'audio_options._format'?: string;
        'audio_options._channels'?: number;
        transcriber?: 0 | 1;
        language?: string;
        metadata?: string;
        filter_profanity?: boolean;
        remove_disfluencies?: boolean;
        detailed_partials?: boolean;
        custom_vocabulary_id?: string;
        delete_after_seconds?: number;
        max_segment_duration_seconds?: number;
        max_connection_wait_seconds?: number;
        allow_interruption?: boolean;
        enable_speaker_switch?: boolean;
        start_ts?: string;
        skip_postprocessing?: boolean;
        priority?: 0 | 1;
        user_agent?: string;
    };
    aws_transcribe_streaming?: (unknown | {
        identify_language: true;
    }) & {
        /**
         * Specify the language code that represents the language spoken. If you're unsure of the language spoken in your audio, consider using IdentifyLanguage to enable automatic language identification.
         */
        language_code?: string;
        /**
         * Specify how you want your vocabulary filter applied to your transcript. To replace words with ***, choose mask. To delete words, choose remove. To flag words without changing them, choose tag.
         */
        vocabulary_filter_method?: string;
        /**
         * Specify the name of the custom vocabulary filter that you want to use when processing your transcription. Note that vocabulary filter names are case sensitive.  If you use Amazon Transcribe in multiple Regions, the vocabulary filter must be available in Amazon Transcribe in each Region. If you include IdentifyLanguage and want to use one or more vocabulary filters with your transcription, use the VocabularyFilterNames parameter instead.
         */
        vocabulary_filter_name?: string;
        /**
         * Specify the name of the custom vocabulary that you want to use when processing your transcription. Note that vocabulary names are case sensitive. If you use Amazon Transcribe multiple Regions, the vocabulary must be available in Amazon Transcribe in each Region. If you include IdentifyLanguage and want to use one or more custom vocabularies with your transcription, use the VocabularyNames parameter instead.
         */
        vocabulary_name?: string;
        /**
         * The Amazon Web Services Region in which to use Amazon Transcribe. If you don't specify a Region, then the MediaRegion of the meeting is used. However, if Amazon Transcribe is not available in the MediaRegion, then a TranscriptFailed event is sent. Use auto to use Amazon Transcribe in a Region near the meeting’s MediaRegion. For more information, refer to Choosing a transcription Region in the Amazon Chime SDK Developer Guide.
         */
        region?: string;
        /**
         * Enables partial result stabilization for your transcription. Partial result stabilization can reduce latency in your output, but may impact accuracy.
         */
        enable_partial_results_stabilization?: boolean;
        /**
         * Specify the level of stability to use when you enable partial results stabilization (EnablePartialResultsStabilization). Low stability provides the highest accuracy. High stability transcribes faster, but with slightly lower accuracy.
         */
        partial_results_stability?: string;
        /**
         * Labels all personally identifiable information (PII) identified in your transcript. If you don't include PiiEntityTypes, all PII is identified.  You can’t set ContentIdentificationType and ContentRedactionType.
         */
        content_identification_type?: string;
        /**
         * Content redaction is performed at the segment level. If you don't include PiiEntityTypes, all PII is redacted.  You can’t set ContentRedactionType and ContentIdentificationType.
         */
        content_redaction_type?: string;
        /**
         * Specify which types of personally identifiable information (PII) you want to redact in your transcript. You can include as many types as you'd like, or you can select ALL. Values must be comma-separated and can include: ADDRESS, BANK_ACCOUNT_NUMBER, BANK_ROUTING, CREDIT_DEBIT_CVV, CREDIT_DEBIT_EXPIRY CREDIT_DEBIT_NUMBER, EMAIL,NAME, PHONE, PIN, SSN, or ALL. Note that if you include PiiEntityTypes, you must also include ContentIdentificationType or ContentRedactionType. If you include ContentRedactionType or ContentIdentificationType, but do not include PiiEntityTypes, all PII is redacted or identified.
         */
        pii_entity_types?: string;
        /**
         * Specify the name of the custom language model that you want to use when processing your transcription. Note that language model names are case sensitive. The language of the specified language model must match the language code. If the languages don't match, the custom language model isn't applied. There are no errors or warnings associated with a language mismatch. If you use Amazon Transcribe in multiple Regions, the custom language model must be available in Amazon Transcribe in each Region.
         */
        language_model_name?: string;
        /**
         * Enables automatic language identification for your transcription. If you include IdentifyLanguage, you can optionally use LanguageOptions to include a list of language codes that you think may be present in your audio stream. Including language options can improve transcription accuracy. You can also use PreferredLanguage to include a preferred language. Doing so can help Amazon Transcribe identify the language faster. You must include either LanguageCode or IdentifyLanguage. Language identification can't be combined with custom language models or redaction.
         */
        identify_language?: boolean;
        /**
         * Specify two or more language codes that represent the languages you think may be present in your media; including more than five is not recommended. If you're unsure what languages are present, do not include this parameter. Including language options can improve the accuracy of language identification. If you include LanguageOptions, you must also include IdentifyLanguage.  You can only include one language dialect per language. For example, you cannot include en-US and en-AU.
         */
        language_options?: string;
        /**
         * Specify a preferred language from the subset of languages codes you specified in LanguageOptions. You can only use this parameter if you include IdentifyLanguage and LanguageOptions.
         */
        preferred_language?: string;
        /**
         * Specify the names of the custom vocabularies that you want to use when processing your transcription. Note that vocabulary names are case sensitive. If you use Amazon Transcribe in multiple Regions, the vocabulary must be available in Amazon Transcribe in each Region. If you don't include IdentifyLanguage and want to use a custom vocabulary with your transcription, use the VocabularyName parameter instead.
         */
        vocabulary_names?: string;
        /**
         * Specify the names of the custom vocabulary filters that you want to use when processing your transcription. Note that vocabulary filter names are case sensitive. If you use Amazon Transcribe in multiple Regions, the vocabulary filter must be available in Amazon Transcribe in each Region.  If you're not including IdentifyLanguage and want to use a custom vocabulary filter with your transcription, use the VocabularyFilterName parameter instead.
         */
        vocabulary_filter_names?: string;
    };
    /**
     * Speechmatics Real-time Transcription Settings
     *
     * You must specify `language` (e.g `en`)
     *
     * Docs: https://docs.speechmatics.com/rt-api-ref#transcription-config
     */
    speechmatics_streaming?: {
        language: string;
        /**
         * Request a specialized model based on 'language' but optimized for a particular field, e.g. "finance" or "medical".
         */
        domain?: string;
        output_locale?: string;
        additional_vocab?: Array<string | {
            content: string;
            sounds_like?: Array<string>;
        }>;
        diarization?: 'none' | 'speaker';
        max_delay?: number;
        max_delay_mode?: 'flexible' | 'fixed';
        speaker_diarization_config?: {
            max_speakers?: number;
            prefer_current_speaker?: boolean;
            speaker_sensitivity?: number;
        };
        audio_filtering_config?: {
            volume_threshold?: number;
        };
        transcript_filtering_config?: {
            remove_disfluencies?: boolean;
            replacements?: Array<{
                from: string;
                to: string;
            }>;
        };
        enable_partials?: boolean;
        enable_entities?: boolean;
        operating_point?: 'standard' | 'enhanced';
        punctuation_overrides?: {
            /**
             * The punctuation marks which the client is prepared to accept in transcription output, or the special value 'all' (the default). Unsupported marks are ignored. This value is used to guide the transcription process.
             */
            permitted_marks?: Array<string>;
            /**
             * Ranges between zero and one. Higher values will produce more punctuation. The default is 0.5.
             */
            sensitivity?: number;
        };
        /**
         * This mode will detect when a speaker has stopped talking. The end_of_utterance_silence_trigger is the time in seconds after which the server will assume that the speaker has finished speaking, and will emit an EndOfUtterance message. A value of 0 disables the feature.
         */
        conversation_config?: {
            end_of_utterance_silence_trigger?: number;
        };
    };
    /**
     * Closed captions from the meeting platform
     *
     * Captures native meeting platform's built-in closed captions. **[Pros, cons, and supported platforms listed in the following guide.](https://docs.recall.ai/docs/meeting-caption-transcription)**
     */
    meeting_captions?: {
        [key: string]: unknown;
    };
};

export type BotRecordingEmbed = {
    readonly id: string;
    readonly created_at: string;
    started_at?: string | null;
    completed_at?: string | null;
    expires_at?: string | null;
    status: RecordingStatus | null;
    media_shortcuts: RecordingShortcuts | null;
    metadata?: {
        [key: string]: string | null;
    } | null;
};

export type BotVariant = {
    /**
     * Choose the bot variant to use for Zoom meetings.
     *
     * * `web` - web
     * * `web_4_core` - web_4_core
     * * `web_gpu` - web_gpu
     * * `native` - native
     */
    zoom?: ZoomEnum;
    /**
     * Choose the bot variant to use for Google Meet meetings.
     *
     * * `web` - web
     * * `web_4_core` - web_4_core
     * * `web_gpu` - web_gpu
     */
    google_meet?: WebexEnum;
    /**
     * Choose the bot variant to use for Microsoft Teams meetings.
     *
     * * `web` - web
     * * `web_4_core` - web_4_core
     * * `web_gpu` - web_gpu
     */
    microsoft_teams?: WebexEnum;
    /**
     * Choose the bot variant to use for Webex meetings.
     *
     * * `web` - web
     * * `web_4_core` - web_4_core
     * * `web_gpu` - web_gpu
     */
    webex?: WebexEnum;
};

export type BreakoutRoom = ({
    mode: 'join_main_room';
} & JoinMainRoom) | ({
    mode: 'join_specific_room';
} & JoinSpecificRoom) | ({
    mode: 'auto_accept_all_invites';
} & AutoAcceptAllInvites);

export type Chat = {
    on_bot_join?: ChatOnBotJoin;
    on_participant_join?: ChatOnParticipantJoin;
};

export type ChatOnBotJoin = {
    send_to: SendToEnum;
    message: string;
    /**
     * Pin message after sending. Only Google Meet and Microsoft Teams are supported at this time
     */
    pin?: boolean;
};

export type ChatOnParticipantJoin = {
    message: string;
    exclude_host: boolean;
};

/**
 * * `leading` - leading
 * * `trailing` - trailing
 */
export type DebounceModeEnum = 'leading' | 'trailing';

/**
 * * `mp3` - Mp3
 * * `raw` - Raw
 */
export type Format33bEnum = 'mp3' | 'raw';

/**
 * * `mp4` - Mp4
 */
export type Format3B3Enum = 'mp4';

/**
 * Generic object
 */
export type GenericObject = {
    [key: string]: unknown;
};

export type GoogleMeet = {
    /**
     * The ID of the google login group to use for this meeting.
     */
    google_login_group_id?: string | null;
};

/**
 * * `auto` - auto
 * * `bg` - bg
 * * `ca` - ca
 * * `cs` - cs
 * * `da` - da
 * * `de` - de
 * * `el` - el
 * * `en` - en
 * * `en_au` - en_au
 * * `en_uk` - en_uk
 * * `en_us` - en_us
 * * `es` - es
 * * `et` - et
 * * `fi` - fi
 * * `fr` - fr
 * * `hi` - hi
 * * `hu` - hu
 * * `id` - id
 * * `it` - it
 * * `ja` - ja
 * * `ko` - ko
 * * `lt` - lt
 * * `lv` - lv
 * * `ms` - ms
 * * `nl` - nl
 * * `no` - no
 * * `pl` - pl
 * * `pt` - pt
 * * `ro` - ro
 * * `ru` - ru
 * * `sk` - sk
 * * `sv` - sv
 * * `th` - th
 * * `tr` - tr
 * * `uk` - uk
 * * `vi` - vi
 * * `zh` - zh
 */
export type LanguageCodeEnum = 'auto' | 'bg' | 'ca' | 'cs' | 'da' | 'de' | 'el' | 'en' | 'en_au' | 'en_uk' | 'en_us' | 'es' | 'et' | 'fi' | 'fr' | 'hi' | 'hu' | 'id' | 'it' | 'ja' | 'ko' | 'lt' | 'lv' | 'ms' | 'nl' | 'no' | 'pl' | 'pt' | 'ro' | 'ru' | 'sk' | 'sv' | 'th' | 'tr' | 'uk' | 'vi' | 'zh';

export type MeetingMetadataArtifactData = {
    readonly title: string | null;
    zoom: MeetingMetadataArtifactDataZoom | null;
};

export type MeetingMetadataArtifactDataZoom = {
    meeting_uuid: string | null;
};

export type MeetingMetadataArtifactShortcut = {
    readonly id: string;
    readonly created_at: string;
    status: ArtifactStatus;
    metadata: {
        [key: string]: string | null;
    };
    data: MeetingMetadataArtifactData;
};

export type OutputMedia = {
    camera?: OutputMediaConfig;
    screenshare?: OutputMediaConfig;
};

export type OutputMediaConfig = {
    kind: 'webpage';
} & OutputMediaWebpage;

export type OutputMediaWebpage = {
    kind: OutputMediaWebpageKindEnum;
    config: OutputMediaWebpageConfig;
};

export type OutputMediaWebpageConfig = {
    /**
     * URL for the webpage
     */
    url: string;
};

/**
 * * `webpage` - webpage
 */
export type OutputMediaWebpageKindEnum = 'webpage';

export type PaginatedBotList = {
    count?: number;
    next?: string | null;
    previous?: string | null;
    results?: Array<Bot>;
};

export type ParticipantEventsArtifactData = {
    /**
     * Download all participant events for the recording. **[See response format here](https://docs.recall.ai/docs/download-schemas#json-participant-event-download-url)**
     */
    readonly participant_events_download_url: string | null;
    /**
     * Download speaker timeline for the recording. **[See response format here](https://docs.recall.ai/docs/download-schemas#json-speaker-timeline-download-url)**
     */
    readonly speaker_timeline_download_url: string | null;
    /**
     * Download all participants for the recording. **[See response format here](https://docs.recall.ai/docs/download-schemas#json-participant-download-url)**
     */
    readonly participants_download_url: string | null;
};

export type ParticipantEventsArtifactShortcut = {
    readonly id: string;
    readonly created_at: string;
    status: ArtifactStatus;
    metadata: {
        [key: string]: string | null;
    };
    data: ParticipantEventsArtifactData;
};

export type RecallaiSpellingEntry = {
    /**
     * Find any of these items in the source transcript
     */
    find: Array<string>;
    /**
     * Replace found matches with this exact string (will not change case)
     */
    replace: string;
};

export type RecallaiStreamingTranscription = {
    /**
     * Must be `en` in low latency mode. Docs: https://docs.recall.ai/docs/recallai-transcription
     *
     * * `auto` - auto
     * * `bg` - bg
     * * `ca` - ca
     * * `cs` - cs
     * * `da` - da
     * * `de` - de
     * * `el` - el
     * * `en` - en
     * * `en_au` - en_au
     * * `en_uk` - en_uk
     * * `en_us` - en_us
     * * `es` - es
     * * `et` - et
     * * `fi` - fi
     * * `fr` - fr
     * * `hi` - hi
     * * `hu` - hu
     * * `id` - id
     * * `it` - it
     * * `ja` - ja
     * * `ko` - ko
     * * `lt` - lt
     * * `lv` - lv
     * * `ms` - ms
     * * `nl` - nl
     * * `no` - no
     * * `pl` - pl
     * * `pt` - pt
     * * `ro` - ro
     * * `ru` - ru
     * * `sk` - sk
     * * `sv` - sv
     * * `th` - th
     * * `tr` - tr
     * * `uk` - uk
     * * `vi` - vi
     * * `zh` - zh
     */
    language_code?: LanguageCodeEnum;
    /**
     * List of text strings to find/replace in the transcript.
     */
    spelling?: Array<RecallaiSpellingEntry>;
    /**
     * Increases the chances that these terms appear in the transcript over some sound-alikes.
     */
    key_terms?: Array<string>;
    filter_profanity?: boolean;
    /**
     * The mode of the transcription algorithm. If you just want the transcript very quickly after the call is over, use `prioritize_accuracy`. If you need the words on realtime endpoints within seconds of utterance, use `prioritize_low_latency`, with the caveat that most features are unsupported in low latency mode.
     *
     * * `prioritize_low_latency` - prioritize_low_latency
     * * `prioritize_accuracy` - prioritize_accuracy
     */
    mode?: RecallaiStreamingTranscriptionModeEnum;
};

/**
 * * `prioritize_low_latency` - prioritize_low_latency
 * * `prioritize_accuracy` - prioritize_accuracy
 */
export type RecallaiStreamingTranscriptionModeEnum = 'prioritize_low_latency' | 'prioritize_accuracy';

export type RecordingShortcuts = {
    video_mixed: VideoMixedArtifactShortcut | null;
    transcript: TranscriptArtifactShortcut | null;
    participant_events: ParticipantEventsArtifactShortcut | null;
    meeting_metadata: MeetingMetadataArtifactShortcut | null;
    audio_mixed: AudioMixedArtifactShortcut | null;
};

export type RecordingStatus = {
    code: RecordingStatusCodeEnum;
    sub_code: string | null;
    updated_at: string;
};

/**
 * * `processing` - Processing
 * * `paused` - Paused
 * * `done` - Done
 * * `failed` - Failed
 * * `deleted` - Deleted
 */
export type RecordingStatusCodeEnum = 'processing' | 'paused' | 'done' | 'failed' | 'deleted';

/**
 * * `host` - host
 * * `everyone` - everyone
 * * `everyone_except_host` - everyone_except_host
 */
export type SendToEnum = 'host' | 'everyone' | 'everyone_except_host';

export type SlackTeamIntegrationMinimal = {
    readonly id: string;
    metadata: {
        [key: string]: string | null;
    };
};

/**
 * * `call_join` - call_join
 * * `participant_join` - participant_join
 * * `participant_speak` - participant_speak
 * * `manual` - manual
 */
export type StartRecordingOnEnum = 'call_join' | 'participant_join' | 'participant_speak' | 'manual';

export type TranscriptArtifactData = {
    /**
     * Download transcript for the recording. **[See response format here](https://docs.recall.ai/docs/download-schemas#json-transcript-download-url)**
     */
    readonly download_url: string | null;
    /**
     * Download raw transcription data received from the provider for the recording. **[See response format here](https://docs.recall.ai/docs/download-schemas#json-transcript-provider-data-download-url)**
     */
    readonly provider_data_download_url: string | null;
};

export type TranscriptArtifactDiarization = {
    readonly use_separate_streams_when_available: boolean;
};

export type TranscriptArtifactProvider = {
    /**
     * The parameters for creating a transcript
     */
    readonly assembly_ai_async: {
        /**
         * The language of your audio file. Possible values are found in [Supported Languages](https://www.assemblyai.com/docs/concepts/supported-languages).
         * The default value is 'en_us'.
         *
         */
        language_code?: 'en' | 'en_au' | 'en_uk' | 'en_us' | 'es' | 'fr' | 'de' | 'it' | 'pt' | 'nl' | 'af' | 'sq' | 'am' | 'ar' | 'hy' | 'as' | 'az' | 'ba' | 'eu' | 'be' | 'bn' | 'bs' | 'br' | 'bg' | 'my' | 'ca' | 'zh' | 'hr' | 'cs' | 'da' | 'et' | 'fo' | 'fi' | 'gl' | 'ka' | 'el' | 'gu' | 'ht' | 'ha' | 'haw' | 'he' | 'hi' | 'hu' | 'is' | 'id' | 'ja' | 'jw' | 'kn' | 'kk' | 'km' | 'ko' | 'lo' | 'la' | 'lv' | 'ln' | 'lt' | 'lb' | 'mk' | 'mg' | 'ms' | 'ml' | 'mt' | 'mi' | 'mr' | 'mn' | 'ne' | 'no' | 'nn' | 'oc' | 'pa' | 'ps' | 'fa' | 'pl' | 'ro' | 'ru' | 'sa' | 'sr' | 'sn' | 'sd' | 'si' | 'sk' | 'sl' | 'so' | 'su' | 'sw' | 'sv' | 'tl' | 'tg' | 'ta' | 'tt' | 'te' | 'th' | 'bo' | 'tr' | 'tk' | 'uk' | 'ur' | 'uz' | 'vi' | 'cy' | 'yi' | 'yo' | string | null;
        /**
         * Enable [Automatic language detection](https://www.assemblyai.com/docs/models/speech-recognition#automatic-language-detection), either true or false.
         */
        language_detection?: boolean;
        /**
         * The confidence threshold for the automatically detected language.
         * An error will be returned if the language confidence is below this threshold.
         * Defaults to 0.
         *
         */
        language_confidence_threshold?: number;
        /**
         * The speech model to use for the transcription. When `null`, the "best" model is used.
         */
        speech_model?: 'best' | 'nano' | 'slam-1' | 'universal' | string | null;
        /**
         * Enable Automatic Punctuation, can be true or false
         */
        punctuate?: boolean;
        /**
         * Enable Text Formatting, can be true or false
         */
        format_text?: boolean;
        /**
         * Transcribe Filler Words, like "umm", in your media file; can be true or false
         */
        disfluencies?: boolean;
        /**
         * Enable [Multichannel](https://www.assemblyai.com/docs/models/speech-recognition#multichannel-transcription) transcription, can be true or false.
         */
        multichannel?: boolean;
        /**
         * Enable [Dual Channel](https://www.assemblyai.com/docs/models/speech-recognition#dual-channel-transcription) transcription, can be true or false.
         *
         * @deprecated
         */
        dual_channel?: boolean;
        /**
         * The URL to which we send webhook requests.
         * We sends two different types of webhook requests.
         * One request when a transcript is completed or failed, and one request when the redacted audio is ready if redact_pii_audio is enabled.
         *
         */
        webhook_url?: string;
        /**
         * The header name to be sent with the transcript completed or failed webhook requests
         */
        webhook_auth_header_name?: string | null;
        /**
         * The header value to send back with the transcript completed or failed webhook requests for added security
         */
        webhook_auth_header_value?: string | null;
        /**
         * Enable Key Phrases, either true or false
         */
        auto_highlights?: boolean;
        /**
         * The point in time, in milliseconds, to begin transcribing in your media file
         */
        audio_start_from?: number;
        /**
         * The point in time, in milliseconds, to stop transcribing in your media file
         */
        audio_end_at?: number;
        /**
         * The list of custom vocabulary to boost transcription probability for
         *
         * @deprecated
         */
        word_boost?: Array<string>;
        /**
         * How much to boost specified words
         */
        boost_param?: 'low' | 'default' | 'high';
        /**
         * Filter profanity from the transcribed text, can be true or false
         */
        filter_profanity?: boolean;
        /**
         * Redact PII from the transcribed text using the Redact PII model, can be true or false
         */
        redact_pii?: boolean;
        /**
         * Generate a copy of the original media file with spoken PII "beeped" out, can be true or false. See [PII redaction](https://www.assemblyai.com/docs/models/pii-redaction) for more details.
         */
        redact_pii_audio?: boolean;
        /**
         * Controls the filetype of the audio created by redact_pii_audio. Currently supports mp3 (default) and wav. See [PII redaction](https://www.assemblyai.com/docs/models/pii-redaction) for more details.
         */
        redact_pii_audio_quality?: 'mp3' | 'wav';
        /**
         * The list of PII Redaction policies to enable. See [PII redaction](https://www.assemblyai.com/docs/models/pii-redaction) for more details.
         */
        redact_pii_policies?: Array<'account_number' | 'banking_information' | 'blood_type' | 'credit_card_cvv' | 'credit_card_expiration' | 'credit_card_number' | 'date' | 'date_interval' | 'date_of_birth' | 'drivers_license' | 'drug' | 'duration' | 'email_address' | 'event' | 'filename' | 'gender_sexuality' | 'healthcare_number' | 'injury' | 'ip_address' | 'language' | 'location' | 'marital_status' | 'medical_condition' | 'medical_process' | 'money_amount' | 'nationality' | 'number_sequence' | 'occupation' | 'organization' | 'passport_number' | 'password' | 'person_age' | 'person_name' | 'phone_number' | 'physical_attribute' | 'political_affiliation' | 'religion' | 'statistics' | 'time' | 'url' | 'us_social_security_number' | 'username' | 'vehicle_id' | 'zodiac_sign'>;
        /**
         * The replacement logic for detected PII, can be "entity_type" or "hash". See [PII redaction](https://www.assemblyai.com/docs/models/pii-redaction) for more details.
         */
        redact_pii_sub?: 'entity_name' | 'hash' | string | null;
        /**
         * Enable [Speaker diarization](https://www.assemblyai.com/docs/models/speaker-diarization), can be true or false
         */
        speaker_labels?: boolean;
        /**
         * Tells the speaker label model how many speakers it should attempt to identify, up to 10. See [Speaker diarization](https://www.assemblyai.com/docs/models/speaker-diarization) for more details.
         */
        speakers_expected?: number | null;
        /**
         * Enable [Content Moderation](https://www.assemblyai.com/docs/models/content-moderation), can be true or false
         */
        content_safety?: boolean;
        /**
         * The confidence threshold for the Content Moderation model. Values must be between 25 and 100.
         */
        content_safety_confidence?: number;
        /**
         * Enable [Topic Detection](https://www.assemblyai.com/docs/models/topic-detection), can be true or false
         */
        iab_categories?: boolean;
        /**
         * Customize how words are spelled and formatted using to and from values
         */
        custom_spelling?: Array<{
            /**
             * Words or phrases to replace
             */
            from: Array<string>;
            /**
             * Word to replace with
             */
            to: string;
        }>;
        /**
         * <Warning>`keyterms_prompt` is only supported when the `speech_model` is specified as `slam-1`</Warning>
         * Improve accuracy with up to 1000 domain-specific words or phrases (maximum 6 words per phrase).
         *
         */
        keyterms_prompt?: Array<string>;
        /**
         * This parameter does not currently have any functionality attached to it.
         *
         * @deprecated
         */
        prompt?: string;
        /**
         * Enable [Sentiment Analysis](https://www.assemblyai.com/docs/models/sentiment-analysis), can be true or false
         */
        sentiment_analysis?: boolean;
        /**
         * Enable [Auto Chapters](https://www.assemblyai.com/docs/models/auto-chapters), can be true or false
         */
        auto_chapters?: boolean;
        /**
         * Enable [Entity Detection](https://www.assemblyai.com/docs/models/entity-detection), can be true or false
         */
        entity_detection?: boolean;
        /**
         * Reject audio files that contain less than this fraction of speech.
         * Valid values are in the range [0, 1] inclusive.
         *
         */
        speech_threshold?: number | null;
        /**
         * Enable [Summarization](https://www.assemblyai.com/docs/models/summarization), can be true or false
         */
        summarization?: boolean;
        /**
         * The model to summarize the transcript
         */
        summary_model?: 'informative' | 'conversational' | 'catchy';
        /**
         * The type of summary
         */
        summary_type?: 'bullets' | 'bullets_verbose' | 'gist' | 'headline' | 'paragraph';
        /**
         * Enable custom topics, either true or false
         *
         * @deprecated
         */
        custom_topics?: boolean;
        /**
         * The list of custom topics
         */
        topics?: Array<string>;
    } | null;
    readonly deepgram_async: {
        callback?: string;
        callback_method?: 'POST' | 'PUT';
        custom_topic?: string | Array<string>;
        custom_topic_mode?: 'extended' | 'strict';
        custom_intent?: string | Array<string>;
        custom_intent_mode?: 'extended' | 'strict';
        detect_entities?: boolean;
        detect_language?: boolean | Array<string>;
        diarize?: boolean;
        dictation?: boolean;
        encoding?: 'linear16' | 'flac' | 'mulaw' | 'amr-nb' | 'amr-wb' | 'opus' | 'speex' | 'g729';
        extra?: string | Array<string>;
        filler_words?: boolean;
        intents?: boolean;
        keyterm?: Array<string>;
        keywords?: string | Array<string>;
        language?: 'bg' | 'ca' | 'zh' | 'zh-CN' | 'zh-TW' | 'zh-HK' | 'zh-Hans' | 'zh-Hant' | 'cs' | 'da' | 'da-DK' | 'nl' | 'nl-BE' | 'en' | 'en-US' | 'en-AU' | 'en-GB' | 'en-NZ' | 'en-IN' | 'et' | 'fi' | 'fr' | 'fr-CA' | 'de' | 'de-CH' | 'el' | 'hi' | 'hi-Latn' | 'hu' | 'id' | 'it' | 'ja' | 'ko' | 'ko-KR' | 'lv' | 'lt' | 'ms' | 'multi' | 'no' | 'pl' | 'pt' | 'pt-BR' | 'pt-PT' | 'ro' | 'ru' | 'sk' | 'es' | 'es-419' | 'es-LATAM' | 'sv' | 'sv-SE' | 'taq' | 'th' | 'th-TH' | 'tr' | 'uk' | 'vi';
        measurements?: boolean;
        mip_opt_out?: boolean;
        model?: 'nova-3' | 'nova-3-general' | 'nova-3-medical' | 'nova-2' | 'nova-2-general' | 'nova-2-meeting' | 'nova-2-finance' | 'nova-2-conversationalai' | 'nova-2-voicemail' | 'nova-2-video' | 'nova-2-medical' | 'nova-2-drivethru' | 'nova-2-automotive' | 'nova' | 'nova-general' | 'nova-phonecall' | 'nova-medical' | 'enhanced' | 'enhanced-general' | 'enhanced-meeting' | 'enhanced-phonecall' | 'enhanced-finance' | 'base' | 'meeting' | 'phonecall' | 'finance' | 'conversationalai' | 'voicemail' | 'video' | string;
        multichannel?: boolean;
        numerals?: boolean;
        paragraphs?: boolean;
        profanity_filter?: boolean;
        punctuate?: boolean;
        redact?: string | Array<'pci' | 'pii' | 'numbers'>;
        replace?: string | Array<string>;
        search?: string | Array<string>;
        sentiment?: boolean;
        smart_format?: boolean;
        summarize?: 'v1' | 'v2' | 'true' | 'false';
        tag?: string | Array<string>;
        topics?: boolean;
        utterances?: boolean;
        utt_split?: number;
        version?: 'latest' | string;
    } | null;
    readonly gladia_v2_async: {
        /**
         * **[Deprecated]** Context to feed the transcription model with for possible better accuracy
         *
         * @deprecated
         */
        context_prompt?: string;
        /**
         * **[Beta]** Can be either boolean to enable custom_vocabulary for this audio or an array with specific vocabulary list to feed the transcription model with
         */
        custom_vocabulary?: boolean;
        /**
         * **[Beta]** Custom vocabulary configuration, if `custom_vocabulary` is enabled
         */
        custom_vocabulary_config?: {
            /**
             * Specific vocabulary list to feed the transcription model with. Each item can be a string or an object with the following properties: value, intensity, pronunciations, language.
             */
            vocabulary: Array<{
                /**
                 * The text used to replace in the transcription.
                 */
                value: string;
                /**
                 * The global intensity of the feature.
                 */
                intensity?: number;
                /**
                 * The pronunciations used in the transcription.
                 */
                pronunciations?: Array<string>;
                /**
                 * Specify the language in which it will be pronounced when sound comparison occurs. Default to transcription language.
                 */
                language?: string;
            } | string>;
            /**
             * Default intensity for the custom vocabulary
             */
            default_intensity?: number;
        };
        /**
         * **[Deprecated]** Use `language_config` instead. Detect the language from the given audio
         *
         * @deprecated
         */
        detect_language?: boolean;
        /**
         * **[Deprecated]** Use `language_config` instead.Detect multiple languages in the given audio
         *
         * @deprecated
         */
        enable_code_switching?: boolean;
        /**
         * **[Deprecated]** Use `language_config` instead. Specify the configuration for code switching
         *
         * @deprecated
         */
        code_switching_config?: {
            /**
             * Specify the languages you want to use when detecting multiple languages
             */
            languages?: Array<'af' | 'sq' | 'am' | 'ar' | 'hy' | 'as' | 'az' | 'ba' | 'eu' | 'be' | 'bn' | 'bs' | 'br' | 'bg' | 'ca' | 'zh' | 'hr' | 'cs' | 'da' | 'nl' | 'en' | 'et' | 'fo' | 'fi' | 'fr' | 'gl' | 'ka' | 'de' | 'el' | 'gu' | 'ht' | 'ha' | 'haw' | 'he' | 'hi' | 'hu' | 'is' | 'id' | 'it' | 'ja' | 'jv' | 'kn' | 'kk' | 'km' | 'ko' | 'lo' | 'la' | 'lv' | 'ln' | 'lt' | 'lb' | 'mk' | 'mg' | 'ms' | 'ml' | 'mt' | 'mi' | 'mr' | 'mn' | 'mymr' | 'ne' | 'no' | 'nn' | 'oc' | 'ps' | 'fa' | 'pl' | 'pt' | 'pa' | 'ro' | 'ru' | 'sa' | 'sr' | 'sn' | 'sd' | 'si' | 'sk' | 'sl' | 'so' | 'es' | 'su' | 'sw' | 'sv' | 'tl' | 'tg' | 'ta' | 'tt' | 'te' | 'th' | 'bo' | 'tr' | 'tk' | 'uk' | 'ur' | 'uz' | 'vi' | 'cy' | 'yi' | 'yo' | 'jp'>;
        };
        /**
         * **[Deprecated]** Use `language_config` instead. Set the spoken language for the given audio (ISO 639 standard)
         *
         * @deprecated
         */
        language?: 'af' | 'sq' | 'am' | 'ar' | 'hy' | 'as' | 'az' | 'ba' | 'eu' | 'be' | 'bn' | 'bs' | 'br' | 'bg' | 'ca' | 'zh' | 'hr' | 'cs' | 'da' | 'nl' | 'en' | 'et' | 'fo' | 'fi' | 'fr' | 'gl' | 'ka' | 'de' | 'el' | 'gu' | 'ht' | 'ha' | 'haw' | 'he' | 'hi' | 'hu' | 'is' | 'id' | 'it' | 'ja' | 'jv' | 'kn' | 'kk' | 'km' | 'ko' | 'lo' | 'la' | 'lv' | 'ln' | 'lt' | 'lb' | 'mk' | 'mg' | 'ms' | 'ml' | 'mt' | 'mi' | 'mr' | 'mn' | 'mymr' | 'ne' | 'no' | 'nn' | 'oc' | 'ps' | 'fa' | 'pl' | 'pt' | 'pa' | 'ro' | 'ru' | 'sa' | 'sr' | 'sn' | 'sd' | 'si' | 'sk' | 'sl' | 'so' | 'es' | 'su' | 'sw' | 'sv' | 'tl' | 'tg' | 'ta' | 'tt' | 'te' | 'th' | 'bo' | 'tr' | 'tk' | 'uk' | 'ur' | 'uz' | 'vi' | 'cy' | 'yi' | 'yo' | 'jp';
        /**
         * **[Deprecated]** Use `callback`/`callback_config` instead. Callback URL we will do a `POST` request to with the result of the transcription
         *
         * @deprecated
         */
        callback_url?: string;
        /**
         * Enable callback for this transcription. If true, the `callback_config` property will be used to customize the callback behaviour
         */
        callback?: boolean;
        /**
         * Customize the callback behaviour (url and http method)
         */
        callback_config?: {
            /**
             * The URL to be called with the result of the transcription
             */
            url: string;
            /**
             * The HTTP method to be used. Allowed values are `POST` or `PUT` (default: `POST`)
             */
            method?: 'POST' | 'PUT';
        };
        /**
         * Enable subtitles generation for this transcription
         */
        subtitles?: boolean;
        /**
         * Configuration for subtitles generation if `subtitles` is enabled
         */
        subtitles_config?: {
            formats?: Array<'srt' | 'vtt'>;
            /**
             * Minimum duration of a subtitle in seconds
             */
            minimum_duration?: number;
            /**
             * Maximum duration of a subtitle in seconds
             */
            maximum_duration?: number;
            /**
             * Maximum number of characters per row in a subtitle
             */
            maximum_characters_per_row?: number;
            /**
             * Maximum number of rows per caption
             */
            maximum_rows_per_caption?: number;
            /**
             * Style of the subtitles. Compliance mode refers to : https://loc.gov/preservation/digital/formats//fdd/fdd000569.shtml#:~:text=SRT%20files%20are%20basic%20text,alongside%2C%20example%3A%20%22MyVideo123
             */
            style?: 'default' | 'compliance';
        };
        /**
         * Enable speaker recognition (diarization) for this audio
         */
        diarization?: boolean;
        /**
         * Speaker recognition configuration, if `diarization` is enabled
         */
        diarization_config?: {
            /**
             * Exact number of speakers in the audio
             */
            number_of_speakers?: number;
            /**
             * Minimum number of speakers in the audio
             */
            min_speakers?: number;
            /**
             * Maximum number of speakers in the audio
             */
            max_speakers?: number;
            /**
             * **[Alpha]** Use enhanced diarization for this audio
             */
            enhanced?: boolean;
        };
        /**
         * **[Beta]** Enable translation for this audio
         */
        translation?: boolean;
        /**
         * **[Beta]** Translation configuration, if `translation` is enabled
         */
        translation_config?: {
            target_languages: Array<'af' | 'sq' | 'am' | 'ar' | 'hy' | 'as' | 'ast' | 'az' | 'ba' | 'eu' | 'be' | 'bn' | 'bs' | 'br' | 'bg' | 'my' | 'ca' | 'ceb' | 'zh' | 'hr' | 'cs' | 'da' | 'nl' | 'en' | 'et' | 'fo' | 'fi' | 'fr' | 'fy' | 'ff' | 'gd' | 'gl' | 'lg' | 'ka' | 'de' | 'el' | 'gu' | 'ht' | 'ha' | 'haw' | 'he' | 'hi' | 'hu' | 'is' | 'ig' | 'ilo' | 'id' | 'ga' | 'it' | 'ja' | 'jp' | 'jv' | 'kn' | 'kk' | 'km' | 'ko' | 'lo' | 'la' | 'lv' | 'ln' | 'lt' | 'lb' | 'mk' | 'mg' | 'ms' | 'ml' | 'mt' | 'mi' | 'mr' | 'mo' | 'mn' | 'mymr' | 'ne' | 'no' | 'nn' | 'oc' | 'or' | 'pa' | 'ps' | 'fa' | 'pl' | 'pt' | 'ro' | 'ru' | 'sa' | 'sr' | 'sn' | 'sd' | 'si' | 'sk' | 'sl' | 'so' | 'es' | 'su' | 'sw' | 'ss' | 'sv' | 'tl' | 'tg' | 'ta' | 'tt' | 'te' | 'th' | 'bo' | 'tn' | 'tr' | 'tk' | 'uk' | 'ur' | 'uz' | 'vi' | 'cy' | 'wo' | 'xh' | 'yi' | 'yo' | 'zu'>;
            /**
             * Model you want the translation model to use to translate
             */
            model?: 'base' | 'enhanced';
            /**
             * Align translated utterances with the original ones
             */
            match_original_utterances?: boolean;
            /**
             * Whether to apply lipsync to the translated transcription.
             */
            lipsync?: boolean;
            /**
             * Enables or disables context-aware translation features that allow the model to adapt translations based on provided context.
             */
            context_adaptation?: boolean;
            /**
             * Context information to improve translation accuracy
             */
            context?: string;
            /**
             * Forces the translation to use informal language forms when available in the target language.
             */
            informal?: boolean;
        };
        /**
         * **[Beta]** Enable summarization for this audio
         */
        summarization?: boolean;
        /**
         * **[Beta]** Summarization configuration, if `summarization` is enabled
         */
        summarization_config?: {
            /**
             * The type of summarization to apply
             */
            type?: 'general' | 'bullet_points' | 'concise';
        };
        /**
         * **[Alpha]** Enable moderation for this audio
         */
        moderation?: boolean;
        /**
         * **[Alpha]** Enable named entity recognition for this audio
         */
        named_entity_recognition?: boolean;
        /**
         * **[Alpha]** Enable chapterization for this audio
         */
        chapterization?: boolean;
        /**
         * **[Alpha]** Enable names consistency for this audio
         */
        name_consistency?: boolean;
        /**
         * **[Alpha]** Enable custom spelling for this audio
         */
        custom_spelling?: boolean;
        /**
         * **[Alpha]** Enable structured data extraction for this audio
         */
        structured_data_extraction?: boolean;
        /**
         * **[Alpha]** Structured data extraction configuration, if `structured_data_extraction` is enabled
         */
        structured_data_extraction_config?: {
            /**
             * The list of classes to extract from the audio transcription
             */
            classes: Array<Array<string>>;
        };
        /**
         * **[Alpha]** Enable sentiment analysis for this audio
         */
        sentiment_analysis?: boolean;
        /**
         * **[Alpha]** Enable audio to llm processing for this audio
         */
        audio_to_llm?: boolean;
        /**
         * **[Alpha]** Audio to llm configuration, if `audio_to_llm` is enabled
         */
        audio_to_llm_config?: {
            /**
             * The list of prompts applied on the audio transcription
             */
            prompts: Array<Array<string>>;
        };
        /**
         * Enable sentences for this audio
         */
        sentences?: boolean;
        /**
         * **[Alpha]** Allows to change the output display_mode for this audio. The output will be reordered, creating new utterances when speakers overlapped
         */
        display_mode?: boolean;
        /**
         * **[Alpha]** Use enhanced punctuation for this audio
         */
        punctuation_enhanced?: boolean;
        /**
         * Specify the language configuration
         */
        language_config?: {
            /**
             * If one language is set, it will be used for the transcription. Otherwise, language will be auto-detected by the model.
             */
            languages?: Array<'af' | 'sq' | 'am' | 'ar' | 'hy' | 'as' | 'az' | 'ba' | 'eu' | 'be' | 'bn' | 'bs' | 'br' | 'bg' | 'ca' | 'zh' | 'hr' | 'cs' | 'da' | 'nl' | 'en' | 'et' | 'fo' | 'fi' | 'fr' | 'gl' | 'ka' | 'de' | 'el' | 'gu' | 'ht' | 'ha' | 'haw' | 'he' | 'hi' | 'hu' | 'is' | 'id' | 'it' | 'ja' | 'jv' | 'kn' | 'kk' | 'km' | 'ko' | 'lo' | 'la' | 'lv' | 'ln' | 'lt' | 'lb' | 'mk' | 'mg' | 'ms' | 'ml' | 'mt' | 'mi' | 'mr' | 'mn' | 'mymr' | 'ne' | 'no' | 'nn' | 'oc' | 'ps' | 'fa' | 'pl' | 'pt' | 'pa' | 'ro' | 'ru' | 'sa' | 'sr' | 'sn' | 'sd' | 'si' | 'sk' | 'sl' | 'so' | 'es' | 'su' | 'sw' | 'sv' | 'tl' | 'tg' | 'ta' | 'tt' | 'te' | 'th' | 'bo' | 'tr' | 'tk' | 'uk' | 'ur' | 'uz' | 'vi' | 'cy' | 'yi' | 'yo' | 'jp'>;
            /**
             * If true, language will be auto-detected on each utterance. Otherwise, language will be auto-detected on first utterance and then used for the rest of the transcription. If one language is set, this option will be ignored.
             */
            code_switching?: boolean;
        };
    } | null;
    readonly rev_async: {
        /**
         * Optional metadata for the job
         */
        metadata?: string | null;
        /**
         * Optional setting for the number of Seconds after job completion
         * when the job should be auto-deleted
         */
        delete_after?: string | null;
        /**
         * Optional language setting for foreign languages
         */
        language?: string | null;
        /**
         * Optional setting for turning on/off diarization.
         * If not set, we will assume the value is false.
         */
        skip_diarization?: boolean | null;
        /**
         * Optional setting for turning on/off postprocessing.
         * If not set, we will assume the value is false.
         */
        skip_postprocessing?: boolean | null;
        /**
         * Optional setting for passing in custom vocabularies.
         */
        custom_vocabularies?: Array<{
            phrases: Array<string>;
        }> | null;
        /**
         * The id of the prebuilt custom vocabulary job
         */
        custom_vocabulary_id?: string | null;
        /**
         * If true, only exact phrases submitted in the Rev.Ai.Api.Models.SubmitJobOptions.CustomVocabularies option will be used as custom vocabulary,
         * i.e. phrases will not be split into individual words for processing.
         * Defaults to true if Rev.Ai.Api.Models.SubmitJobOptions.CustomVocabularies is set.
         */
        strict_custom_vocabulary?: boolean | null;
        /**
         * Optional setting for disabling punctuation.
         * If unset, the value is assumed to be false.
         */
        skip_punctuation?: boolean | null;
        /**
         * Optional setting for removing disfluencies
         * If unset the value is assumed to be false
         */
        remove_disfluencies?: boolean | null;
        /**
         * Optional setting for removing atmospherics
         * If unset the value is assumed to be false
         */
        remove_atmospherics?: boolean | null;
        /**
         * Optional setting for removing profanities
         * If unset the value is assumed to be false
         */
        filter_profanity?: boolean | null;
        /**
         * Optional setting for adding data classification labels
         * If unset the value is assumed to be false
         */
        add_data_labels?: boolean | null;
        /**
         * Optional setting for redacting certain data labels
         * If unset the value is assumed to be false
         */
        enable_redaction?: boolean | null;
        /**
         * Optional speaker channels count used to indicate how many individual channels
         * to transcribe for a given audio
         */
        speaker_channels_count?: number | null;
        /**
         * Optional count of speakers in the audio to improve diarization accuracy
         */
        speakers_count?: number | null;
        /**
         * Optional count of alternatives to generate
         */
        alternatives_count?: number | null;
        deletion_length_penalty?: number | null;
        /**
         * Optional chunk size to be sent to Revspeech for processing
         */
        chunk_size?: string | null;
        transcriber?: 0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 | 10 | 11 | 12 | 13 | 14 | 15 | 16 | 17;
        /**
         * Instructs Revver to transcribe audio with all details including disfluencies and other verbal interactions
         * such as laughter
         */
        verbatim?: boolean | null;
        /**
         * Whether human order should be rushed at a greater cost to the customer
         */
        rush?: boolean | null;
        /**
         * Specific segments of the file to be transcribed by a human
         */
        segments_to_transcribe?: Array<{
            start: string;
            end: string;
        }> | null;
        /**
         * Whether human order is test mode and should return a dummy transcript
         */
        test_mode?: boolean | null;
        /**
         * Speaker names for human transcription
         */
        speaker_names?: Array<{
            display_name?: string;
        }> | null;
        /**
         * Whether to predict topics while performing speech rec
         */
        predict_topics?: boolean | null;
        /**
         * Returns the top n words of a transcript. Defaults to 0
         */
        top_nwords?: number | null;
        /**
         * Whether improved alignment should be used with transcription
         */
        forced_alignment?: boolean | null;
        /**
         * Whether transcription should be done with fusion
         */
        enable_fusion?: boolean | null;
        domain?: 0 | 1;
        /**
         * If set adds one second of silence to the audio at the end
         */
        apply_duration_padding?: boolean | null;
        diarization_type?: 0 | 10 | 20;
        /**
         * Summarization options.
         */
        summarization_config?: {
            /**
             * User defined prompt.
             */
            prompt?: string | null;
            /**
             * Summarization model options for Rev.ai API
             */
            model?: 0 | 1;
            /**
             * Summarization formatting options.
             */
            type?: 0 | 1;
        };
        /**
         * Caption options.
         */
        captions_config?: {
            [key: string]: never;
        };
        /**
         * Options for translation as part of Async job request
         */
        translation_config?: {
            /**
             * Target languages
             */
            target_languages?: Array<{
                /**
                 * Defines the type of models supported to translated the content
                 */
                model?: 0 | 1;
                /**
                 * Target language
                 */
                language: string;
            }>;
        };
    } | null;
    /**
     * JSON object that contains various groups of job configuration
     * parameters. Based on the value of `type`, a type-specific object
     * such as `transcription_config` is required to be present to
     * specify all configuration settings or parameters needed to
     * process the job inputs as expected.
     *
     * If the results of the job are to be forwarded on completion,
     * `notification_config` can be provided with a list of callbacks
     * to be made; no assumptions should be made about the order in
     * which they will occur.
     *
     * Customer specific job details or metadata can be supplied in
     * `tracking`, and this information will be available where
     * possible in the job results and in callbacks.
     *
     */
    readonly speechmatics_async: {
        alignment_config?: {
            language: string;
        };
        transcription_config?: {
            /**
             * Language model to process the audio input, normally specified as an ISO language code
             */
            language: string;
            /**
             * Request a specialized model based on 'language' but optimized for a particular field, e.g. "finance" or "medical".
             */
            domain?: string;
            /**
             * Language locale to be used when generating the transcription output, normally specified as an ISO language code
             */
            output_locale?: string;
            operating_point?: 'standard' | 'enhanced';
            /**
             * List of custom words or phrases that should be recognized. Alternative pronunciations can be specified to aid recognition.
             */
            additional_vocab?: Array<{
                content: string;
                sounds_like?: Array<string>;
            }>;
            /**
             * Control punctuation settings.
             */
            punctuation_overrides?: {
                /**
                 * Ranges between zero and one. Higher values will produce more punctuation. The default is 0.5.
                 */
                sensitivity?: number;
                /**
                 * The punctuation marks which the client is prepared to accept in transcription output, or the special value 'all' (the default). Unsupported marks are ignored. This value is used to guide the transcription process.
                 */
                permitted_marks?: Array<string>;
            };
            /**
             * Specify whether speaker or channel labels are added to the transcript.
             * The default is `none`.
             * - **none**: no speaker or channel labels are added.
             * - **speaker**: speaker attribution is performed based on acoustic matching;
             * all input channels are mixed into a single stream for processing.
             * - **channel**: multiple input channels are processed individually and collated
             * into a single transcript.
             */
            diarization?: 'none' | 'speaker' | 'channel';
            /**
             * Transcript labels to use when using collating separate input channels.
             */
            channel_diarization_labels?: Array<string>;
            /**
             * Include additional 'entity' objects in the transcription results (e.g. dates, numbers) and their original spoken form. These entities are interleaved with other types of results. The concatenation of these words is represented as a single entity with the concatenated written form present in the 'content' field. The entities contain a 'spoken_form' field, which can be used in place of the corresponding 'word' type results, in case a spoken form is preferred to a written form. They also contain a 'written_form', which can be used instead of the entity, if you want a breakdown of the words without spaces. They can still contain non-breaking spaces and other special whitespace characters, as they are considered part of the word for the formatting output. In case of a written_form, the individual word times are estimated and might not be accurate if the order of the words in the written form does not correspond to the order they were actually spoken (such as 'one hundred million dollars' and '$100 million').
             */
            enable_entities?: boolean;
            /**
             * Whether or not to enable flexible endpointing and allow the entity to continue to be spoken.
             */
            max_delay_mode?: 'fixed' | 'flexible';
            /**
             * Configuration for applying filtering to the transcription
             */
            transcript_filtering_config?: {
                /**
                 * If true, words that are identified as disfluencies will be removed from the transcript. If false (default), they are tagged in the transcript as 'disfluency'.
                 */
                remove_disfluencies?: boolean;
                /**
                 * A list of replacements to apply to the transcript. Each replacement is a pair of strings, where the first string is the pattern to be replaced and the second string is the replacement text.
                 */
                replacements?: Array<{
                    from: string;
                    to: string;
                }>;
            };
            /**
             * Configuration for speaker diarization
             */
            speaker_diarization_config?: {
                /**
                 * If true, the algorithm will prefer to stay with the current active speaker if it is a close enough match, even if other speakers may be closer.  This is useful for cases where we can flip incorrectly between similar speakers during a single speaker section."
                 */
                prefer_current_speaker?: boolean;
                /**
                 * Controls how sensitive the algorithm is in terms of keeping similar speakers separate, as opposed to combining them into a single speaker.  Higher values will typically lead to more speakers, as the degree of difference between speakers in order to allow them to remain distinct will be lower.  A lower value for this parameter will conversely guide the algorithm towards being less sensitive in terms of retaining similar speakers, and as such may lead to fewer speakers overall.  The default is 0.5.
                 */
                speaker_sensitivity?: number;
            };
        };
        tracking?: {
            /**
             * The title of the job.
             */
            title?: string;
            /**
             * External system reference.
             */
            reference?: string;
            tags?: Array<string>;
        };
        output_config?: {
            /**
             * Parameters that override default values of srt conversion. max_line_length: sets maximum count of characters per subtitle line including white space. max_lines: sets maximum count of lines in a subtitle section.
             */
            srt_overrides?: {
                max_line_length?: number;
                max_lines?: number;
            };
        };
        translation_config?: {
            target_languages: Array<string>;
        };
        language_identification_config?: {
            expected_languages?: Array<string>;
            /**
             * Action to take if all of the predicted languages are below the confidence threshold
             */
            low_confidence_action?: 'allow' | 'reject' | 'use_default_language';
            default_language?: string;
        };
        summarization_config?: {
            content_type?: 'auto' | 'informative' | 'conversational';
            summary_length?: 'brief' | 'detailed';
            summary_type?: 'paragraphs' | 'bullets';
        };
        topic_detection_config?: {
            topics?: Array<string>;
        };
        audio_events_config?: {
            types?: Array<string>;
        };
    } | null;
    /**
     * Recall.ai async transcription Settings
     *
     * Docs: https://docs.recall.ai/docs/recallai-transcription
     */
    readonly recallai_async: {
        /**
         * Must be `en` in low latency mode. Docs: https://docs.recall.ai/docs/recallai-transcription
         *
         * * `auto` - auto
         * * `bg` - bg
         * * `ca` - ca
         * * `cs` - cs
         * * `da` - da
         * * `de` - de
         * * `el` - el
         * * `en` - en
         * * `en_au` - en_au
         * * `en_uk` - en_uk
         * * `en_us` - en_us
         * * `es` - es
         * * `et` - et
         * * `fi` - fi
         * * `fr` - fr
         * * `hi` - hi
         * * `hu` - hu
         * * `id` - id
         * * `it` - it
         * * `ja` - ja
         * * `ko` - ko
         * * `lt` - lt
         * * `lv` - lv
         * * `ms` - ms
         * * `nl` - nl
         * * `no` - no
         * * `pl` - pl
         * * `pt` - pt
         * * `ro` - ro
         * * `ru` - ru
         * * `sk` - sk
         * * `sv` - sv
         * * `th` - th
         * * `tr` - tr
         * * `uk` - uk
         * * `vi` - vi
         * * `zh` - zh
         */
        language_code?: 'auto' | 'bg' | 'ca' | 'cs' | 'da' | 'de' | 'el' | 'en' | 'en_au' | 'en_uk' | 'en_us' | 'es' | 'et' | 'fi' | 'fr' | 'hi' | 'hu' | 'id' | 'it' | 'ja' | 'ko' | 'lt' | 'lv' | 'ms' | 'nl' | 'no' | 'pl' | 'pt' | 'ro' | 'ru' | 'sk' | 'sv' | 'th' | 'tr' | 'uk' | 'vi' | 'zh';
        /**
         * List of text strings to find/replace in the transcript.
         */
        spelling?: Array<RecallaiSpellingEntry>;
        /**
         * Increases the chances that these terms appear in the transcript over some sound-alikes.
         */
        key_terms?: Array<string>;
        filter_profanity?: boolean;
    } | null;
    /**
     * AssemblyAi async chunked transcription Settings
     *
     * Docs: https://www.assemblyai.com/docs/api-reference/transcripts/submit
     */
    assembly_ai_async_chunked?: {
        /**
         * The language of your audio file. Possible values are found in [Supported Languages](https://www.assemblyai.com/docs/concepts/supported-languages).
         * The default value is 'en_us'.
         *
         */
        language_code?: 'en' | 'en_au' | 'en_uk' | 'en_us' | 'es' | 'fr' | 'de' | 'it' | 'pt' | 'nl' | 'af' | 'sq' | 'am' | 'ar' | 'hy' | 'as' | 'az' | 'ba' | 'eu' | 'be' | 'bn' | 'bs' | 'br' | 'bg' | 'my' | 'ca' | 'zh' | 'hr' | 'cs' | 'da' | 'et' | 'fo' | 'fi' | 'gl' | 'ka' | 'el' | 'gu' | 'ht' | 'ha' | 'haw' | 'he' | 'hi' | 'hu' | 'is' | 'id' | 'ja' | 'jw' | 'kn' | 'kk' | 'km' | 'ko' | 'lo' | 'la' | 'lv' | 'ln' | 'lt' | 'lb' | 'mk' | 'mg' | 'ms' | 'ml' | 'mt' | 'mi' | 'mr' | 'mn' | 'ne' | 'no' | 'nn' | 'oc' | 'pa' | 'ps' | 'fa' | 'pl' | 'ro' | 'ru' | 'sa' | 'sr' | 'sn' | 'sd' | 'si' | 'sk' | 'sl' | 'so' | 'su' | 'sw' | 'sv' | 'tl' | 'tg' | 'ta' | 'tt' | 'te' | 'th' | 'bo' | 'tr' | 'tk' | 'uk' | 'ur' | 'uz' | 'vi' | 'cy' | 'yi' | 'yo' | string | null;
        /**
         * Enable [Automatic language detection](https://www.assemblyai.com/docs/models/speech-recognition#automatic-language-detection), either true or false.
         */
        language_detection?: boolean;
        /**
         * The confidence threshold for the automatically detected language.
         * An error will be returned if the language confidence is below this threshold.
         * Defaults to 0.
         *
         */
        language_confidence_threshold?: number;
        /**
         * The speech model to use for the transcription. When `null`, the "best" model is used.
         */
        speech_model?: 'best' | 'nano' | 'slam-1' | 'universal' | string | null;
        /**
         * Enable Automatic Punctuation, can be true or false
         */
        punctuate?: boolean;
        /**
         * Enable Text Formatting, can be true or false
         */
        format_text?: boolean;
        /**
         * Transcribe Filler Words, like "umm", in your media file; can be true or false
         */
        disfluencies?: boolean;
        /**
         * Enable [Multichannel](https://www.assemblyai.com/docs/models/speech-recognition#multichannel-transcription) transcription, can be true or false.
         */
        multichannel?: boolean;
        /**
         * Enable [Dual Channel](https://www.assemblyai.com/docs/models/speech-recognition#dual-channel-transcription) transcription, can be true or false.
         *
         * @deprecated
         */
        dual_channel?: boolean;
        /**
         * Enable Key Phrases, either true or false
         */
        auto_highlights?: boolean;
        /**
         * The point in time, in milliseconds, to begin transcribing in your media file
         */
        audio_start_from?: number;
        /**
         * The point in time, in milliseconds, to stop transcribing in your media file
         */
        audio_end_at?: number;
        /**
         * The list of custom vocabulary to boost transcription probability for
         *
         * @deprecated
         */
        word_boost?: Array<string>;
        /**
         * How much to boost specified words
         */
        boost_param?: 'low' | 'default' | 'high';
        /**
         * Filter profanity from the transcribed text, can be true or false
         */
        filter_profanity?: boolean;
        /**
         * Redact PII from the transcribed text using the Redact PII model, can be true or false
         */
        redact_pii?: boolean;
        /**
         * Generate a copy of the original media file with spoken PII "beeped" out, can be true or false. See [PII redaction](https://www.assemblyai.com/docs/models/pii-redaction) for more details.
         */
        redact_pii_audio?: boolean;
        /**
         * Controls the filetype of the audio created by redact_pii_audio. Currently supports mp3 (default) and wav. See [PII redaction](https://www.assemblyai.com/docs/models/pii-redaction) for more details.
         */
        redact_pii_audio_quality?: 'mp3' | 'wav';
        /**
         * The list of PII Redaction policies to enable. See [PII redaction](https://www.assemblyai.com/docs/models/pii-redaction) for more details.
         */
        redact_pii_policies?: Array<'account_number' | 'banking_information' | 'blood_type' | 'credit_card_cvv' | 'credit_card_expiration' | 'credit_card_number' | 'date' | 'date_interval' | 'date_of_birth' | 'drivers_license' | 'drug' | 'duration' | 'email_address' | 'event' | 'filename' | 'gender_sexuality' | 'healthcare_number' | 'injury' | 'ip_address' | 'language' | 'location' | 'marital_status' | 'medical_condition' | 'medical_process' | 'money_amount' | 'nationality' | 'number_sequence' | 'occupation' | 'organization' | 'passport_number' | 'password' | 'person_age' | 'person_name' | 'phone_number' | 'physical_attribute' | 'political_affiliation' | 'religion' | 'statistics' | 'time' | 'url' | 'us_social_security_number' | 'username' | 'vehicle_id' | 'zodiac_sign'>;
        /**
         * The replacement logic for detected PII, can be "entity_type" or "hash". See [PII redaction](https://www.assemblyai.com/docs/models/pii-redaction) for more details.
         */
        redact_pii_sub?: 'entity_name' | 'hash' | string | null;
        /**
         * Enable [Speaker diarization](https://www.assemblyai.com/docs/models/speaker-diarization), can be true or false
         */
        speaker_labels?: boolean;
        /**
         * Tells the speaker label model how many speakers it should attempt to identify, up to 10. See [Speaker diarization](https://www.assemblyai.com/docs/models/speaker-diarization) for more details.
         */
        speakers_expected?: number | null;
        /**
         * Enable [Content Moderation](https://www.assemblyai.com/docs/models/content-moderation), can be true or false
         */
        content_safety?: boolean;
        /**
         * The confidence threshold for the Content Moderation model. Values must be between 25 and 100.
         */
        content_safety_confidence?: number;
        /**
         * Enable [Topic Detection](https://www.assemblyai.com/docs/models/topic-detection), can be true or false
         */
        iab_categories?: boolean;
        /**
         * Customize how words are spelled and formatted using to and from values
         */
        custom_spelling?: Array<{
            /**
             * Words or phrases to replace
             */
            from: Array<string>;
            /**
             * Word to replace with
             */
            to: string;
        }>;
        /**
         * <Warning>`keyterms_prompt` is only supported when the `speech_model` is specified as `slam-1`</Warning>
         * Improve accuracy with up to 1000 domain-specific words or phrases (maximum 6 words per phrase).
         *
         */
        keyterms_prompt?: Array<string>;
        /**
         * This parameter does not currently have any functionality attached to it.
         *
         * @deprecated
         */
        prompt?: string;
        /**
         * Enable [Sentiment Analysis](https://www.assemblyai.com/docs/models/sentiment-analysis), can be true or false
         */
        sentiment_analysis?: boolean;
        /**
         * Enable [Auto Chapters](https://www.assemblyai.com/docs/models/auto-chapters), can be true or false
         */
        auto_chapters?: boolean;
        /**
         * Enable [Entity Detection](https://www.assemblyai.com/docs/models/entity-detection), can be true or false
         */
        entity_detection?: boolean;
        /**
         * Reject audio files that contain less than this fraction of speech.
         * Valid values are in the range [0, 1] inclusive.
         *
         */
        speech_threshold?: number | null;
        /**
         * Enable [Summarization](https://www.assemblyai.com/docs/models/summarization), can be true or false
         */
        summarization?: boolean;
        /**
         * The model to summarize the transcript
         */
        summary_model?: 'informative' | 'conversational' | 'catchy';
        /**
         * The type of summary
         */
        summary_type?: 'bullets' | 'bullets_verbose' | 'gist' | 'headline' | 'paragraph';
        /**
         * Enable custom topics, either true or false
         *
         * @deprecated
         */
        custom_topics?: boolean;
        /**
         * The list of custom topics
         */
        topics?: Array<string>;
    };
    /**
     * AssemblyAi Real-time Transcription Settings
     *
     * Docs: https://www.assemblyai.com/docs/api-reference/streaming-api/streaming-api
     */
    assembly_ai_v3_streaming?: {
        /**
         * Whether to return formatted final transcripts
         */
        format_turns?: boolean;
        /**
         * The confidence threshold (0.0 to 1.0) to use when determining if the end of a turn has been reached
         */
        end_of_turn_confidence_threshold?: unknown;
        /**
         * The minimum amount of silence in milliseconds required to detect end of turn when confident
         */
        min_end_of_turn_silence_when_confident?: unknown;
        /**
         * The maximum amount of silence in milliseconds allowed in a turn before end of turn is triggered
         */
        max_turn_silence?: unknown;
    };
    /**
     * Recall.ai Real-time Transcription Settings
     *
     * Docs: https://docs.recall.ai/docs/recallai-transcription
     */
    recallai_streaming?: RecallaiStreamingTranscription;
    /**
     * Deepgram Real-time Transcription Settings
     *
     * Docs: https://developers.deepgram.com/reference/streaming
     */
    deepgram_streaming?: {
        /**
         * Defaults to `false`. Recognize speaker changes. Each word in the transcript will be assigned a speaker number starting at 0
         */
        diarize?: 'true' | 'false';
        /**
         * Identify and extract key entities from content in submitted audio
         */
        dictation?: 'true' | 'false';
        /**
         * Indicates how long Deepgram will wait to detect whether a speaker has finished speaking or pauses for a significant period of time. When set to a value, the streaming endpoint immediately finalizes the transcription for the processed time range and returns the transcript with a speech_final parameter set to true. Can also be set to false to disable endpointing
         */
        endpointing?: unknown;
        /**
         * Arbitrary key-value pairs that are attached to the API response for usage in downstream processing
         */
        extra?: unknown;
        /**
         * Filler Words can help transcribe interruptions in your audio, like "uh" and "um"
         */
        filler_words?: 'true' | 'false';
        /**
         * Specifies whether the streaming endpoint should provide ongoing transcription updates as more audio is received. When set to true, the endpoint sends continuous updates, meaning transcription results may evolve over time
         */
        interim_results?: 'true' | 'false';
        /**
         * Key term prompting can boost or suppress specialized terminology and brands. Only compatible with Nova-3
         */
        keyterm?: unknown;
        /**
         * Keywords can boost or suppress specialized terminology and brands
         */
        keywords?: unknown;
        /**
         * The [BCP-47 language tag](https://tools.ietf.org/html/bcp47) that hints at the primary spoken language. Depending on the Model you choose only certain languages are available
         */
        language?: 'bg' | 'ca' | 'cs' | 'da' | 'da-DK' | 'de' | 'de-CH' | 'el' | 'en' | 'en-AU' | 'en-GB' | 'en-IN' | 'en-NZ' | 'en-US' | 'es' | 'es-419' | 'es-LATAM' | 'et' | 'fi' | 'fr' | 'fr-CA' | 'hi' | 'hi-Latn' | 'hu' | 'id' | 'it' | 'ja' | 'ko' | 'ko-KR' | 'lt' | 'lv' | 'ms' | 'multi' | 'nl' | 'nl-BE' | 'no' | 'pl' | 'pt' | 'pt-BR' | 'pt-PT' | 'ro' | 'ru' | 'sk' | 'sv' | 'sv-SE' | 'taq' | 'th' | 'th-TH' | 'tr' | 'uk' | 'vi' | 'zh' | 'zh-CN' | 'zh-HK' | 'zh-Hans' | 'zh-Hant' | 'zh-TW';
        /**
         * Opts out requests from the Deepgram Model Improvement Program. Refer to our Docs for pricing impacts before setting this to true. https://dpgr.am/deepgram-mip
         */
        mip_opt_out?: unknown;
        /**
         * AI model to use for the transcription
         */
        model?: 'nova-3' | 'nova-3-general' | 'nova-3-medical' | 'nova-2' | 'nova-2-general' | 'nova-2-meeting' | 'nova-2-finance' | 'nova-2-conversationalai' | 'nova-2-voicemail' | 'nova-2-video' | 'nova-2-medical' | 'nova-2-drivethru' | 'nova-2-automotive' | 'nova' | 'nova-general' | 'nova-phonecall' | 'nova-medical' | 'enhanced' | 'enhanced-general' | 'enhanced-meeting' | 'enhanced-phonecall' | 'enhanced-finance' | 'base' | 'meeting' | 'phonecall' | 'finance' | 'conversationalai' | 'voicemail' | 'video' | 'custom';
        /**
         * Transcribe each audio channel independently
         */
        multichannel?: 'true' | 'false';
        /**
         * Convert numbers from written format to numerical format
         */
        numerals?: 'true' | 'false';
        /**
         * Profanity Filter looks for recognized profanity and converts it to the nearest recognized non-profane word or removes it from the transcript completely
         */
        profanity_filter?: 'true' | 'false';
        /**
         * Add punctuation and capitalization to the transcript
         */
        punctuate?: 'true' | 'false';
        /**
         * Redaction removes sensitive information from your transcripts
         */
        redact?: 'true' | 'false' | 'pci' | 'numbers' | 'aggressive_numbers' | 'ssn';
        /**
         * Search for terms or phrases in submitted audio and replaces them
         */
        replace?: unknown;
        /**
         * Search for terms or phrases in submitted audio
         */
        search?: unknown;
        /**
         * Apply formatting to transcript output. When set to true, additional formatting will be applied to transcripts to improve readability
         */
        smart_format?: 'true' | 'false';
        /**
         * Label your requests for the purpose of identification during usage reporting
         */
        tag?: unknown;
        /**
         * Indicates how long Deepgram will wait to send an UtteranceEnd message after a word has been transcribed. Use with interim_results
         */
        utterance_end_ms?: unknown;
        /**
         * Indicates that speech has started. You'll begin receiving Speech Started messages upon speech starting
         */
        vad_events?: 'true' | 'false';
        /**
         * Version of an AI model to use
         */
        version?: unknown;
    };
    /**
     * GladiaV2 Real-time Transcription Settings
     *
     * Docs: https://docs.gladia.io/api-reference/v2/live/init
     */
    gladia_v2_streaming?: {
        /**
         * The model used to process the audio. "solaria-1" is used by default.
         */
        model?: 'solaria-1';
        /**
         * The endpointing duration in seconds. Endpointing is the duration of silence which will cause an utterance to be considered as finished
         */
        endpointing?: number;
        /**
         * The maximum duration in seconds without endpointing. If endpointing is not detected after this duration, current utterance will be considered as finished
         */
        maximum_duration_without_endpointing?: number;
        /**
         * Specify the language configuration
         */
        language_config?: {
            /**
             * If one language is set, it will be used for the transcription. Otherwise, language will be auto-detected by the model.
             */
            languages?: Array<'af' | 'sq' | 'am' | 'ar' | 'hy' | 'as' | 'az' | 'ba' | 'eu' | 'be' | 'bn' | 'bs' | 'br' | 'bg' | 'ca' | 'zh' | 'hr' | 'cs' | 'da' | 'nl' | 'en' | 'et' | 'fo' | 'fi' | 'fr' | 'gl' | 'ka' | 'de' | 'el' | 'gu' | 'ht' | 'ha' | 'haw' | 'he' | 'hi' | 'hu' | 'is' | 'id' | 'it' | 'ja' | 'jv' | 'kn' | 'kk' | 'km' | 'ko' | 'lo' | 'la' | 'lv' | 'ln' | 'lt' | 'lb' | 'mk' | 'mg' | 'ms' | 'ml' | 'mt' | 'mi' | 'mr' | 'mn' | 'mymr' | 'ne' | 'no' | 'nn' | 'oc' | 'ps' | 'fa' | 'pl' | 'pt' | 'pa' | 'ro' | 'ru' | 'sa' | 'sr' | 'sn' | 'sd' | 'si' | 'sk' | 'sl' | 'so' | 'es' | 'su' | 'sw' | 'sv' | 'tl' | 'tg' | 'ta' | 'tt' | 'te' | 'th' | 'bo' | 'tr' | 'tk' | 'uk' | 'ur' | 'uz' | 'vi' | 'cy' | 'yi' | 'yo' | 'jp'>;
            /**
             * If true, language will be auto-detected on each utterance. Otherwise, language will be auto-detected on first utterance and then used for the rest of the transcription. If one language is set, this option will be ignored.
             */
            code_switching?: boolean;
        };
        /**
         * Specify the pre-processing configuration
         */
        pre_processing?: {
            /**
             * If true, apply pre-processing to the audio stream to enhance the quality.
             */
            audio_enhancer?: boolean;
            /**
             * Sensitivity configuration for Speech Threshold. A value close to 1 will apply stricter thresholds, making it less likely to detect background sounds as speech.
             */
            speech_threshold?: number;
        };
        /**
         * Specify the realtime processing configuration
         */
        realtime_processing?: {
            /**
             * If true, enable custom vocabulary for the transcription.
             */
            custom_vocabulary?: boolean;
            /**
             * Custom vocabulary configuration, if `custom_vocabulary` is enabled
             */
            custom_vocabulary_config?: {
                /**
                 * Specific vocabulary list to feed the transcription model with. Each item can be a string or an object with the following properties: value, intensity, pronunciations, language.
                 */
                vocabulary: Array<{
                    /**
                     * The text used to replace in the transcription.
                     */
                    value: string;
                    /**
                     * The global intensity of the feature.
                     */
                    intensity?: number;
                    /**
                     * The pronunciations used in the transcription.
                     */
                    pronunciations?: Array<string>;
                    /**
                     * Specify the language in which it will be pronounced when sound comparison occurs. Default to transcription language.
                     */
                    language?: string;
                } | string>;
                /**
                 * Default intensity for the custom vocabulary
                 */
                default_intensity?: number;
            };
            /**
             * If true, enable custom spelling for the transcription.
             */
            custom_spelling?: boolean;
            /**
             * If true, enable translation for the transcription
             */
            translation?: boolean;
            /**
             * Translation configuration, if `translation` is enabled
             */
            translation_config?: {
                target_languages: Array<'af' | 'sq' | 'am' | 'ar' | 'hy' | 'as' | 'ast' | 'az' | 'ba' | 'eu' | 'be' | 'bn' | 'bs' | 'br' | 'bg' | 'my' | 'ca' | 'ceb' | 'zh' | 'hr' | 'cs' | 'da' | 'nl' | 'en' | 'et' | 'fo' | 'fi' | 'fr' | 'fy' | 'ff' | 'gd' | 'gl' | 'lg' | 'ka' | 'de' | 'el' | 'gu' | 'ht' | 'ha' | 'haw' | 'he' | 'hi' | 'hu' | 'is' | 'ig' | 'ilo' | 'id' | 'ga' | 'it' | 'ja' | 'jp' | 'jv' | 'kn' | 'kk' | 'km' | 'ko' | 'lo' | 'la' | 'lv' | 'ln' | 'lt' | 'lb' | 'mk' | 'mg' | 'ms' | 'ml' | 'mt' | 'mi' | 'mr' | 'mo' | 'mn' | 'mymr' | 'ne' | 'no' | 'nn' | 'oc' | 'or' | 'pa' | 'ps' | 'fa' | 'pl' | 'pt' | 'ro' | 'ru' | 'sa' | 'sr' | 'sn' | 'sd' | 'si' | 'sk' | 'sl' | 'so' | 'es' | 'su' | 'sw' | 'ss' | 'sv' | 'tl' | 'tg' | 'ta' | 'tt' | 'te' | 'th' | 'bo' | 'tn' | 'tr' | 'tk' | 'uk' | 'ur' | 'uz' | 'vi' | 'cy' | 'wo' | 'xh' | 'yi' | 'yo' | 'zu'>;
                /**
                 * Model you want the translation model to use to translate
                 */
                model?: 'base' | 'enhanced';
                /**
                 * Align translated utterances with the original ones
                 */
                match_original_utterances?: boolean;
                /**
                 * Whether to apply lipsync to the translated transcription.
                 */
                lipsync?: boolean;
                /**
                 * Enables or disables context-aware translation features that allow the model to adapt translations based on provided context.
                 */
                context_adaptation?: boolean;
                /**
                 * Context information to improve translation accuracy
                 */
                context?: string;
                /**
                 * Forces the translation to use informal language forms when available in the target language.
                 */
                informal?: boolean;
            };
            /**
             * If true, enable named entity recognition for the transcription.
             */
            named_entity_recognition?: boolean;
            /**
             * If true, enable sentiment analysis for the transcription.
             */
            sentiment_analysis?: boolean;
        };
        /**
         * Specify the post-processing configuration
         */
        post_processing?: {
            /**
             * If true, generates summarization for the whole transcription.
             */
            summarization?: boolean;
            /**
             * Summarization configuration, if `summarization` is enabled
             */
            summarization_config?: {
                /**
                 * The type of summarization to apply
                 */
                type?: 'general' | 'bullet_points' | 'concise';
            };
            /**
             * If true, generates chapters for the whole transcription.
             */
            chapterization?: boolean;
        };
        /**
         * Specify the websocket messages configuration
         */
        messages_config?: {
            /**
             * If true, final utterance will be sent to websocket.
             */
            receive_final_transcripts?: boolean;
            /**
             * If true, begin and end speech events will be sent to websocket.
             */
            receive_speech_events?: boolean;
            /**
             * If true, pre-processing events will be sent to websocket.
             */
            receive_pre_processing_events?: boolean;
            /**
             * If true, realtime processing events will be sent to websocket.
             */
            receive_realtime_processing_events?: boolean;
            /**
             * If true, post-processing events will be sent to websocket.
             */
            receive_post_processing_events?: boolean;
            /**
             * If true, acknowledgments will be sent to websocket.
             */
            receive_acknowledgments?: boolean;
            /**
             * If true, errors will be sent to websocket.
             */
            receive_errors?: boolean;
            /**
             * If true, lifecycle events will be sent to websocket.
             */
            receive_lifecycle_events?: boolean;
        };
        /**
         * If true, messages will be sent to configured url.
         */
        callback?: boolean;
        /**
         * Specify the callback configuration
         */
        callback_config?: {
            /**
             * URL on which we will do a `POST` request with configured messages
             */
            url?: string;
            /**
             * If true, final utterance will be sent to the defined callback.
             */
            receive_final_transcripts?: boolean;
            /**
             * If true, begin and end speech events will be sent to the defined callback.
             */
            receive_speech_events?: boolean;
            /**
             * If true, pre-processing events will be sent to the defined callback.
             */
            receive_pre_processing_events?: boolean;
            /**
             * If true, realtime processing events will be sent to the defined callback.
             */
            receive_realtime_processing_events?: boolean;
            /**
             * If true, post-processing events will be sent to the defined callback.
             */
            receive_post_processing_events?: boolean;
            /**
             * If true, acknowledgments will be sent to the defined callback.
             */
            receive_acknowledgments?: boolean;
            /**
             * If true, errors will be sent to the defined callback.
             */
            receive_errors?: boolean;
            /**
             * If true, lifecycle events will be sent to the defined callback.
             */
            receive_lifecycle_events?: boolean;
        };
    };
    /**
     * Rev Real-time Transcription Settings
     *
     * Docs: https://docs.rev.ai/api/streaming/requests/
     */
    rev_streaming?: {
        obscure_expletives?: boolean;
        delete_after?: string;
        'audio_options._content_type'?: string;
        'audio_options._layout'?: string;
        'audio_options._rate'?: number;
        'audio_options._format'?: string;
        'audio_options._channels'?: number;
        transcriber?: 0 | 1;
        language?: string;
        metadata?: string;
        filter_profanity?: boolean;
        remove_disfluencies?: boolean;
        detailed_partials?: boolean;
        custom_vocabulary_id?: string;
        delete_after_seconds?: number;
        max_segment_duration_seconds?: number;
        max_connection_wait_seconds?: number;
        allow_interruption?: boolean;
        enable_speaker_switch?: boolean;
        start_ts?: string;
        skip_postprocessing?: boolean;
        priority?: 0 | 1;
        user_agent?: string;
    };
    aws_transcribe_streaming?: (unknown | {
        identify_language: true;
    }) & {
        /**
         * Specify the language code that represents the language spoken. If you're unsure of the language spoken in your audio, consider using IdentifyLanguage to enable automatic language identification.
         */
        language_code?: string;
        /**
         * Specify how you want your vocabulary filter applied to your transcript. To replace words with ***, choose mask. To delete words, choose remove. To flag words without changing them, choose tag.
         */
        vocabulary_filter_method?: string;
        /**
         * Specify the name of the custom vocabulary filter that you want to use when processing your transcription. Note that vocabulary filter names are case sensitive.  If you use Amazon Transcribe in multiple Regions, the vocabulary filter must be available in Amazon Transcribe in each Region. If you include IdentifyLanguage and want to use one or more vocabulary filters with your transcription, use the VocabularyFilterNames parameter instead.
         */
        vocabulary_filter_name?: string;
        /**
         * Specify the name of the custom vocabulary that you want to use when processing your transcription. Note that vocabulary names are case sensitive. If you use Amazon Transcribe multiple Regions, the vocabulary must be available in Amazon Transcribe in each Region. If you include IdentifyLanguage and want to use one or more custom vocabularies with your transcription, use the VocabularyNames parameter instead.
         */
        vocabulary_name?: string;
        /**
         * The Amazon Web Services Region in which to use Amazon Transcribe. If you don't specify a Region, then the MediaRegion of the meeting is used. However, if Amazon Transcribe is not available in the MediaRegion, then a TranscriptFailed event is sent. Use auto to use Amazon Transcribe in a Region near the meeting’s MediaRegion. For more information, refer to Choosing a transcription Region in the Amazon Chime SDK Developer Guide.
         */
        region?: string;
        /**
         * Enables partial result stabilization for your transcription. Partial result stabilization can reduce latency in your output, but may impact accuracy.
         */
        enable_partial_results_stabilization?: boolean;
        /**
         * Specify the level of stability to use when you enable partial results stabilization (EnablePartialResultsStabilization). Low stability provides the highest accuracy. High stability transcribes faster, but with slightly lower accuracy.
         */
        partial_results_stability?: string;
        /**
         * Labels all personally identifiable information (PII) identified in your transcript. If you don't include PiiEntityTypes, all PII is identified.  You can’t set ContentIdentificationType and ContentRedactionType.
         */
        content_identification_type?: string;
        /**
         * Content redaction is performed at the segment level. If you don't include PiiEntityTypes, all PII is redacted.  You can’t set ContentRedactionType and ContentIdentificationType.
         */
        content_redaction_type?: string;
        /**
         * Specify which types of personally identifiable information (PII) you want to redact in your transcript. You can include as many types as you'd like, or you can select ALL. Values must be comma-separated and can include: ADDRESS, BANK_ACCOUNT_NUMBER, BANK_ROUTING, CREDIT_DEBIT_CVV, CREDIT_DEBIT_EXPIRY CREDIT_DEBIT_NUMBER, EMAIL,NAME, PHONE, PIN, SSN, or ALL. Note that if you include PiiEntityTypes, you must also include ContentIdentificationType or ContentRedactionType. If you include ContentRedactionType or ContentIdentificationType, but do not include PiiEntityTypes, all PII is redacted or identified.
         */
        pii_entity_types?: string;
        /**
         * Specify the name of the custom language model that you want to use when processing your transcription. Note that language model names are case sensitive. The language of the specified language model must match the language code. If the languages don't match, the custom language model isn't applied. There are no errors or warnings associated with a language mismatch. If you use Amazon Transcribe in multiple Regions, the custom language model must be available in Amazon Transcribe in each Region.
         */
        language_model_name?: string;
        /**
         * Enables automatic language identification for your transcription. If you include IdentifyLanguage, you can optionally use LanguageOptions to include a list of language codes that you think may be present in your audio stream. Including language options can improve transcription accuracy. You can also use PreferredLanguage to include a preferred language. Doing so can help Amazon Transcribe identify the language faster. You must include either LanguageCode or IdentifyLanguage. Language identification can't be combined with custom language models or redaction.
         */
        identify_language?: boolean;
        /**
         * Specify two or more language codes that represent the languages you think may be present in your media; including more than five is not recommended. If you're unsure what languages are present, do not include this parameter. Including language options can improve the accuracy of language identification. If you include LanguageOptions, you must also include IdentifyLanguage.  You can only include one language dialect per language. For example, you cannot include en-US and en-AU.
         */
        language_options?: string;
        /**
         * Specify a preferred language from the subset of languages codes you specified in LanguageOptions. You can only use this parameter if you include IdentifyLanguage and LanguageOptions.
         */
        preferred_language?: string;
        /**
         * Specify the names of the custom vocabularies that you want to use when processing your transcription. Note that vocabulary names are case sensitive. If you use Amazon Transcribe in multiple Regions, the vocabulary must be available in Amazon Transcribe in each Region. If you don't include IdentifyLanguage and want to use a custom vocabulary with your transcription, use the VocabularyName parameter instead.
         */
        vocabulary_names?: string;
        /**
         * Specify the names of the custom vocabulary filters that you want to use when processing your transcription. Note that vocabulary filter names are case sensitive. If you use Amazon Transcribe in multiple Regions, the vocabulary filter must be available in Amazon Transcribe in each Region.  If you're not including IdentifyLanguage and want to use a custom vocabulary filter with your transcription, use the VocabularyFilterName parameter instead.
         */
        vocabulary_filter_names?: string;
    };
    /**
     * Speechmatics Real-time Transcription Settings
     *
     * You must specify `language` (e.g `en`)
     *
     * Docs: https://docs.speechmatics.com/rt-api-ref#transcription-config
     */
    speechmatics_streaming?: {
        language: string;
        /**
         * Request a specialized model based on 'language' but optimized for a particular field, e.g. "finance" or "medical".
         */
        domain?: string;
        output_locale?: string;
        additional_vocab?: Array<string | {
            content: string;
            sounds_like?: Array<string>;
        }>;
        diarization?: 'none' | 'speaker';
        max_delay?: number;
        max_delay_mode?: 'flexible' | 'fixed';
        speaker_diarization_config?: {
            max_speakers?: number;
            prefer_current_speaker?: boolean;
            speaker_sensitivity?: number;
        };
        audio_filtering_config?: {
            volume_threshold?: number;
        };
        transcript_filtering_config?: {
            remove_disfluencies?: boolean;
            replacements?: Array<{
                from: string;
                to: string;
            }>;
        };
        enable_partials?: boolean;
        enable_entities?: boolean;
        operating_point?: 'standard' | 'enhanced';
        punctuation_overrides?: {
            /**
             * The punctuation marks which the client is prepared to accept in transcription output, or the special value 'all' (the default). Unsupported marks are ignored. This value is used to guide the transcription process.
             */
            permitted_marks?: Array<string>;
            /**
             * Ranges between zero and one. Higher values will produce more punctuation. The default is 0.5.
             */
            sensitivity?: number;
        };
        /**
         * This mode will detect when a speaker has stopped talking. The end_of_utterance_silence_trigger is the time in seconds after which the server will assume that the speaker has finished speaking, and will emit an EndOfUtterance message. A value of 0 disables the feature.
         */
        conversation_config?: {
            end_of_utterance_silence_trigger?: number;
        };
    };
};

export type TranscriptArtifactShortcut = {
    readonly id: string;
    readonly created_at: string;
    status: ArtifactStatus;
    metadata: {
        [key: string]: string | null;
    };
    data: TranscriptArtifactData;
    diarization: TranscriptArtifactDiarization | null;
    provider: TranscriptArtifactProvider;
};

export type VideoMixedArtifactData = {
    readonly download_url: string | null;
};

export type VideoMixedArtifactShortcut = {
    readonly id: string;
    readonly created_at: string;
    status: ArtifactStatus;
    metadata: {
        [key: string]: string | null;
    };
    data: VideoMixedArtifactData;
    /**
     * Format of the mixed video file
     *
     * * `mp4` - Mp4
     */
    format: Format3B3Enum;
};

/**
 * * `speaker_view` - speaker_view
 * * `gallery_view` - gallery_view
 * * `gallery_view_v2` - gallery_view_v2
 * * `audio_only` - audio_only
 */
export type VideoMixedLayoutEnum = 'speaker_view' | 'gallery_view' | 'gallery_view_v2' | 'audio_only';

/**
 * * `hide` - hide
 * * `beside` - beside
 * * `overlap` - overlap
 */
export type VideoMixedParticipantVideoWhenScreenshareEnum = 'hide' | 'beside' | 'overlap';

export type VideoOutput = {
    /**
     * The kind of data encoded in b64_data
     *
     * * `jpeg` - jpeg
     */
    kind: VideoOutputKindEnum;
};

/**
 * * `jpeg` - jpeg
 */
export type VideoOutputKindEnum = 'jpeg';

/**
 * * `web` - web
 * * `web_4_core` - web_4_core
 * * `web_gpu` - web_gpu
 */
export type WebexEnum = 'web' | 'web_4_core' | 'web_gpu';

export type Zoom = {
    /**
     * A URL which Recall will make a GET request to, in order to retrieve the Zoom Join Token for Local Recording, which the Zoom bot uses to automatically record. This token can be generated through the Zoom API. Docs: https://marketplace.zoom.us/docs/api-reference/zoom-api/methods/#operation/meetingLocalRecordingJoinToken
     */
    join_token_url?: string;
    /**
     * A URL which Recall will make  GET request to, in order to retrieve the ZAK. The V1 Zoom bot uses this to join meetings that require authentication to join.This token can be generated through the Zoom API. Docs: https://marketplace.zoom.us/docs/api-reference/zoom-api/methods/#operation/userZak
     */
    zak_url?: string;
    /**
     * This is only required for registration-required webinars. This should be the email address registered for the webinar.
     */
    user_email?: string;
};

/**
 * * `web` - web
 * * `web_4_core` - web_4_core
 * * `web_gpu` - web_gpu
 * * `native` - native
 */
export type ZoomEnum = 'web' | 'web_4_core' | 'web_gpu' | 'native';

export type AutoAcceptAllInvites = {
    mode?: AutoAcceptAllInvitesModeEnum;
};

/**
 * * `auto_accept_all_invites` - auto_accept_all_invites
 */
export type AutoAcceptAllInvitesModeEnum = 'auto_accept_all_invites';

export type JoinMainRoom = {
    mode?: JoinMainRoomModeEnum;
};

/**
 * * `join_main_room` - join_main_room
 */
export type JoinMainRoomModeEnum = 'join_main_room';

export type JoinSpecificRoom = {
    mode?: JoinSpecificRoomModeEnum;
    room_id: string;
};

/**
 * * `join_specific_room` - join_specific_room
 */
export type JoinSpecificRoomModeEnum = 'join_specific_room';

export type PatchedBot = {
    readonly id?: string;
    /**
     * Structured meeting URL metadata for the associated platform.
     */
    meeting_url?: MeetingUrl;
    /**
     * The name of the bot that will be displayed in the call.
     * *(Note: Authenticated Google Meet bots will use the Google account name and this field will be ignored.)*
     */
    bot_name?: string;
    /**
     * The time at which the bot will join the call, formatted in ISO 8601. This field can only be read from scheduled bots that have not yet joined a call. Once a bot has joined a call, it's join_at will be cleared.
     */
    join_at?: string | null;
    /**
     * Configure the recording generated by the bot. Includes options for getting meeting transcript,  the layout of the recorded video, when to start recording and more.
     */
    recording_config?: BotRecordingConfig | null;
    readonly status_changes?: Array<BotEvent>;
    readonly recordings?: Array<BotRecordingEmbed>;
    /**
     * Settings for the bot output media.
     */
    output_media?: OutputMedia | null;
    /**
     * Settings for the bot to output video. Image should be 16:9. Recommended resolution is 640x360.
     */
    automatic_video_output?: AutomaticVideoOutput | null;
    /**
     * Settings for the bot to output audio.
     */
    automatic_audio_output?: AutomaticAudioOutput | null;
    /**
     * Settings for the bot to send chat messages.
     * *(Note: Chat functionality is only supported for Zoom, Google Meet and Microsoft Teams currently.)*
     */
    chat?: Chat | null;
    automatic_leave?: AutomaticLeave | null;
    /**
     * Configure bot variants per meeting platforms, e.g. {"zoom": "web_4_core"}.
     */
    variant?: BotVariant | null;
    /**
     * The calendar meetings associated with this bot. This field is **populated only for bots that are dispatched via Calendar V1 API integration**.
     */
    readonly calendar_meetings?: Array<BotCalendarMeeting>;
    /**
     * Zoom specific parameters
     */
    zoom?: Zoom | null;
    /**
     * Google Meet specific parameters
     */
    google_meet?: GoogleMeet | null;
    slack_team?: SlackTeamIntegrationMinimal | null;
    /**
     * Configure how the bot handles breakout rooms. Currently, Zoom is supported.Examples: {"mode": "join_main_room"} | {"mode": "join_specific_room", "room_id": "<uuid>"} | {"mode": "auto_accept_all_invites"} (default).
     */
    breakout_room?: BreakoutRoom | null;
    metadata?: {
        [key: string]: string | null;
    };
};

export type CalendarEvent = {
    readonly id: string;
    readonly start_time: string;
    readonly end_time: string;
    readonly calendar_id: string;
    readonly raw: unknown;
    readonly platform: string;
    readonly platform_id: string;
    readonly ical_uid: string;
    meeting_platform: MeetingPlatformEnum;
    readonly meeting_url: string | null;
    readonly created_at: string;
    readonly updated_at: string;
    readonly is_deleted: boolean;
    readonly bots: Array<CalendarEventBot>;
};

export type CalendarEventBot = {
    readonly bot_id: string;
    readonly start_time: string;
    readonly deduplication_key: string;
    readonly meeting_url: string;
};

/**
 * * `zoom` - Zoom
 * * `google_meet` - Meet
 * * `goto_meeting` - Goto
 * * `microsoft_teams` - Teams
 * * `microsoft_teams_live` - Teams Live
 * * `webex` - Webex
 * * `chime_sdk` - Chime Sdk
 * * `webrtc` - Webrtc
 * * `zoom_rtms` - Zoom Rtms
 * * `google_meet_media_api` - Google Meet Media Api
 * * `slack_authenticator` - Slack Authenticator
 * * `slack_huddle_observer` - Slack Huddle Observer
 */
export type MeetingPlatformEnum = 'zoom' | 'google_meet' | 'goto_meeting' | 'microsoft_teams' | 'microsoft_teams_live' | 'webex' | 'chime_sdk' | 'webrtc' | 'zoom_rtms' | 'google_meet_media_api' | 'slack_authenticator' | 'slack_huddle_observer';

export type PaginatedCalendarEventList = {
    next?: string | null;
    previous?: string | null;
    results?: Array<CalendarEvent>;
};

export type CalendarEventAddBot = {
    /**
     * Pass this key to deduplicate bots across multiple calendar events. Please ensure this remain consistent across all calendar events that mush share a single bot. For more details, refer to Calendar V2 scheduling guide.
     */
    deduplication_key: string;
    /**
     * The config object(JSON) to be passed to the bot. It supports all properties available in **[Create Bot request. ](https://docs.recall.ai/reference/bot_create)**
     *
     * `meeting_url` - automatically populated from the calendar event unless specified in bot_config.
     * `join_at` - automatically populated from the calendar event
     */
    bot_config: {
        [key: string]: unknown;
    };
};

export type Calendar = {
    readonly id: string;
    oauth_client_id: string;
    oauth_client_secret: string;
    oauth_refresh_token: string;
    platform: CalendarPlatformEnum;
    /**
     * @deprecated
     */
    webhook_url?: string;
    oauth_email?: string;
    readonly platform_email: string | null;
    readonly status: string;
    readonly status_changes: unknown;
    readonly created_at: string;
    readonly updated_at: string;
};

/**
 * * `google_calendar` - Google Calendar
 * * `microsoft_outlook` - Microsoft Outlook
 */
export type CalendarPlatformEnum = 'google_calendar' | 'microsoft_outlook';

export type PaginatedCalendarList = {
    next?: string | null;
    previous?: string | null;
    results?: Array<Calendar>;
};

export type CalendarAccessToken = {
    readonly token: string;
    readonly expires_at: string;
};

export type CalendarAccountAccessTokenError = {
    code: CalendarAccountAccessTokenErrorCodeEnum;
    message: string;
};

/**
 * * `no_oauth_credentials` - no_oauth_credentials
 * * `bad_refresh_token` - bad_refresh_token
 * * `error` - error
 */
export type CalendarAccountAccessTokenErrorCodeEnum = 'no_oauth_credentials' | 'bad_refresh_token' | 'error';

export type PatchedCalendar = {
    readonly id?: string;
    oauth_client_id?: string;
    oauth_client_secret?: string;
    oauth_refresh_token?: string;
    platform?: CalendarPlatformEnum;
    /**
     * @deprecated
     */
    webhook_url?: string;
    oauth_email?: string;
    readonly platform_email?: string | null;
    readonly status?: string;
    readonly status_changes?: unknown;
    readonly created_at?: string;
    readonly updated_at?: string;
};

export type BotMinimal = {
    readonly id: string;
    metadata: {
        [key: string]: string | null;
    };
};

export type DesktopSdkUploadMinimal = {
    readonly id: string;
    metadata: {
        [key: string]: string | null;
    };
};

export type PaginatedRecordingList = {
    next?: string | null;
    previous?: string | null;
    results?: Array<Recording>;
};

export type RealtimeEndpointMinimal = {
    readonly id: string;
    metadata: {
        [key: string]: string | null;
    };
};

export type Recording = {
    readonly id: string;
    readonly created_at: string;
    readonly started_at: string | null;
    readonly completed_at: string | null;
    status: RecordingStatus;
    media_shortcuts: RecordingShortcuts;
    readonly realtime_endpoints: Array<RealtimeEndpointMinimal>;
    bot: BotMinimal | null;
    desktop_sdk_upload: DesktopSdkUploadMinimal | null;
    readonly expires_at: string;
    metadata: {
        [key: string]: string | null;
    };
};

export type RecallaiAsyncTranscription = {
    /**
     * Must be `en` in low latency mode. Docs: https://docs.recall.ai/docs/recallai-transcription
     *
     * * `auto` - auto
     * * `bg` - bg
     * * `ca` - ca
     * * `cs` - cs
     * * `da` - da
     * * `de` - de
     * * `el` - el
     * * `en` - en
     * * `en_au` - en_au
     * * `en_uk` - en_uk
     * * `en_us` - en_us
     * * `es` - es
     * * `et` - et
     * * `fi` - fi
     * * `fr` - fr
     * * `hi` - hi
     * * `hu` - hu
     * * `id` - id
     * * `it` - it
     * * `ja` - ja
     * * `ko` - ko
     * * `lt` - lt
     * * `lv` - lv
     * * `ms` - ms
     * * `nl` - nl
     * * `no` - no
     * * `pl` - pl
     * * `pt` - pt
     * * `ro` - ro
     * * `ru` - ru
     * * `sk` - sk
     * * `sv` - sv
     * * `th` - th
     * * `tr` - tr
     * * `uk` - uk
     * * `vi` - vi
     * * `zh` - zh
     */
    language_code?: LanguageCodeEnum;
    /**
     * List of text strings to find/replace in the transcript.
     */
    spelling?: Array<RecallaiSpellingEntry>;
    /**
     * Increases the chances that these terms appear in the transcript over some sound-alikes.
     */
    key_terms?: Array<string>;
    filter_profanity?: boolean;
};

export type RecordingCreateTranscriptArtifact = {
    [key: string]: unknown;
};

export type RecordingCreateTranscriptArtifactProvider = {
    recallai_async?: RecallaiAsyncTranscription;
};

export type RecordingCreateTranscriptDiarization = {
    use_separate_streams_when_available?: boolean;
};

export type RecordingMinimal = {
    readonly id: string;
    metadata: {
        [key: string]: string | null;
    } | null;
};

export type TranscriptArtifact = {
    readonly id: string;
    recording: RecordingMinimal;
    readonly created_at: string;
    status: ArtifactStatus;
    metadata: {
        [key: string]: string | null;
    };
    data: TranscriptArtifactData;
    diarization: TranscriptArtifactDiarization | null;
    provider: TranscriptArtifactProvider;
};

export type PaginatedTranscriptArtifactList = {
    next?: string | null;
    previous?: string | null;
    results?: Array<TranscriptArtifact>;
};

export type PatchedTranscriptArtifact = {
    readonly id?: string;
    recording?: RecordingMinimal;
    readonly created_at?: string;
    status?: ArtifactStatus;
    metadata?: {
        [key: string]: string | null;
    };
    data?: TranscriptArtifactData;
    diarization?: TranscriptArtifactDiarization | null;
    provider?: TranscriptArtifactProvider;
};

export type AudioMixedArtifact = {
    readonly id: string;
    recording: RecordingMinimal;
    readonly created_at: string;
    status: ArtifactStatus;
    metadata: {
        [key: string]: string | null;
    };
    data: AudioMixedArtifactData;
    /**
     * Format of the mixed audio file
     *
     * * `mp3` - Mp3
     * * `raw` - Raw
     */
    format: Format33bEnum;
};

export type PaginatedAudioMixedArtifactList = {
    next?: string | null;
    previous?: string | null;
    results?: Array<AudioMixedArtifact>;
};

export type PatchedAudioMixedArtifact = {
    readonly id?: string;
    recording?: RecordingMinimal;
    readonly created_at?: string;
    status?: ArtifactStatus;
    metadata?: {
        [key: string]: string | null;
    };
    data?: AudioMixedArtifactData;
    /**
     * Format of the mixed audio file
     *
     * * `mp3` - Mp3
     * * `raw` - Raw
     */
    format?: Format33bEnum;
};

export type AudioSeparateArtifact = {
    readonly id: string;
    recording: RecordingMinimal;
    readonly created_at: string;
    status: ArtifactStatus;
    metadata: {
        [key: string]: string | null;
    };
    data: AudioSeparateArtifactData;
    /**
     * Format of the participant separated audio files
     *
     * * `raw` - Raw
     * * `ogg` - Ogg
     */
    format: AudioSeparateArtifactFormatEnum;
};

export type AudioSeparateArtifactData = {
    /**
     * Download all audio separate parts for the recording. **[See response format here](https://docs.recall.ai/docs/download-schemas#json-audio-separate-download-url)**
     */
    readonly download_url: string | null;
};

/**
 * * `raw` - Raw
 * * `ogg` - Ogg
 */
export type AudioSeparateArtifactFormatEnum = 'raw' | 'ogg';

export type PaginatedAudioSeparateArtifactList = {
    next?: string | null;
    previous?: string | null;
    results?: Array<AudioSeparateArtifact>;
};

export type PatchedAudioSeparateArtifact = {
    readonly id?: string;
    recording?: RecordingMinimal;
    readonly created_at?: string;
    status?: ArtifactStatus;
    metadata?: {
        [key: string]: string | null;
    };
    data?: AudioSeparateArtifactData;
    /**
     * Format of the participant separated audio files
     *
     * * `raw` - Raw
     * * `ogg` - Ogg
     */
    format?: AudioSeparateArtifactFormatEnum;
};

export type PaginatedVideoMixedArtifactList = {
    next?: string | null;
    previous?: string | null;
    results?: Array<VideoMixedArtifact>;
};

export type VideoMixedArtifact = {
    readonly id: string;
    recording: RecordingMinimal;
    readonly created_at: string;
    status: ArtifactStatus;
    metadata: {
        [key: string]: string | null;
    };
    data: VideoMixedArtifactData;
    /**
     * Format of the mixed video file
     *
     * * `mp4` - Mp4
     */
    format: Format3B3Enum;
};

export type PatchedVideoMixedArtifact = {
    readonly id?: string;
    recording?: RecordingMinimal;
    readonly created_at?: string;
    status?: ArtifactStatus;
    metadata?: {
        [key: string]: string | null;
    };
    data?: VideoMixedArtifactData;
    /**
     * Format of the mixed video file
     *
     * * `mp4` - Mp4
     */
    format?: Format3B3Enum;
};

export type PaginatedVideoSeparateArtifactList = {
    next?: string | null;
    previous?: string | null;
    results?: Array<VideoSeparateArtifact>;
};

export type VideoSeparateArtifact = {
    readonly id: string;
    recording: RecordingMinimal;
    readonly created_at: string;
    status: ArtifactStatus;
    metadata: {
        [key: string]: string | null;
    };
    data: VideoSeparateArtifactData;
    /**
     * Format of the participant separated video files
     *
     * * `mp4` - Mp4
     * * `h264` - H264
     */
    format: VideoSeparateArtifactFormatEnum;
};

export type VideoSeparateArtifactData = {
    /**
     * Download all video separate parts for the recording. **[See response format here](https://docs.recall.ai/docs/download-schemas#json-video-separate-download-url)**
     */
    readonly download_url: string | null;
};

/**
 * * `mp4` - Mp4
 * * `h264` - H264
 */
export type VideoSeparateArtifactFormatEnum = 'mp4' | 'h264';

export type PatchedVideoSeparateArtifact = {
    readonly id?: string;
    recording?: RecordingMinimal;
    readonly created_at?: string;
    status?: ArtifactStatus;
    metadata?: {
        [key: string]: string | null;
    };
    data?: VideoSeparateArtifactData;
    /**
     * Format of the participant separated video files
     *
     * * `mp4` - Mp4
     * * `h264` - H264
     */
    format?: VideoSeparateArtifactFormatEnum;
};

/**
 * Structured meeting URL metadata. The available fields depend on the meeting platform.
 */
export type MeetingUrl = {
    /**
     * Zoom meeting ID extracted from the meeting URL.
     */
    meeting_id: string;
    /**
     * Zoom meeting password component, if the meeting URL includes one.
     */
    meeting_password?: string | null;
    /**
     * Constant identifier for Zoom meetings.
     */
    platform: 'zoom';
} | {
    /**
     * Google Meet meeting ID.
     */
    meeting_id: string;
    /**
     * Constant identifier for Google Meet meetings.
     */
    platform: 'google_meet';
} | {
    /**
     * Teams meeting identifier. Certain Teams variants do not supply this field.
     */
    meeting_id: string | null;
    /**
     * Meeting password if included in the Teams URL.
     */
    meeting_password: string | null;
    /**
     * Organizer identifier embedded in the Teams URL.
     */
    organizer_id: string | null;
    /**
     * Tenant identifier embedded in the Teams URL.
     */
    tenant_id: string | null;
    /**
     * Message identifier embedded in the Teams URL.
     */
    message_id: string | null;
    /**
     * Thread identifier embedded in the Teams URL.
     */
    thread_id: string | null;
    /**
     * Business meeting identifier for Teams for Business URLs.
     */
    business_meeting_id: string | null;
    /**
     * Business meeting password for Teams for Business URLs when present.
     */
    business_meeting_password: string | null;
    /**
     * Distinguishes between Teams (business) and Teams Live meetings.
     */
    platform: 'microsoft_teams' | 'microsoft_teams_live';
} | {
    /**
     * Customer subdomain extracted from the Webex URL.
     */
    meeting_subdomain: string;
    /**
     * Webex meeting identifier (mtid).
     */
    meeting_mtid: string;
    /**
     * Webex meeting resource path.
     */
    meeting_path: string;
    /**
     * Constant identifier for Webex meetings.
     */
    platform: 'webex';
} | {
    /**
     * Customer subdomain extracted from the Webex URL.
     */
    meeting_subdomain: string;
    /**
     * Personal room identifier embedded in the Webex URL.
     */
    meeting_personal_room_id: string;
    /**
     * Constant identifier for Webex meetings.
     */
    platform: 'webex';
} | {
    /**
     * GoTo meeting identifier.
     */
    meeting_id: string;
    /**
     * Constant identifier for GoTo meetings.
     */
    platform: 'goto_meeting';
};

export type AudioMixedArtifactShortcutWritable = {
    metadata: {
        [key: string]: string | null;
    };
};

export type AudioOutputDataWritable = {
    /**
     * The kind of data encoded in b64_data
     *
     * * `mp3` - mp3
     */
    kind: AudioOutputDataKindEnum;
    /**
     * Data encoded in Base64 format, using the standard alphabet (specified here: https://datatracker.ietf.org/doc/html/rfc4648#section-4)
     */
    b64_data: string;
};

export type BotWritable = {
    /**
     * Canonical meeting URL string accepted when scheduling or updating bots.
     */
    meeting_url: string;
    /**
     * The name of the bot that will be displayed in the call.
     * *(Note: Authenticated Google Meet bots will use the Google account name and this field will be ignored.)*
     */
    bot_name?: string;
    /**
     * The time at which the bot will join the call, formatted in ISO 8601. This field can only be read from scheduled bots that have not yet joined a call. Once a bot has joined a call, it's join_at will be cleared.
     */
    join_at?: string | null;
    /**
     * Configure the recording generated by the bot. Includes options for getting meeting transcript,  the layout of the recorded video, when to start recording and more.
     */
    recording_config?: BotRecordingConfig | null;
    /**
     * Settings for the bot output media.
     */
    output_media?: OutputMedia | null;
    /**
     * Settings for the bot to output video. Image should be 16:9. Recommended resolution is 640x360.
     */
    automatic_video_output?: AutomaticVideoOutput | null;
    /**
     * Settings for the bot to output audio.
     */
    automatic_audio_output?: AutomaticAudioOutput | null;
    /**
     * Settings for the bot to send chat messages.
     * *(Note: Chat functionality is only supported for Zoom, Google Meet and Microsoft Teams currently.)*
     */
    chat?: Chat | null;
    automatic_leave?: AutomaticLeave | null;
    /**
     * Configure bot variants per meeting platforms, e.g. {"zoom": "web_4_core"}.
     */
    variant?: BotVariant | null;
    /**
     * Zoom specific parameters
     */
    zoom?: Zoom | null;
    /**
     * Google Meet specific parameters
     */
    google_meet?: GoogleMeet | null;
    /**
     * Configure how the bot handles breakout rooms. Currently, Zoom is supported.Examples: {"mode": "join_main_room"} | {"mode": "join_specific_room", "room_id": "<uuid>"} | {"mode": "auto_accept_all_invites"} (default).
     */
    breakout_room?: BreakoutRoom | null;
    metadata?: {
        [key: string]: string | null;
    };
};

export type BotRecordingEmbedWritable = {
    started_at?: string | null;
    completed_at?: string | null;
    expires_at?: string | null;
    metadata?: {
        [key: string]: string | null;
    } | null;
};

export type MeetingMetadataArtifactDataWritable = {
    [key: string]: unknown;
};

export type MeetingMetadataArtifactShortcutWritable = {
    metadata: {
        [key: string]: string | null;
    };
};

export type ParticipantEventsArtifactShortcutWritable = {
    metadata: {
        [key: string]: string | null;
    };
};

export type RecordingShortcutsWritable = {
    [key: string]: unknown;
};

export type SlackTeamIntegrationMinimalWritable = {
    metadata: {
        [key: string]: string | null;
    };
};

export type TranscriptArtifactProviderWritable = {
    /**
     * AssemblyAi async chunked transcription Settings
     *
     * Docs: https://www.assemblyai.com/docs/api-reference/transcripts/submit
     */
    assembly_ai_async_chunked?: {
        /**
         * The language of your audio file. Possible values are found in [Supported Languages](https://www.assemblyai.com/docs/concepts/supported-languages).
         * The default value is 'en_us'.
         *
         */
        language_code?: 'en' | 'en_au' | 'en_uk' | 'en_us' | 'es' | 'fr' | 'de' | 'it' | 'pt' | 'nl' | 'af' | 'sq' | 'am' | 'ar' | 'hy' | 'as' | 'az' | 'ba' | 'eu' | 'be' | 'bn' | 'bs' | 'br' | 'bg' | 'my' | 'ca' | 'zh' | 'hr' | 'cs' | 'da' | 'et' | 'fo' | 'fi' | 'gl' | 'ka' | 'el' | 'gu' | 'ht' | 'ha' | 'haw' | 'he' | 'hi' | 'hu' | 'is' | 'id' | 'ja' | 'jw' | 'kn' | 'kk' | 'km' | 'ko' | 'lo' | 'la' | 'lv' | 'ln' | 'lt' | 'lb' | 'mk' | 'mg' | 'ms' | 'ml' | 'mt' | 'mi' | 'mr' | 'mn' | 'ne' | 'no' | 'nn' | 'oc' | 'pa' | 'ps' | 'fa' | 'pl' | 'ro' | 'ru' | 'sa' | 'sr' | 'sn' | 'sd' | 'si' | 'sk' | 'sl' | 'so' | 'su' | 'sw' | 'sv' | 'tl' | 'tg' | 'ta' | 'tt' | 'te' | 'th' | 'bo' | 'tr' | 'tk' | 'uk' | 'ur' | 'uz' | 'vi' | 'cy' | 'yi' | 'yo' | string | null;
        /**
         * Enable [Automatic language detection](https://www.assemblyai.com/docs/models/speech-recognition#automatic-language-detection), either true or false.
         */
        language_detection?: boolean;
        /**
         * The confidence threshold for the automatically detected language.
         * An error will be returned if the language confidence is below this threshold.
         * Defaults to 0.
         *
         */
        language_confidence_threshold?: number;
        /**
         * The speech model to use for the transcription. When `null`, the "best" model is used.
         */
        speech_model?: 'best' | 'nano' | 'slam-1' | 'universal' | string | null;
        /**
         * Enable Automatic Punctuation, can be true or false
         */
        punctuate?: boolean;
        /**
         * Enable Text Formatting, can be true or false
         */
        format_text?: boolean;
        /**
         * Transcribe Filler Words, like "umm", in your media file; can be true or false
         */
        disfluencies?: boolean;
        /**
         * Enable [Multichannel](https://www.assemblyai.com/docs/models/speech-recognition#multichannel-transcription) transcription, can be true or false.
         */
        multichannel?: boolean;
        /**
         * Enable [Dual Channel](https://www.assemblyai.com/docs/models/speech-recognition#dual-channel-transcription) transcription, can be true or false.
         *
         * @deprecated
         */
        dual_channel?: boolean;
        /**
         * Enable Key Phrases, either true or false
         */
        auto_highlights?: boolean;
        /**
         * The point in time, in milliseconds, to begin transcribing in your media file
         */
        audio_start_from?: number;
        /**
         * The point in time, in milliseconds, to stop transcribing in your media file
         */
        audio_end_at?: number;
        /**
         * The list of custom vocabulary to boost transcription probability for
         *
         * @deprecated
         */
        word_boost?: Array<string>;
        /**
         * How much to boost specified words
         */
        boost_param?: 'low' | 'default' | 'high';
        /**
         * Filter profanity from the transcribed text, can be true or false
         */
        filter_profanity?: boolean;
        /**
         * Redact PII from the transcribed text using the Redact PII model, can be true or false
         */
        redact_pii?: boolean;
        /**
         * Generate a copy of the original media file with spoken PII "beeped" out, can be true or false. See [PII redaction](https://www.assemblyai.com/docs/models/pii-redaction) for more details.
         */
        redact_pii_audio?: boolean;
        /**
         * Controls the filetype of the audio created by redact_pii_audio. Currently supports mp3 (default) and wav. See [PII redaction](https://www.assemblyai.com/docs/models/pii-redaction) for more details.
         */
        redact_pii_audio_quality?: 'mp3' | 'wav';
        /**
         * The list of PII Redaction policies to enable. See [PII redaction](https://www.assemblyai.com/docs/models/pii-redaction) for more details.
         */
        redact_pii_policies?: Array<'account_number' | 'banking_information' | 'blood_type' | 'credit_card_cvv' | 'credit_card_expiration' | 'credit_card_number' | 'date' | 'date_interval' | 'date_of_birth' | 'drivers_license' | 'drug' | 'duration' | 'email_address' | 'event' | 'filename' | 'gender_sexuality' | 'healthcare_number' | 'injury' | 'ip_address' | 'language' | 'location' | 'marital_status' | 'medical_condition' | 'medical_process' | 'money_amount' | 'nationality' | 'number_sequence' | 'occupation' | 'organization' | 'passport_number' | 'password' | 'person_age' | 'person_name' | 'phone_number' | 'physical_attribute' | 'political_affiliation' | 'religion' | 'statistics' | 'time' | 'url' | 'us_social_security_number' | 'username' | 'vehicle_id' | 'zodiac_sign'>;
        /**
         * The replacement logic for detected PII, can be "entity_type" or "hash". See [PII redaction](https://www.assemblyai.com/docs/models/pii-redaction) for more details.
         */
        redact_pii_sub?: 'entity_name' | 'hash' | string | null;
        /**
         * Enable [Speaker diarization](https://www.assemblyai.com/docs/models/speaker-diarization), can be true or false
         */
        speaker_labels?: boolean;
        /**
         * Tells the speaker label model how many speakers it should attempt to identify, up to 10. See [Speaker diarization](https://www.assemblyai.com/docs/models/speaker-diarization) for more details.
         */
        speakers_expected?: number | null;
        /**
         * Enable [Content Moderation](https://www.assemblyai.com/docs/models/content-moderation), can be true or false
         */
        content_safety?: boolean;
        /**
         * The confidence threshold for the Content Moderation model. Values must be between 25 and 100.
         */
        content_safety_confidence?: number;
        /**
         * Enable [Topic Detection](https://www.assemblyai.com/docs/models/topic-detection), can be true or false
         */
        iab_categories?: boolean;
        /**
         * Customize how words are spelled and formatted using to and from values
         */
        custom_spelling?: Array<{
            /**
             * Words or phrases to replace
             */
            from: Array<string>;
            /**
             * Word to replace with
             */
            to: string;
        }>;
        /**
         * <Warning>`keyterms_prompt` is only supported when the `speech_model` is specified as `slam-1`</Warning>
         * Improve accuracy with up to 1000 domain-specific words or phrases (maximum 6 words per phrase).
         *
         */
        keyterms_prompt?: Array<string>;
        /**
         * This parameter does not currently have any functionality attached to it.
         *
         * @deprecated
         */
        prompt?: string;
        /**
         * Enable [Sentiment Analysis](https://www.assemblyai.com/docs/models/sentiment-analysis), can be true or false
         */
        sentiment_analysis?: boolean;
        /**
         * Enable [Auto Chapters](https://www.assemblyai.com/docs/models/auto-chapters), can be true or false
         */
        auto_chapters?: boolean;
        /**
         * Enable [Entity Detection](https://www.assemblyai.com/docs/models/entity-detection), can be true or false
         */
        entity_detection?: boolean;
        /**
         * Reject audio files that contain less than this fraction of speech.
         * Valid values are in the range [0, 1] inclusive.
         *
         */
        speech_threshold?: number | null;
        /**
         * Enable [Summarization](https://www.assemblyai.com/docs/models/summarization), can be true or false
         */
        summarization?: boolean;
        /**
         * The model to summarize the transcript
         */
        summary_model?: 'informative' | 'conversational' | 'catchy';
        /**
         * The type of summary
         */
        summary_type?: 'bullets' | 'bullets_verbose' | 'gist' | 'headline' | 'paragraph';
        /**
         * Enable custom topics, either true or false
         *
         * @deprecated
         */
        custom_topics?: boolean;
        /**
         * The list of custom topics
         */
        topics?: Array<string>;
    };
    /**
     * AssemblyAi Real-time Transcription Settings
     *
     * Docs: https://www.assemblyai.com/docs/api-reference/streaming-api/streaming-api
     */
    assembly_ai_v3_streaming?: {
        /**
         * Whether to return formatted final transcripts
         */
        format_turns?: boolean;
        /**
         * The confidence threshold (0.0 to 1.0) to use when determining if the end of a turn has been reached
         */
        end_of_turn_confidence_threshold?: unknown;
        /**
         * The minimum amount of silence in milliseconds required to detect end of turn when confident
         */
        min_end_of_turn_silence_when_confident?: unknown;
        /**
         * The maximum amount of silence in milliseconds allowed in a turn before end of turn is triggered
         */
        max_turn_silence?: unknown;
    };
    /**
     * Recall.ai Real-time Transcription Settings
     *
     * Docs: https://docs.recall.ai/docs/recallai-transcription
     */
    recallai_streaming?: RecallaiStreamingTranscription;
    /**
     * Deepgram Real-time Transcription Settings
     *
     * Docs: https://developers.deepgram.com/reference/streaming
     */
    deepgram_streaming?: {
        /**
         * Defaults to `false`. Recognize speaker changes. Each word in the transcript will be assigned a speaker number starting at 0
         */
        diarize?: 'true' | 'false';
        /**
         * Identify and extract key entities from content in submitted audio
         */
        dictation?: 'true' | 'false';
        /**
         * Indicates how long Deepgram will wait to detect whether a speaker has finished speaking or pauses for a significant period of time. When set to a value, the streaming endpoint immediately finalizes the transcription for the processed time range and returns the transcript with a speech_final parameter set to true. Can also be set to false to disable endpointing
         */
        endpointing?: unknown;
        /**
         * Arbitrary key-value pairs that are attached to the API response for usage in downstream processing
         */
        extra?: unknown;
        /**
         * Filler Words can help transcribe interruptions in your audio, like "uh" and "um"
         */
        filler_words?: 'true' | 'false';
        /**
         * Specifies whether the streaming endpoint should provide ongoing transcription updates as more audio is received. When set to true, the endpoint sends continuous updates, meaning transcription results may evolve over time
         */
        interim_results?: 'true' | 'false';
        /**
         * Key term prompting can boost or suppress specialized terminology and brands. Only compatible with Nova-3
         */
        keyterm?: unknown;
        /**
         * Keywords can boost or suppress specialized terminology and brands
         */
        keywords?: unknown;
        /**
         * The [BCP-47 language tag](https://tools.ietf.org/html/bcp47) that hints at the primary spoken language. Depending on the Model you choose only certain languages are available
         */
        language?: 'bg' | 'ca' | 'cs' | 'da' | 'da-DK' | 'de' | 'de-CH' | 'el' | 'en' | 'en-AU' | 'en-GB' | 'en-IN' | 'en-NZ' | 'en-US' | 'es' | 'es-419' | 'es-LATAM' | 'et' | 'fi' | 'fr' | 'fr-CA' | 'hi' | 'hi-Latn' | 'hu' | 'id' | 'it' | 'ja' | 'ko' | 'ko-KR' | 'lt' | 'lv' | 'ms' | 'multi' | 'nl' | 'nl-BE' | 'no' | 'pl' | 'pt' | 'pt-BR' | 'pt-PT' | 'ro' | 'ru' | 'sk' | 'sv' | 'sv-SE' | 'taq' | 'th' | 'th-TH' | 'tr' | 'uk' | 'vi' | 'zh' | 'zh-CN' | 'zh-HK' | 'zh-Hans' | 'zh-Hant' | 'zh-TW';
        /**
         * Opts out requests from the Deepgram Model Improvement Program. Refer to our Docs for pricing impacts before setting this to true. https://dpgr.am/deepgram-mip
         */
        mip_opt_out?: unknown;
        /**
         * AI model to use for the transcription
         */
        model?: 'nova-3' | 'nova-3-general' | 'nova-3-medical' | 'nova-2' | 'nova-2-general' | 'nova-2-meeting' | 'nova-2-finance' | 'nova-2-conversationalai' | 'nova-2-voicemail' | 'nova-2-video' | 'nova-2-medical' | 'nova-2-drivethru' | 'nova-2-automotive' | 'nova' | 'nova-general' | 'nova-phonecall' | 'nova-medical' | 'enhanced' | 'enhanced-general' | 'enhanced-meeting' | 'enhanced-phonecall' | 'enhanced-finance' | 'base' | 'meeting' | 'phonecall' | 'finance' | 'conversationalai' | 'voicemail' | 'video' | 'custom';
        /**
         * Transcribe each audio channel independently
         */
        multichannel?: 'true' | 'false';
        /**
         * Convert numbers from written format to numerical format
         */
        numerals?: 'true' | 'false';
        /**
         * Profanity Filter looks for recognized profanity and converts it to the nearest recognized non-profane word or removes it from the transcript completely
         */
        profanity_filter?: 'true' | 'false';
        /**
         * Add punctuation and capitalization to the transcript
         */
        punctuate?: 'true' | 'false';
        /**
         * Redaction removes sensitive information from your transcripts
         */
        redact?: 'true' | 'false' | 'pci' | 'numbers' | 'aggressive_numbers' | 'ssn';
        /**
         * Search for terms or phrases in submitted audio and replaces them
         */
        replace?: unknown;
        /**
         * Search for terms or phrases in submitted audio
         */
        search?: unknown;
        /**
         * Apply formatting to transcript output. When set to true, additional formatting will be applied to transcripts to improve readability
         */
        smart_format?: 'true' | 'false';
        /**
         * Label your requests for the purpose of identification during usage reporting
         */
        tag?: unknown;
        /**
         * Indicates how long Deepgram will wait to send an UtteranceEnd message after a word has been transcribed. Use with interim_results
         */
        utterance_end_ms?: unknown;
        /**
         * Indicates that speech has started. You'll begin receiving Speech Started messages upon speech starting
         */
        vad_events?: 'true' | 'false';
        /**
         * Version of an AI model to use
         */
        version?: unknown;
    };
    /**
     * GladiaV2 Real-time Transcription Settings
     *
     * Docs: https://docs.gladia.io/api-reference/v2/live/init
     */
    gladia_v2_streaming?: {
        /**
         * The model used to process the audio. "solaria-1" is used by default.
         */
        model?: 'solaria-1';
        /**
         * The endpointing duration in seconds. Endpointing is the duration of silence which will cause an utterance to be considered as finished
         */
        endpointing?: number;
        /**
         * The maximum duration in seconds without endpointing. If endpointing is not detected after this duration, current utterance will be considered as finished
         */
        maximum_duration_without_endpointing?: number;
        /**
         * Specify the language configuration
         */
        language_config?: {
            /**
             * If one language is set, it will be used for the transcription. Otherwise, language will be auto-detected by the model.
             */
            languages?: Array<'af' | 'sq' | 'am' | 'ar' | 'hy' | 'as' | 'az' | 'ba' | 'eu' | 'be' | 'bn' | 'bs' | 'br' | 'bg' | 'ca' | 'zh' | 'hr' | 'cs' | 'da' | 'nl' | 'en' | 'et' | 'fo' | 'fi' | 'fr' | 'gl' | 'ka' | 'de' | 'el' | 'gu' | 'ht' | 'ha' | 'haw' | 'he' | 'hi' | 'hu' | 'is' | 'id' | 'it' | 'ja' | 'jv' | 'kn' | 'kk' | 'km' | 'ko' | 'lo' | 'la' | 'lv' | 'ln' | 'lt' | 'lb' | 'mk' | 'mg' | 'ms' | 'ml' | 'mt' | 'mi' | 'mr' | 'mn' | 'mymr' | 'ne' | 'no' | 'nn' | 'oc' | 'ps' | 'fa' | 'pl' | 'pt' | 'pa' | 'ro' | 'ru' | 'sa' | 'sr' | 'sn' | 'sd' | 'si' | 'sk' | 'sl' | 'so' | 'es' | 'su' | 'sw' | 'sv' | 'tl' | 'tg' | 'ta' | 'tt' | 'te' | 'th' | 'bo' | 'tr' | 'tk' | 'uk' | 'ur' | 'uz' | 'vi' | 'cy' | 'yi' | 'yo' | 'jp'>;
            /**
             * If true, language will be auto-detected on each utterance. Otherwise, language will be auto-detected on first utterance and then used for the rest of the transcription. If one language is set, this option will be ignored.
             */
            code_switching?: boolean;
        };
        /**
         * Specify the pre-processing configuration
         */
        pre_processing?: {
            /**
             * If true, apply pre-processing to the audio stream to enhance the quality.
             */
            audio_enhancer?: boolean;
            /**
             * Sensitivity configuration for Speech Threshold. A value close to 1 will apply stricter thresholds, making it less likely to detect background sounds as speech.
             */
            speech_threshold?: number;
        };
        /**
         * Specify the realtime processing configuration
         */
        realtime_processing?: {
            /**
             * If true, enable custom vocabulary for the transcription.
             */
            custom_vocabulary?: boolean;
            /**
             * Custom vocabulary configuration, if `custom_vocabulary` is enabled
             */
            custom_vocabulary_config?: {
                /**
                 * Specific vocabulary list to feed the transcription model with. Each item can be a string or an object with the following properties: value, intensity, pronunciations, language.
                 */
                vocabulary: Array<{
                    /**
                     * The text used to replace in the transcription.
                     */
                    value: string;
                    /**
                     * The global intensity of the feature.
                     */
                    intensity?: number;
                    /**
                     * The pronunciations used in the transcription.
                     */
                    pronunciations?: Array<string>;
                    /**
                     * Specify the language in which it will be pronounced when sound comparison occurs. Default to transcription language.
                     */
                    language?: string;
                } | string>;
                /**
                 * Default intensity for the custom vocabulary
                 */
                default_intensity?: number;
            };
            /**
             * If true, enable custom spelling for the transcription.
             */
            custom_spelling?: boolean;
            /**
             * If true, enable translation for the transcription
             */
            translation?: boolean;
            /**
             * Translation configuration, if `translation` is enabled
             */
            translation_config?: {
                target_languages: Array<'af' | 'sq' | 'am' | 'ar' | 'hy' | 'as' | 'ast' | 'az' | 'ba' | 'eu' | 'be' | 'bn' | 'bs' | 'br' | 'bg' | 'my' | 'ca' | 'ceb' | 'zh' | 'hr' | 'cs' | 'da' | 'nl' | 'en' | 'et' | 'fo' | 'fi' | 'fr' | 'fy' | 'ff' | 'gd' | 'gl' | 'lg' | 'ka' | 'de' | 'el' | 'gu' | 'ht' | 'ha' | 'haw' | 'he' | 'hi' | 'hu' | 'is' | 'ig' | 'ilo' | 'id' | 'ga' | 'it' | 'ja' | 'jp' | 'jv' | 'kn' | 'kk' | 'km' | 'ko' | 'lo' | 'la' | 'lv' | 'ln' | 'lt' | 'lb' | 'mk' | 'mg' | 'ms' | 'ml' | 'mt' | 'mi' | 'mr' | 'mo' | 'mn' | 'mymr' | 'ne' | 'no' | 'nn' | 'oc' | 'or' | 'pa' | 'ps' | 'fa' | 'pl' | 'pt' | 'ro' | 'ru' | 'sa' | 'sr' | 'sn' | 'sd' | 'si' | 'sk' | 'sl' | 'so' | 'es' | 'su' | 'sw' | 'ss' | 'sv' | 'tl' | 'tg' | 'ta' | 'tt' | 'te' | 'th' | 'bo' | 'tn' | 'tr' | 'tk' | 'uk' | 'ur' | 'uz' | 'vi' | 'cy' | 'wo' | 'xh' | 'yi' | 'yo' | 'zu'>;
                /**
                 * Model you want the translation model to use to translate
                 */
                model?: 'base' | 'enhanced';
                /**
                 * Align translated utterances with the original ones
                 */
                match_original_utterances?: boolean;
                /**
                 * Whether to apply lipsync to the translated transcription.
                 */
                lipsync?: boolean;
                /**
                 * Enables or disables context-aware translation features that allow the model to adapt translations based on provided context.
                 */
                context_adaptation?: boolean;
                /**
                 * Context information to improve translation accuracy
                 */
                context?: string;
                /**
                 * Forces the translation to use informal language forms when available in the target language.
                 */
                informal?: boolean;
            };
            /**
             * If true, enable named entity recognition for the transcription.
             */
            named_entity_recognition?: boolean;
            /**
             * If true, enable sentiment analysis for the transcription.
             */
            sentiment_analysis?: boolean;
        };
        /**
         * Specify the post-processing configuration
         */
        post_processing?: {
            /**
             * If true, generates summarization for the whole transcription.
             */
            summarization?: boolean;
            /**
             * Summarization configuration, if `summarization` is enabled
             */
            summarization_config?: {
                /**
                 * The type of summarization to apply
                 */
                type?: 'general' | 'bullet_points' | 'concise';
            };
            /**
             * If true, generates chapters for the whole transcription.
             */
            chapterization?: boolean;
        };
        /**
         * Specify the websocket messages configuration
         */
        messages_config?: {
            /**
             * If true, final utterance will be sent to websocket.
             */
            receive_final_transcripts?: boolean;
            /**
             * If true, begin and end speech events will be sent to websocket.
             */
            receive_speech_events?: boolean;
            /**
             * If true, pre-processing events will be sent to websocket.
             */
            receive_pre_processing_events?: boolean;
            /**
             * If true, realtime processing events will be sent to websocket.
             */
            receive_realtime_processing_events?: boolean;
            /**
             * If true, post-processing events will be sent to websocket.
             */
            receive_post_processing_events?: boolean;
            /**
             * If true, acknowledgments will be sent to websocket.
             */
            receive_acknowledgments?: boolean;
            /**
             * If true, errors will be sent to websocket.
             */
            receive_errors?: boolean;
            /**
             * If true, lifecycle events will be sent to websocket.
             */
            receive_lifecycle_events?: boolean;
        };
        /**
         * If true, messages will be sent to configured url.
         */
        callback?: boolean;
        /**
         * Specify the callback configuration
         */
        callback_config?: {
            /**
             * URL on which we will do a `POST` request with configured messages
             */
            url?: string;
            /**
             * If true, final utterance will be sent to the defined callback.
             */
            receive_final_transcripts?: boolean;
            /**
             * If true, begin and end speech events will be sent to the defined callback.
             */
            receive_speech_events?: boolean;
            /**
             * If true, pre-processing events will be sent to the defined callback.
             */
            receive_pre_processing_events?: boolean;
            /**
             * If true, realtime processing events will be sent to the defined callback.
             */
            receive_realtime_processing_events?: boolean;
            /**
             * If true, post-processing events will be sent to the defined callback.
             */
            receive_post_processing_events?: boolean;
            /**
             * If true, acknowledgments will be sent to the defined callback.
             */
            receive_acknowledgments?: boolean;
            /**
             * If true, errors will be sent to the defined callback.
             */
            receive_errors?: boolean;
            /**
             * If true, lifecycle events will be sent to the defined callback.
             */
            receive_lifecycle_events?: boolean;
        };
    };
    /**
     * Rev Real-time Transcription Settings
     *
     * Docs: https://docs.rev.ai/api/streaming/requests/
     */
    rev_streaming?: {
        obscure_expletives?: boolean;
        delete_after?: string;
        'audio_options._content_type'?: string;
        'audio_options._layout'?: string;
        'audio_options._rate'?: number;
        'audio_options._format'?: string;
        'audio_options._channels'?: number;
        transcriber?: 0 | 1;
        language?: string;
        metadata?: string;
        filter_profanity?: boolean;
        remove_disfluencies?: boolean;
        detailed_partials?: boolean;
        custom_vocabulary_id?: string;
        delete_after_seconds?: number;
        max_segment_duration_seconds?: number;
        max_connection_wait_seconds?: number;
        allow_interruption?: boolean;
        enable_speaker_switch?: boolean;
        start_ts?: string;
        skip_postprocessing?: boolean;
        priority?: 0 | 1;
        user_agent?: string;
    };
    aws_transcribe_streaming?: (unknown | {
        identify_language: true;
    }) & {
        /**
         * Specify the language code that represents the language spoken. If you're unsure of the language spoken in your audio, consider using IdentifyLanguage to enable automatic language identification.
         */
        language_code?: string;
        /**
         * Specify how you want your vocabulary filter applied to your transcript. To replace words with ***, choose mask. To delete words, choose remove. To flag words without changing them, choose tag.
         */
        vocabulary_filter_method?: string;
        /**
         * Specify the name of the custom vocabulary filter that you want to use when processing your transcription. Note that vocabulary filter names are case sensitive.  If you use Amazon Transcribe in multiple Regions, the vocabulary filter must be available in Amazon Transcribe in each Region. If you include IdentifyLanguage and want to use one or more vocabulary filters with your transcription, use the VocabularyFilterNames parameter instead.
         */
        vocabulary_filter_name?: string;
        /**
         * Specify the name of the custom vocabulary that you want to use when processing your transcription. Note that vocabulary names are case sensitive. If you use Amazon Transcribe multiple Regions, the vocabulary must be available in Amazon Transcribe in each Region. If you include IdentifyLanguage and want to use one or more custom vocabularies with your transcription, use the VocabularyNames parameter instead.
         */
        vocabulary_name?: string;
        /**
         * The Amazon Web Services Region in which to use Amazon Transcribe. If you don't specify a Region, then the MediaRegion of the meeting is used. However, if Amazon Transcribe is not available in the MediaRegion, then a TranscriptFailed event is sent. Use auto to use Amazon Transcribe in a Region near the meeting’s MediaRegion. For more information, refer to Choosing a transcription Region in the Amazon Chime SDK Developer Guide.
         */
        region?: string;
        /**
         * Enables partial result stabilization for your transcription. Partial result stabilization can reduce latency in your output, but may impact accuracy.
         */
        enable_partial_results_stabilization?: boolean;
        /**
         * Specify the level of stability to use when you enable partial results stabilization (EnablePartialResultsStabilization). Low stability provides the highest accuracy. High stability transcribes faster, but with slightly lower accuracy.
         */
        partial_results_stability?: string;
        /**
         * Labels all personally identifiable information (PII) identified in your transcript. If you don't include PiiEntityTypes, all PII is identified.  You can’t set ContentIdentificationType and ContentRedactionType.
         */
        content_identification_type?: string;
        /**
         * Content redaction is performed at the segment level. If you don't include PiiEntityTypes, all PII is redacted.  You can’t set ContentRedactionType and ContentIdentificationType.
         */
        content_redaction_type?: string;
        /**
         * Specify which types of personally identifiable information (PII) you want to redact in your transcript. You can include as many types as you'd like, or you can select ALL. Values must be comma-separated and can include: ADDRESS, BANK_ACCOUNT_NUMBER, BANK_ROUTING, CREDIT_DEBIT_CVV, CREDIT_DEBIT_EXPIRY CREDIT_DEBIT_NUMBER, EMAIL,NAME, PHONE, PIN, SSN, or ALL. Note that if you include PiiEntityTypes, you must also include ContentIdentificationType or ContentRedactionType. If you include ContentRedactionType or ContentIdentificationType, but do not include PiiEntityTypes, all PII is redacted or identified.
         */
        pii_entity_types?: string;
        /**
         * Specify the name of the custom language model that you want to use when processing your transcription. Note that language model names are case sensitive. The language of the specified language model must match the language code. If the languages don't match, the custom language model isn't applied. There are no errors or warnings associated with a language mismatch. If you use Amazon Transcribe in multiple Regions, the custom language model must be available in Amazon Transcribe in each Region.
         */
        language_model_name?: string;
        /**
         * Enables automatic language identification for your transcription. If you include IdentifyLanguage, you can optionally use LanguageOptions to include a list of language codes that you think may be present in your audio stream. Including language options can improve transcription accuracy. You can also use PreferredLanguage to include a preferred language. Doing so can help Amazon Transcribe identify the language faster. You must include either LanguageCode or IdentifyLanguage. Language identification can't be combined with custom language models or redaction.
         */
        identify_language?: boolean;
        /**
         * Specify two or more language codes that represent the languages you think may be present in your media; including more than five is not recommended. If you're unsure what languages are present, do not include this parameter. Including language options can improve the accuracy of language identification. If you include LanguageOptions, you must also include IdentifyLanguage.  You can only include one language dialect per language. For example, you cannot include en-US and en-AU.
         */
        language_options?: string;
        /**
         * Specify a preferred language from the subset of languages codes you specified in LanguageOptions. You can only use this parameter if you include IdentifyLanguage and LanguageOptions.
         */
        preferred_language?: string;
        /**
         * Specify the names of the custom vocabularies that you want to use when processing your transcription. Note that vocabulary names are case sensitive. If you use Amazon Transcribe in multiple Regions, the vocabulary must be available in Amazon Transcribe in each Region. If you don't include IdentifyLanguage and want to use a custom vocabulary with your transcription, use the VocabularyName parameter instead.
         */
        vocabulary_names?: string;
        /**
         * Specify the names of the custom vocabulary filters that you want to use when processing your transcription. Note that vocabulary filter names are case sensitive. If you use Amazon Transcribe in multiple Regions, the vocabulary filter must be available in Amazon Transcribe in each Region.  If you're not including IdentifyLanguage and want to use a custom vocabulary filter with your transcription, use the VocabularyFilterName parameter instead.
         */
        vocabulary_filter_names?: string;
    };
    /**
     * Speechmatics Real-time Transcription Settings
     *
     * You must specify `language` (e.g `en`)
     *
     * Docs: https://docs.speechmatics.com/rt-api-ref#transcription-config
     */
    speechmatics_streaming?: {
        language: string;
        /**
         * Request a specialized model based on 'language' but optimized for a particular field, e.g. "finance" or "medical".
         */
        domain?: string;
        output_locale?: string;
        additional_vocab?: Array<string | {
            content: string;
            sounds_like?: Array<string>;
        }>;
        diarization?: 'none' | 'speaker';
        max_delay?: number;
        max_delay_mode?: 'flexible' | 'fixed';
        speaker_diarization_config?: {
            max_speakers?: number;
            prefer_current_speaker?: boolean;
            speaker_sensitivity?: number;
        };
        audio_filtering_config?: {
            volume_threshold?: number;
        };
        transcript_filtering_config?: {
            remove_disfluencies?: boolean;
            replacements?: Array<{
                from: string;
                to: string;
            }>;
        };
        enable_partials?: boolean;
        enable_entities?: boolean;
        operating_point?: 'standard' | 'enhanced';
        punctuation_overrides?: {
            /**
             * The punctuation marks which the client is prepared to accept in transcription output, or the special value 'all' (the default). Unsupported marks are ignored. This value is used to guide the transcription process.
             */
            permitted_marks?: Array<string>;
            /**
             * Ranges between zero and one. Higher values will produce more punctuation. The default is 0.5.
             */
            sensitivity?: number;
        };
        /**
         * This mode will detect when a speaker has stopped talking. The end_of_utterance_silence_trigger is the time in seconds after which the server will assume that the speaker has finished speaking, and will emit an EndOfUtterance message. A value of 0 disables the feature.
         */
        conversation_config?: {
            end_of_utterance_silence_trigger?: number;
        };
    };
};

export type TranscriptArtifactShortcutWritable = {
    metadata: {
        [key: string]: string | null;
    };
};

export type VideoMixedArtifactShortcutWritable = {
    metadata: {
        [key: string]: string | null;
    };
};

export type VideoOutputWritable = {
    /**
     * The kind of data encoded in b64_data
     *
     * * `jpeg` - jpeg
     */
    kind: VideoOutputKindEnum;
    /**
     * Data encoded in Base64 format, using the standard alphabet (specified here: https://datatracker.ietf.org/doc/html/rfc4648#section-4)
     */
    b64_data: string;
};

export type PatchedBotWritable = {
    /**
     * Canonical meeting URL string accepted when scheduling or updating bots.
     */
    meeting_url?: string;
    /**
     * The name of the bot that will be displayed in the call.
     * *(Note: Authenticated Google Meet bots will use the Google account name and this field will be ignored.)*
     */
    bot_name?: string;
    /**
     * The time at which the bot will join the call, formatted in ISO 8601. This field can only be read from scheduled bots that have not yet joined a call. Once a bot has joined a call, it's join_at will be cleared.
     */
    join_at?: string | null;
    /**
     * Configure the recording generated by the bot. Includes options for getting meeting transcript,  the layout of the recorded video, when to start recording and more.
     */
    recording_config?: BotRecordingConfig | null;
    /**
     * Settings for the bot output media.
     */
    output_media?: OutputMedia | null;
    /**
     * Settings for the bot to output video. Image should be 16:9. Recommended resolution is 640x360.
     */
    automatic_video_output?: AutomaticVideoOutput | null;
    /**
     * Settings for the bot to output audio.
     */
    automatic_audio_output?: AutomaticAudioOutput | null;
    /**
     * Settings for the bot to send chat messages.
     * *(Note: Chat functionality is only supported for Zoom, Google Meet and Microsoft Teams currently.)*
     */
    chat?: Chat | null;
    automatic_leave?: AutomaticLeave | null;
    /**
     * Configure bot variants per meeting platforms, e.g. {"zoom": "web_4_core"}.
     */
    variant?: BotVariant | null;
    /**
     * Zoom specific parameters
     */
    zoom?: Zoom | null;
    /**
     * Google Meet specific parameters
     */
    google_meet?: GoogleMeet | null;
    /**
     * Configure how the bot handles breakout rooms. Currently, Zoom is supported.Examples: {"mode": "join_main_room"} | {"mode": "join_specific_room", "room_id": "<uuid>"} | {"mode": "auto_accept_all_invites"} (default).
     */
    breakout_room?: BreakoutRoom | null;
    metadata?: {
        [key: string]: string | null;
    };
};

export type PaginatedCalendarEventListWritable = {
    next?: string | null;
    previous?: string | null;
    results?: Array<unknown>;
};

export type CalendarWritable = {
    oauth_client_id: string;
    oauth_client_secret: string;
    oauth_refresh_token: string;
    platform: CalendarPlatformEnum;
    /**
     * @deprecated
     */
    webhook_url?: string;
    oauth_email?: string;
};

export type PatchedCalendarWritable = {
    oauth_client_id?: string;
    oauth_client_secret?: string;
    oauth_refresh_token?: string;
    platform?: CalendarPlatformEnum;
    /**
     * @deprecated
     */
    webhook_url?: string;
    oauth_email?: string;
};

export type BotMinimalWritable = {
    metadata: {
        [key: string]: string | null;
    };
};

export type DesktopSdkUploadMinimalWritable = {
    metadata: {
        [key: string]: string | null;
    };
};

export type RealtimeEndpointMinimalWritable = {
    metadata: {
        [key: string]: string | null;
    };
};

export type RecordingWritable = {
    metadata: {
        [key: string]: string | null;
    };
};

export type RecordingCreateTranscriptArtifactWritable = {
    metadata?: {
        [key: string]: string | null;
    };
    diarization?: RecordingCreateTranscriptDiarization;
    provider: RecordingCreateTranscriptArtifactProviderWritable;
};

export type RecordingCreateTranscriptArtifactProviderWritable = {
    /**
     * The parameters for creating a transcript
     */
    assembly_ai_async?: {
        /**
         * The language of your audio file. Possible values are found in [Supported Languages](https://www.assemblyai.com/docs/concepts/supported-languages).
         * The default value is 'en_us'.
         *
         */
        language_code?: 'en' | 'en_au' | 'en_uk' | 'en_us' | 'es' | 'fr' | 'de' | 'it' | 'pt' | 'nl' | 'af' | 'sq' | 'am' | 'ar' | 'hy' | 'as' | 'az' | 'ba' | 'eu' | 'be' | 'bn' | 'bs' | 'br' | 'bg' | 'my' | 'ca' | 'zh' | 'hr' | 'cs' | 'da' | 'et' | 'fo' | 'fi' | 'gl' | 'ka' | 'el' | 'gu' | 'ht' | 'ha' | 'haw' | 'he' | 'hi' | 'hu' | 'is' | 'id' | 'ja' | 'jw' | 'kn' | 'kk' | 'km' | 'ko' | 'lo' | 'la' | 'lv' | 'ln' | 'lt' | 'lb' | 'mk' | 'mg' | 'ms' | 'ml' | 'mt' | 'mi' | 'mr' | 'mn' | 'ne' | 'no' | 'nn' | 'oc' | 'pa' | 'ps' | 'fa' | 'pl' | 'ro' | 'ru' | 'sa' | 'sr' | 'sn' | 'sd' | 'si' | 'sk' | 'sl' | 'so' | 'su' | 'sw' | 'sv' | 'tl' | 'tg' | 'ta' | 'tt' | 'te' | 'th' | 'bo' | 'tr' | 'tk' | 'uk' | 'ur' | 'uz' | 'vi' | 'cy' | 'yi' | 'yo' | string | null;
        /**
         * Enable [Automatic language detection](https://www.assemblyai.com/docs/models/speech-recognition#automatic-language-detection), either true or false.
         */
        language_detection?: boolean;
        /**
         * The confidence threshold for the automatically detected language.
         * An error will be returned if the language confidence is below this threshold.
         * Defaults to 0.
         *
         */
        language_confidence_threshold?: number;
        /**
         * The speech model to use for the transcription. When `null`, the "best" model is used.
         */
        speech_model?: 'best' | 'nano' | 'slam-1' | 'universal' | string | null;
        /**
         * Enable Automatic Punctuation, can be true or false
         */
        punctuate?: boolean;
        /**
         * Enable Text Formatting, can be true or false
         */
        format_text?: boolean;
        /**
         * Transcribe Filler Words, like "umm", in your media file; can be true or false
         */
        disfluencies?: boolean;
        /**
         * Enable [Multichannel](https://www.assemblyai.com/docs/models/speech-recognition#multichannel-transcription) transcription, can be true or false.
         */
        multichannel?: boolean;
        /**
         * Enable [Dual Channel](https://www.assemblyai.com/docs/models/speech-recognition#dual-channel-transcription) transcription, can be true or false.
         *
         * @deprecated
         */
        dual_channel?: boolean;
        /**
         * The URL to which we send webhook requests.
         * We sends two different types of webhook requests.
         * One request when a transcript is completed or failed, and one request when the redacted audio is ready if redact_pii_audio is enabled.
         *
         */
        webhook_url?: string;
        /**
         * The header name to be sent with the transcript completed or failed webhook requests
         */
        webhook_auth_header_name?: string | null;
        /**
         * The header value to send back with the transcript completed or failed webhook requests for added security
         */
        webhook_auth_header_value?: string | null;
        /**
         * Enable Key Phrases, either true or false
         */
        auto_highlights?: boolean;
        /**
         * The point in time, in milliseconds, to begin transcribing in your media file
         */
        audio_start_from?: number;
        /**
         * The point in time, in milliseconds, to stop transcribing in your media file
         */
        audio_end_at?: number;
        /**
         * The list of custom vocabulary to boost transcription probability for
         *
         * @deprecated
         */
        word_boost?: Array<string>;
        /**
         * How much to boost specified words
         */
        boost_param?: 'low' | 'default' | 'high';
        /**
         * Filter profanity from the transcribed text, can be true or false
         */
        filter_profanity?: boolean;
        /**
         * Redact PII from the transcribed text using the Redact PII model, can be true or false
         */
        redact_pii?: boolean;
        /**
         * Generate a copy of the original media file with spoken PII "beeped" out, can be true or false. See [PII redaction](https://www.assemblyai.com/docs/models/pii-redaction) for more details.
         */
        redact_pii_audio?: boolean;
        /**
         * Controls the filetype of the audio created by redact_pii_audio. Currently supports mp3 (default) and wav. See [PII redaction](https://www.assemblyai.com/docs/models/pii-redaction) for more details.
         */
        redact_pii_audio_quality?: 'mp3' | 'wav';
        /**
         * The list of PII Redaction policies to enable. See [PII redaction](https://www.assemblyai.com/docs/models/pii-redaction) for more details.
         */
        redact_pii_policies?: Array<'account_number' | 'banking_information' | 'blood_type' | 'credit_card_cvv' | 'credit_card_expiration' | 'credit_card_number' | 'date' | 'date_interval' | 'date_of_birth' | 'drivers_license' | 'drug' | 'duration' | 'email_address' | 'event' | 'filename' | 'gender_sexuality' | 'healthcare_number' | 'injury' | 'ip_address' | 'language' | 'location' | 'marital_status' | 'medical_condition' | 'medical_process' | 'money_amount' | 'nationality' | 'number_sequence' | 'occupation' | 'organization' | 'passport_number' | 'password' | 'person_age' | 'person_name' | 'phone_number' | 'physical_attribute' | 'political_affiliation' | 'religion' | 'statistics' | 'time' | 'url' | 'us_social_security_number' | 'username' | 'vehicle_id' | 'zodiac_sign'>;
        /**
         * The replacement logic for detected PII, can be "entity_type" or "hash". See [PII redaction](https://www.assemblyai.com/docs/models/pii-redaction) for more details.
         */
        redact_pii_sub?: 'entity_name' | 'hash' | string | null;
        /**
         * Enable [Speaker diarization](https://www.assemblyai.com/docs/models/speaker-diarization), can be true or false
         */
        speaker_labels?: boolean;
        /**
         * Tells the speaker label model how many speakers it should attempt to identify, up to 10. See [Speaker diarization](https://www.assemblyai.com/docs/models/speaker-diarization) for more details.
         */
        speakers_expected?: number | null;
        /**
         * Enable [Content Moderation](https://www.assemblyai.com/docs/models/content-moderation), can be true or false
         */
        content_safety?: boolean;
        /**
         * The confidence threshold for the Content Moderation model. Values must be between 25 and 100.
         */
        content_safety_confidence?: number;
        /**
         * Enable [Topic Detection](https://www.assemblyai.com/docs/models/topic-detection), can be true or false
         */
        iab_categories?: boolean;
        /**
         * Customize how words are spelled and formatted using to and from values
         */
        custom_spelling?: Array<{
            /**
             * Words or phrases to replace
             */
            from: Array<string>;
            /**
             * Word to replace with
             */
            to: string;
        }>;
        /**
         * <Warning>`keyterms_prompt` is only supported when the `speech_model` is specified as `slam-1`</Warning>
         * Improve accuracy with up to 1000 domain-specific words or phrases (maximum 6 words per phrase).
         *
         */
        keyterms_prompt?: Array<string>;
        /**
         * This parameter does not currently have any functionality attached to it.
         *
         * @deprecated
         */
        prompt?: string;
        /**
         * Enable [Sentiment Analysis](https://www.assemblyai.com/docs/models/sentiment-analysis), can be true or false
         */
        sentiment_analysis?: boolean;
        /**
         * Enable [Auto Chapters](https://www.assemblyai.com/docs/models/auto-chapters), can be true or false
         */
        auto_chapters?: boolean;
        /**
         * Enable [Entity Detection](https://www.assemblyai.com/docs/models/entity-detection), can be true or false
         */
        entity_detection?: boolean;
        /**
         * Reject audio files that contain less than this fraction of speech.
         * Valid values are in the range [0, 1] inclusive.
         *
         */
        speech_threshold?: number | null;
        /**
         * Enable [Summarization](https://www.assemblyai.com/docs/models/summarization), can be true or false
         */
        summarization?: boolean;
        /**
         * The model to summarize the transcript
         */
        summary_model?: 'informative' | 'conversational' | 'catchy';
        /**
         * The type of summary
         */
        summary_type?: 'bullets' | 'bullets_verbose' | 'gist' | 'headline' | 'paragraph';
        /**
         * Enable custom topics, either true or false
         *
         * @deprecated
         */
        custom_topics?: boolean;
        /**
         * The list of custom topics
         */
        topics?: Array<string>;
    };
    deepgram_async?: {
        custom_topic?: string | Array<string>;
        custom_topic_mode?: 'extended' | 'strict';
        custom_intent?: string | Array<string>;
        custom_intent_mode?: 'extended' | 'strict';
        detect_entities?: boolean;
        detect_language?: boolean | Array<string>;
        diarize?: boolean;
        dictation?: boolean;
        encoding?: 'linear16' | 'flac' | 'mulaw' | 'amr-nb' | 'amr-wb' | 'opus' | 'speex' | 'g729';
        extra?: string | Array<string>;
        filler_words?: boolean;
        intents?: boolean;
        keyterm?: Array<string>;
        keywords?: string | Array<string>;
        language?: 'bg' | 'ca' | 'zh' | 'zh-CN' | 'zh-TW' | 'zh-HK' | 'zh-Hans' | 'zh-Hant' | 'cs' | 'da' | 'da-DK' | 'nl' | 'nl-BE' | 'en' | 'en-US' | 'en-AU' | 'en-GB' | 'en-NZ' | 'en-IN' | 'et' | 'fi' | 'fr' | 'fr-CA' | 'de' | 'de-CH' | 'el' | 'hi' | 'hi-Latn' | 'hu' | 'id' | 'it' | 'ja' | 'ko' | 'ko-KR' | 'lv' | 'lt' | 'ms' | 'multi' | 'no' | 'pl' | 'pt' | 'pt-BR' | 'pt-PT' | 'ro' | 'ru' | 'sk' | 'es' | 'es-419' | 'es-LATAM' | 'sv' | 'sv-SE' | 'taq' | 'th' | 'th-TH' | 'tr' | 'uk' | 'vi';
        measurements?: boolean;
        mip_opt_out?: boolean;
        model?: 'nova-3' | 'nova-3-general' | 'nova-3-medical' | 'nova-2' | 'nova-2-general' | 'nova-2-meeting' | 'nova-2-finance' | 'nova-2-conversationalai' | 'nova-2-voicemail' | 'nova-2-video' | 'nova-2-medical' | 'nova-2-drivethru' | 'nova-2-automotive' | 'nova' | 'nova-general' | 'nova-phonecall' | 'nova-medical' | 'enhanced' | 'enhanced-general' | 'enhanced-meeting' | 'enhanced-phonecall' | 'enhanced-finance' | 'base' | 'meeting' | 'phonecall' | 'finance' | 'conversationalai' | 'voicemail' | 'video' | string;
        multichannel?: boolean;
        numerals?: boolean;
        paragraphs?: boolean;
        profanity_filter?: boolean;
        punctuate?: boolean;
        redact?: string | Array<'pci' | 'pii' | 'numbers'>;
        replace?: string | Array<string>;
        search?: string | Array<string>;
        sentiment?: boolean;
        smart_format?: boolean;
        summarize?: 'v1' | 'v2' | 'true' | 'false';
        tag?: string | Array<string>;
        topics?: boolean;
        utterances?: boolean;
        utt_split?: number;
        version?: 'latest' | string;
    };
    gladia_v2_async?: {
        /**
         * **[Deprecated]** Context to feed the transcription model with for possible better accuracy
         *
         * @deprecated
         */
        context_prompt?: string;
        /**
         * **[Beta]** Can be either boolean to enable custom_vocabulary for this audio or an array with specific vocabulary list to feed the transcription model with
         */
        custom_vocabulary?: boolean;
        /**
         * **[Beta]** Custom vocabulary configuration, if `custom_vocabulary` is enabled
         */
        custom_vocabulary_config?: {
            /**
             * Specific vocabulary list to feed the transcription model with. Each item can be a string or an object with the following properties: value, intensity, pronunciations, language.
             */
            vocabulary: Array<{
                /**
                 * The text used to replace in the transcription.
                 */
                value: string;
                /**
                 * The global intensity of the feature.
                 */
                intensity?: number;
                /**
                 * The pronunciations used in the transcription.
                 */
                pronunciations?: Array<string>;
                /**
                 * Specify the language in which it will be pronounced when sound comparison occurs. Default to transcription language.
                 */
                language?: string;
            } | string>;
            /**
             * Default intensity for the custom vocabulary
             */
            default_intensity?: number;
        };
        /**
         * **[Deprecated]** Use `language_config` instead. Detect the language from the given audio
         *
         * @deprecated
         */
        detect_language?: boolean;
        /**
         * **[Deprecated]** Use `language_config` instead.Detect multiple languages in the given audio
         *
         * @deprecated
         */
        enable_code_switching?: boolean;
        /**
         * **[Deprecated]** Use `language_config` instead. Specify the configuration for code switching
         *
         * @deprecated
         */
        code_switching_config?: {
            /**
             * Specify the languages you want to use when detecting multiple languages
             */
            languages?: Array<'af' | 'sq' | 'am' | 'ar' | 'hy' | 'as' | 'az' | 'ba' | 'eu' | 'be' | 'bn' | 'bs' | 'br' | 'bg' | 'ca' | 'zh' | 'hr' | 'cs' | 'da' | 'nl' | 'en' | 'et' | 'fo' | 'fi' | 'fr' | 'gl' | 'ka' | 'de' | 'el' | 'gu' | 'ht' | 'ha' | 'haw' | 'he' | 'hi' | 'hu' | 'is' | 'id' | 'it' | 'ja' | 'jv' | 'kn' | 'kk' | 'km' | 'ko' | 'lo' | 'la' | 'lv' | 'ln' | 'lt' | 'lb' | 'mk' | 'mg' | 'ms' | 'ml' | 'mt' | 'mi' | 'mr' | 'mn' | 'mymr' | 'ne' | 'no' | 'nn' | 'oc' | 'ps' | 'fa' | 'pl' | 'pt' | 'pa' | 'ro' | 'ru' | 'sa' | 'sr' | 'sn' | 'sd' | 'si' | 'sk' | 'sl' | 'so' | 'es' | 'su' | 'sw' | 'sv' | 'tl' | 'tg' | 'ta' | 'tt' | 'te' | 'th' | 'bo' | 'tr' | 'tk' | 'uk' | 'ur' | 'uz' | 'vi' | 'cy' | 'yi' | 'yo' | 'jp'>;
        };
        /**
         * **[Deprecated]** Use `language_config` instead. Set the spoken language for the given audio (ISO 639 standard)
         *
         * @deprecated
         */
        language?: 'af' | 'sq' | 'am' | 'ar' | 'hy' | 'as' | 'az' | 'ba' | 'eu' | 'be' | 'bn' | 'bs' | 'br' | 'bg' | 'ca' | 'zh' | 'hr' | 'cs' | 'da' | 'nl' | 'en' | 'et' | 'fo' | 'fi' | 'fr' | 'gl' | 'ka' | 'de' | 'el' | 'gu' | 'ht' | 'ha' | 'haw' | 'he' | 'hi' | 'hu' | 'is' | 'id' | 'it' | 'ja' | 'jv' | 'kn' | 'kk' | 'km' | 'ko' | 'lo' | 'la' | 'lv' | 'ln' | 'lt' | 'lb' | 'mk' | 'mg' | 'ms' | 'ml' | 'mt' | 'mi' | 'mr' | 'mn' | 'mymr' | 'ne' | 'no' | 'nn' | 'oc' | 'ps' | 'fa' | 'pl' | 'pt' | 'pa' | 'ro' | 'ru' | 'sa' | 'sr' | 'sn' | 'sd' | 'si' | 'sk' | 'sl' | 'so' | 'es' | 'su' | 'sw' | 'sv' | 'tl' | 'tg' | 'ta' | 'tt' | 'te' | 'th' | 'bo' | 'tr' | 'tk' | 'uk' | 'ur' | 'uz' | 'vi' | 'cy' | 'yi' | 'yo' | 'jp';
        /**
         * **[Deprecated]** Use `callback`/`callback_config` instead. Callback URL we will do a `POST` request to with the result of the transcription
         *
         * @deprecated
         */
        callback_url?: string;
        /**
         * Enable callback for this transcription. If true, the `callback_config` property will be used to customize the callback behaviour
         */
        callback?: boolean;
        /**
         * Customize the callback behaviour (url and http method)
         */
        callback_config?: {
            /**
             * The URL to be called with the result of the transcription
             */
            url: string;
            /**
             * The HTTP method to be used. Allowed values are `POST` or `PUT` (default: `POST`)
             */
            method?: 'POST' | 'PUT';
        };
        /**
         * Enable subtitles generation for this transcription
         */
        subtitles?: boolean;
        /**
         * Configuration for subtitles generation if `subtitles` is enabled
         */
        subtitles_config?: {
            formats?: Array<'srt' | 'vtt'>;
            /**
             * Minimum duration of a subtitle in seconds
             */
            minimum_duration?: number;
            /**
             * Maximum duration of a subtitle in seconds
             */
            maximum_duration?: number;
            /**
             * Maximum number of characters per row in a subtitle
             */
            maximum_characters_per_row?: number;
            /**
             * Maximum number of rows per caption
             */
            maximum_rows_per_caption?: number;
            /**
             * Style of the subtitles. Compliance mode refers to : https://loc.gov/preservation/digital/formats//fdd/fdd000569.shtml#:~:text=SRT%20files%20are%20basic%20text,alongside%2C%20example%3A%20%22MyVideo123
             */
            style?: 'default' | 'compliance';
        };
        /**
         * Enable speaker recognition (diarization) for this audio
         */
        diarization?: boolean;
        /**
         * Speaker recognition configuration, if `diarization` is enabled
         */
        diarization_config?: {
            /**
             * Exact number of speakers in the audio
             */
            number_of_speakers?: number;
            /**
             * Minimum number of speakers in the audio
             */
            min_speakers?: number;
            /**
             * Maximum number of speakers in the audio
             */
            max_speakers?: number;
            /**
             * **[Alpha]** Use enhanced diarization for this audio
             */
            enhanced?: boolean;
        };
        /**
         * **[Beta]** Enable translation for this audio
         */
        translation?: boolean;
        /**
         * **[Beta]** Translation configuration, if `translation` is enabled
         */
        translation_config?: {
            target_languages: Array<'af' | 'sq' | 'am' | 'ar' | 'hy' | 'as' | 'ast' | 'az' | 'ba' | 'eu' | 'be' | 'bn' | 'bs' | 'br' | 'bg' | 'my' | 'ca' | 'ceb' | 'zh' | 'hr' | 'cs' | 'da' | 'nl' | 'en' | 'et' | 'fo' | 'fi' | 'fr' | 'fy' | 'ff' | 'gd' | 'gl' | 'lg' | 'ka' | 'de' | 'el' | 'gu' | 'ht' | 'ha' | 'haw' | 'he' | 'hi' | 'hu' | 'is' | 'ig' | 'ilo' | 'id' | 'ga' | 'it' | 'ja' | 'jp' | 'jv' | 'kn' | 'kk' | 'km' | 'ko' | 'lo' | 'la' | 'lv' | 'ln' | 'lt' | 'lb' | 'mk' | 'mg' | 'ms' | 'ml' | 'mt' | 'mi' | 'mr' | 'mo' | 'mn' | 'mymr' | 'ne' | 'no' | 'nn' | 'oc' | 'or' | 'pa' | 'ps' | 'fa' | 'pl' | 'pt' | 'ro' | 'ru' | 'sa' | 'sr' | 'sn' | 'sd' | 'si' | 'sk' | 'sl' | 'so' | 'es' | 'su' | 'sw' | 'ss' | 'sv' | 'tl' | 'tg' | 'ta' | 'tt' | 'te' | 'th' | 'bo' | 'tn' | 'tr' | 'tk' | 'uk' | 'ur' | 'uz' | 'vi' | 'cy' | 'wo' | 'xh' | 'yi' | 'yo' | 'zu'>;
            /**
             * Model you want the translation model to use to translate
             */
            model?: 'base' | 'enhanced';
            /**
             * Align translated utterances with the original ones
             */
            match_original_utterances?: boolean;
            /**
             * Whether to apply lipsync to the translated transcription.
             */
            lipsync?: boolean;
            /**
             * Enables or disables context-aware translation features that allow the model to adapt translations based on provided context.
             */
            context_adaptation?: boolean;
            /**
             * Context information to improve translation accuracy
             */
            context?: string;
            /**
             * Forces the translation to use informal language forms when available in the target language.
             */
            informal?: boolean;
        };
        /**
         * **[Beta]** Enable summarization for this audio
         */
        summarization?: boolean;
        /**
         * **[Beta]** Summarization configuration, if `summarization` is enabled
         */
        summarization_config?: {
            /**
             * The type of summarization to apply
             */
            type?: 'general' | 'bullet_points' | 'concise';
        };
        /**
         * **[Alpha]** Enable moderation for this audio
         */
        moderation?: boolean;
        /**
         * **[Alpha]** Enable named entity recognition for this audio
         */
        named_entity_recognition?: boolean;
        /**
         * **[Alpha]** Enable chapterization for this audio
         */
        chapterization?: boolean;
        /**
         * **[Alpha]** Enable names consistency for this audio
         */
        name_consistency?: boolean;
        /**
         * **[Alpha]** Enable custom spelling for this audio
         */
        custom_spelling?: boolean;
        /**
         * **[Alpha]** Enable structured data extraction for this audio
         */
        structured_data_extraction?: boolean;
        /**
         * **[Alpha]** Structured data extraction configuration, if `structured_data_extraction` is enabled
         */
        structured_data_extraction_config?: {
            /**
             * The list of classes to extract from the audio transcription
             */
            classes: Array<Array<string>>;
        };
        /**
         * **[Alpha]** Enable sentiment analysis for this audio
         */
        sentiment_analysis?: boolean;
        /**
         * **[Alpha]** Enable audio to llm processing for this audio
         */
        audio_to_llm?: boolean;
        /**
         * **[Alpha]** Audio to llm configuration, if `audio_to_llm` is enabled
         */
        audio_to_llm_config?: {
            /**
             * The list of prompts applied on the audio transcription
             */
            prompts: Array<Array<string>>;
        };
        /**
         * Enable sentences for this audio
         */
        sentences?: boolean;
        /**
         * **[Alpha]** Allows to change the output display_mode for this audio. The output will be reordered, creating new utterances when speakers overlapped
         */
        display_mode?: boolean;
        /**
         * **[Alpha]** Use enhanced punctuation for this audio
         */
        punctuation_enhanced?: boolean;
        /**
         * Specify the language configuration
         */
        language_config?: {
            /**
             * If one language is set, it will be used for the transcription. Otherwise, language will be auto-detected by the model.
             */
            languages?: Array<'af' | 'sq' | 'am' | 'ar' | 'hy' | 'as' | 'az' | 'ba' | 'eu' | 'be' | 'bn' | 'bs' | 'br' | 'bg' | 'ca' | 'zh' | 'hr' | 'cs' | 'da' | 'nl' | 'en' | 'et' | 'fo' | 'fi' | 'fr' | 'gl' | 'ka' | 'de' | 'el' | 'gu' | 'ht' | 'ha' | 'haw' | 'he' | 'hi' | 'hu' | 'is' | 'id' | 'it' | 'ja' | 'jv' | 'kn' | 'kk' | 'km' | 'ko' | 'lo' | 'la' | 'lv' | 'ln' | 'lt' | 'lb' | 'mk' | 'mg' | 'ms' | 'ml' | 'mt' | 'mi' | 'mr' | 'mn' | 'mymr' | 'ne' | 'no' | 'nn' | 'oc' | 'ps' | 'fa' | 'pl' | 'pt' | 'pa' | 'ro' | 'ru' | 'sa' | 'sr' | 'sn' | 'sd' | 'si' | 'sk' | 'sl' | 'so' | 'es' | 'su' | 'sw' | 'sv' | 'tl' | 'tg' | 'ta' | 'tt' | 'te' | 'th' | 'bo' | 'tr' | 'tk' | 'uk' | 'ur' | 'uz' | 'vi' | 'cy' | 'yi' | 'yo' | 'jp'>;
            /**
             * If true, language will be auto-detected on each utterance. Otherwise, language will be auto-detected on first utterance and then used for the rest of the transcription. If one language is set, this option will be ignored.
             */
            code_switching?: boolean;
        };
    };
    recallai_async?: RecallaiAsyncTranscription;
    rev_async?: {
        /**
         * Optional metadata for the job
         */
        metadata?: string | null;
        /**
         * Optional setting for the number of Seconds after job completion
         * when the job should be auto-deleted
         */
        delete_after?: string | null;
        /**
         * Optional language setting for foreign languages
         */
        language?: string | null;
        /**
         * Optional setting for turning on/off diarization.
         * If not set, we will assume the value is false.
         */
        skip_diarization?: boolean | null;
        /**
         * Optional setting for turning on/off postprocessing.
         * If not set, we will assume the value is false.
         */
        skip_postprocessing?: boolean | null;
        /**
         * Optional setting for passing in custom vocabularies.
         */
        custom_vocabularies?: Array<{
            phrases: Array<string>;
        }> | null;
        /**
         * The id of the prebuilt custom vocabulary job
         */
        custom_vocabulary_id?: string | null;
        /**
         * If true, only exact phrases submitted in the Rev.Ai.Api.Models.SubmitJobOptions.CustomVocabularies option will be used as custom vocabulary,
         * i.e. phrases will not be split into individual words for processing.
         * Defaults to true if Rev.Ai.Api.Models.SubmitJobOptions.CustomVocabularies is set.
         */
        strict_custom_vocabulary?: boolean | null;
        /**
         * Optional setting for disabling punctuation.
         * If unset, the value is assumed to be false.
         */
        skip_punctuation?: boolean | null;
        /**
         * Optional setting for removing disfluencies
         * If unset the value is assumed to be false
         */
        remove_disfluencies?: boolean | null;
        /**
         * Optional setting for removing atmospherics
         * If unset the value is assumed to be false
         */
        remove_atmospherics?: boolean | null;
        /**
         * Optional setting for removing profanities
         * If unset the value is assumed to be false
         */
        filter_profanity?: boolean | null;
        /**
         * Optional setting for adding data classification labels
         * If unset the value is assumed to be false
         */
        add_data_labels?: boolean | null;
        /**
         * Optional setting for redacting certain data labels
         * If unset the value is assumed to be false
         */
        enable_redaction?: boolean | null;
        /**
         * Optional speaker channels count used to indicate how many individual channels
         * to transcribe for a given audio
         */
        speaker_channels_count?: number | null;
        /**
         * Optional count of speakers in the audio to improve diarization accuracy
         */
        speakers_count?: number | null;
        /**
         * Optional count of alternatives to generate
         */
        alternatives_count?: number | null;
        deletion_length_penalty?: number | null;
        /**
         * Optional chunk size to be sent to Revspeech for processing
         */
        chunk_size?: string | null;
        transcriber?: 0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 | 10 | 11 | 12 | 13 | 14 | 15 | 16 | 17;
        /**
         * Instructs Revver to transcribe audio with all details including disfluencies and other verbal interactions
         * such as laughter
         */
        verbatim?: boolean | null;
        /**
         * Whether human order should be rushed at a greater cost to the customer
         */
        rush?: boolean | null;
        /**
         * Specific segments of the file to be transcribed by a human
         */
        segments_to_transcribe?: Array<{
            start: string;
            end: string;
        }> | null;
        /**
         * Whether human order is test mode and should return a dummy transcript
         */
        test_mode?: boolean | null;
        /**
         * Speaker names for human transcription
         */
        speaker_names?: Array<{
            display_name?: string;
        }> | null;
        /**
         * Whether to predict topics while performing speech rec
         */
        predict_topics?: boolean | null;
        /**
         * Returns the top n words of a transcript. Defaults to 0
         */
        top_nwords?: number | null;
        /**
         * Whether improved alignment should be used with transcription
         */
        forced_alignment?: boolean | null;
        /**
         * Whether transcription should be done with fusion
         */
        enable_fusion?: boolean | null;
        domain?: 0 | 1;
        /**
         * If set adds one second of silence to the audio at the end
         */
        apply_duration_padding?: boolean | null;
        diarization_type?: 0 | 10 | 20;
        /**
         * Summarization options.
         */
        summarization_config?: {
            /**
             * User defined prompt.
             */
            prompt?: string | null;
            /**
             * Summarization model options for Rev.ai API
             */
            model?: 0 | 1;
            /**
             * Summarization formatting options.
             */
            type?: 0 | 1;
        };
        /**
         * Caption options.
         */
        captions_config?: {
            [key: string]: never;
        };
        /**
         * Options for translation as part of Async job request
         */
        translation_config?: {
            /**
             * Target languages
             */
            target_languages?: Array<{
                /**
                 * Defines the type of models supported to translated the content
                 */
                model?: 0 | 1;
                /**
                 * Target language
                 */
                language: string;
            }>;
        };
    };
    /**
     * JSON object that contains various groups of job configuration
     * parameters. Based on the value of `type`, a type-specific object
     * such as `transcription_config` is required to be present to
     * specify all configuration settings or parameters needed to
     * process the job inputs as expected.
     *
     * If the results of the job are to be forwarded on completion,
     * `notification_config` can be provided with a list of callbacks
     * to be made; no assumptions should be made about the order in
     * which they will occur.
     *
     * Customer specific job details or metadata can be supplied in
     * `tracking`, and this information will be available where
     * possible in the job results and in callbacks.
     *
     */
    speechmatics_async?: {
        alignment_config?: {
            language: string;
        };
        transcription_config?: {
            /**
             * Language model to process the audio input, normally specified as an ISO language code
             */
            language: string;
            /**
             * Request a specialized model based on 'language' but optimized for a particular field, e.g. "finance" or "medical".
             */
            domain?: string;
            /**
             * Language locale to be used when generating the transcription output, normally specified as an ISO language code
             */
            output_locale?: string;
            operating_point?: 'standard' | 'enhanced';
            /**
             * List of custom words or phrases that should be recognized. Alternative pronunciations can be specified to aid recognition.
             */
            additional_vocab?: Array<{
                content: string;
                sounds_like?: Array<string>;
            }>;
            /**
             * Control punctuation settings.
             */
            punctuation_overrides?: {
                /**
                 * Ranges between zero and one. Higher values will produce more punctuation. The default is 0.5.
                 */
                sensitivity?: number;
                /**
                 * The punctuation marks which the client is prepared to accept in transcription output, or the special value 'all' (the default). Unsupported marks are ignored. This value is used to guide the transcription process.
                 */
                permitted_marks?: Array<string>;
            };
            /**
             * Specify whether speaker or channel labels are added to the transcript.
             * The default is `none`.
             * - **none**: no speaker or channel labels are added.
             * - **speaker**: speaker attribution is performed based on acoustic matching;
             * all input channels are mixed into a single stream for processing.
             * - **channel**: multiple input channels are processed individually and collated
             * into a single transcript.
             */
            diarization?: 'none' | 'speaker' | 'channel';
            /**
             * Transcript labels to use when using collating separate input channels.
             */
            channel_diarization_labels?: Array<string>;
            /**
             * Include additional 'entity' objects in the transcription results (e.g. dates, numbers) and their original spoken form. These entities are interleaved with other types of results. The concatenation of these words is represented as a single entity with the concatenated written form present in the 'content' field. The entities contain a 'spoken_form' field, which can be used in place of the corresponding 'word' type results, in case a spoken form is preferred to a written form. They also contain a 'written_form', which can be used instead of the entity, if you want a breakdown of the words without spaces. They can still contain non-breaking spaces and other special whitespace characters, as they are considered part of the word for the formatting output. In case of a written_form, the individual word times are estimated and might not be accurate if the order of the words in the written form does not correspond to the order they were actually spoken (such as 'one hundred million dollars' and '$100 million').
             */
            enable_entities?: boolean;
            /**
             * Whether or not to enable flexible endpointing and allow the entity to continue to be spoken.
             */
            max_delay_mode?: 'fixed' | 'flexible';
            /**
             * Configuration for applying filtering to the transcription
             */
            transcript_filtering_config?: {
                /**
                 * If true, words that are identified as disfluencies will be removed from the transcript. If false (default), they are tagged in the transcript as 'disfluency'.
                 */
                remove_disfluencies?: boolean;
                /**
                 * A list of replacements to apply to the transcript. Each replacement is a pair of strings, where the first string is the pattern to be replaced and the second string is the replacement text.
                 */
                replacements?: Array<{
                    from: string;
                    to: string;
                }>;
            };
            /**
             * Configuration for speaker diarization
             */
            speaker_diarization_config?: {
                /**
                 * If true, the algorithm will prefer to stay with the current active speaker if it is a close enough match, even if other speakers may be closer.  This is useful for cases where we can flip incorrectly between similar speakers during a single speaker section."
                 */
                prefer_current_speaker?: boolean;
                /**
                 * Controls how sensitive the algorithm is in terms of keeping similar speakers separate, as opposed to combining them into a single speaker.  Higher values will typically lead to more speakers, as the degree of difference between speakers in order to allow them to remain distinct will be lower.  A lower value for this parameter will conversely guide the algorithm towards being less sensitive in terms of retaining similar speakers, and as such may lead to fewer speakers overall.  The default is 0.5.
                 */
                speaker_sensitivity?: number;
            };
        };
        tracking?: {
            /**
             * The title of the job.
             */
            title?: string;
            /**
             * External system reference.
             */
            reference?: string;
            tags?: Array<string>;
        };
        output_config?: {
            /**
             * Parameters that override default values of srt conversion. max_line_length: sets maximum count of characters per subtitle line including white space. max_lines: sets maximum count of lines in a subtitle section.
             */
            srt_overrides?: {
                max_line_length?: number;
                max_lines?: number;
            };
        };
        translation_config?: {
            target_languages: Array<string>;
        };
        language_identification_config?: {
            expected_languages?: Array<string>;
            /**
             * Action to take if all of the predicted languages are below the confidence threshold
             */
            low_confidence_action?: 'allow' | 'reject' | 'use_default_language';
            default_language?: string;
        };
        summarization_config?: {
            content_type?: 'auto' | 'informative' | 'conversational';
            summary_length?: 'brief' | 'detailed';
            summary_type?: 'paragraphs' | 'bullets';
        };
        topic_detection_config?: {
            topics?: Array<string>;
        };
        audio_events_config?: {
            types?: Array<string>;
        };
    };
};

export type RecordingMinimalWritable = {
    metadata: {
        [key: string]: string | null;
    } | null;
};

export type TranscriptArtifactWritable = {
    metadata: {
        [key: string]: string | null;
    };
};

export type PatchedTranscriptArtifactWritable = {
    metadata?: {
        [key: string]: string | null;
    };
};

export type AudioMixedArtifactWritable = {
    metadata: {
        [key: string]: string | null;
    };
};

export type PatchedAudioMixedArtifactWritable = {
    metadata?: {
        [key: string]: string | null;
    };
};

export type AudioSeparateArtifactWritable = {
    metadata: {
        [key: string]: string | null;
    };
};

export type PatchedAudioSeparateArtifactWritable = {
    metadata?: {
        [key: string]: string | null;
    };
};

export type VideoMixedArtifactWritable = {
    metadata: {
        [key: string]: string | null;
    };
};

export type PatchedVideoMixedArtifactWritable = {
    metadata?: {
        [key: string]: string | null;
    };
};

export type VideoSeparateArtifactWritable = {
    metadata: {
        [key: string]: string | null;
    };
};

export type PatchedVideoSeparateArtifactWritable = {
    metadata?: {
        [key: string]: string | null;
    };
};

export type BotListData = {
    body?: never;
    path?: never;
    query?: {
        join_at_after?: string;
        join_at_before?: string;
        meeting_url?: string;
        /**
         * A page number within the paginated result set.
         */
        page?: number;
        /**
         * * `zoom` - Zoom
         * * `google_meet` - Meet
         * * `goto_meeting` - Goto
         * * `microsoft_teams` - Teams
         * * `microsoft_teams_live` - Teams Live
         * * `webex` - Webex
         * * `chime_sdk` - Chime Sdk
         * * `zoom_rtms` - Zoom Rtms
         * * `google_meet_media_api` - Google Meet Media Api
         * * `slack_authenticator` - Slack Authenticator
         * * `slack_huddle_observer` - Slack Huddle Observer
         */
        platform?: Array<'chime_sdk' | 'google_meet' | 'google_meet_media_api' | 'goto_meeting' | 'microsoft_teams' | 'microsoft_teams_live' | 'slack_authenticator' | 'slack_huddle_observer' | 'webex' | 'zoom' | 'zoom_rtms'>;
        /**
         * * `ready` - Ready
         * * `joining_call` - Joining Call
         * * `in_waiting_room` - In Waiting Room
         * * `in_call_not_recording` - In Call Not Recording
         * * `recording_permission_allowed` - Recording Permission Allowed
         * * `recording_permission_denied` - Recording Permission Denied
         * * `in_call_recording` - In Call Recording
         * * `recording_done` - Recording Done
         * * `call_ended` - Call Ended
         * * `done` - Done
         * * `fatal` - Fatal
         * * `media_expired` - Media Expired
         * * `analysis_done` - Analysis Done
         * * `analysis_failed` - Analysis Failed
         */
        status?: Array<'analysis_done' | 'analysis_failed' | 'call_ended' | 'done' | 'fatal' | 'in_call_not_recording' | 'in_call_recording' | 'in_waiting_room' | 'joining_call' | 'media_expired' | 'ready' | 'recording_done' | 'recording_permission_allowed' | 'recording_permission_denied'>;
    };
    url: '/api/v1/bot/';
};

export type BotListResponses = {
    200: PaginatedBotList;
};

export type BotListResponse = BotListResponses[keyof BotListResponses];

export type BotCreateData = {
    body: BotWritable;
    path?: never;
    query?: never;
    url: '/api/v1/bot/';
};

export type BotCreateErrors = {
    /**
     * If no bots are available in the ad-hoc bot pool the HTTP 507 code is returned. The ad-hoc bot pool is replenished every few minutes so retrying the request will eventually succeed.
     */
    507: unknown;
};

export type BotCreateResponses = {
    201: Bot;
};

export type BotCreateResponse = BotCreateResponses[keyof BotCreateResponses];

export type BotDestroyData = {
    body?: never;
    path: {
        /**
         * A UUID string identifying this bot.
         */
        id: string;
    };
    query?: never;
    url: '/api/v1/bot/{id}/';
};

export type BotDestroyResponses = {
    /**
     * No response body
     */
    204: void;
};

export type BotDestroyResponse = BotDestroyResponses[keyof BotDestroyResponses];

export type BotRetrieveData = {
    body?: never;
    path: {
        /**
         * A UUID string identifying this bot.
         */
        id: string;
    };
    query?: never;
    url: '/api/v1/bot/{id}/';
};

export type BotRetrieveResponses = {
    200: Bot;
};

export type BotRetrieveResponse = BotRetrieveResponses[keyof BotRetrieveResponses];

export type BotPartialUpdateData = {
    body?: PatchedBotWritable;
    path: {
        /**
         * A UUID string identifying this bot.
         */
        id: string;
    };
    query?: never;
    url: '/api/v1/bot/{id}/';
};

export type BotPartialUpdateResponses = {
    200: Bot;
};

export type BotPartialUpdateResponse = BotPartialUpdateResponses[keyof BotPartialUpdateResponses];

export type BotDeleteMediaCreateData = {
    body?: never;
    path: {
        /**
         * A UUID string identifying this bot.
         */
        id: string;
    };
    query?: never;
    url: '/api/v1/bot/{id}/delete_media/';
};

export type BotDeleteMediaCreateResponses = {
    200: Bot;
};

export type BotDeleteMediaCreateResponse = BotDeleteMediaCreateResponses[keyof BotDeleteMediaCreateResponses];

export type BotLeaveCallCreateData = {
    body?: never;
    path: {
        /**
         * A UUID string identifying this bot.
         */
        id: string;
    };
    query?: never;
    url: '/api/v1/bot/{id}/leave_call/';
};

export type BotLeaveCallCreateResponses = {
    200: Bot;
};

export type BotLeaveCallCreateResponse = BotLeaveCallCreateResponses[keyof BotLeaveCallCreateResponses];

export type CalendarEventsListData = {
    body?: never;
    path?: never;
    query?: {
        calendar_id?: string;
        /**
         * The pagination cursor value.
         */
        cursor?: string;
        /**
         * Filter results by ical_uid. (Case sensitive prefix match will be performed.)
         */
        ical_uid?: string;
        is_deleted?: boolean;
        start_time?: string;
        start_time__gte?: string;
        start_time__lte?: string;
        updated_at__gte?: string;
    };
    url: '/api/v2/calendar-events/';
};

export type CalendarEventsListResponses = {
    200: PaginatedCalendarEventList;
};

export type CalendarEventsListResponse = CalendarEventsListResponses[keyof CalendarEventsListResponses];

export type CalendarEventsRetrieveData = {
    body?: never;
    path: {
        /**
         * A UUID string identifying this calendar event.
         */
        id: string;
    };
    query?: never;
    url: '/api/v2/calendar-events/{id}/';
};

export type CalendarEventsRetrieveResponses = {
    200: CalendarEvent;
};

export type CalendarEventsRetrieveResponse = CalendarEventsRetrieveResponses[keyof CalendarEventsRetrieveResponses];

export type CalendarEventsBotDestroyData = {
    body?: never;
    path: {
        /**
         * A UUID string identifying this calendar event.
         */
        id: string;
    };
    query?: never;
    url: '/api/v2/calendar-events/{id}/bot/';
};

export type CalendarEventsBotDestroyResponses = {
    200: CalendarEvent;
};

export type CalendarEventsBotDestroyResponse = CalendarEventsBotDestroyResponses[keyof CalendarEventsBotDestroyResponses];

export type CalendarEventsBotCreateData = {
    body: CalendarEventAddBot;
    path: {
        /**
         * A UUID string identifying this calendar event.
         */
        id: string;
    };
    query?: never;
    url: '/api/v2/calendar-events/{id}/bot/';
};

export type CalendarEventsBotCreateResponses = {
    200: CalendarEvent;
};

export type CalendarEventsBotCreateResponse = CalendarEventsBotCreateResponses[keyof CalendarEventsBotCreateResponses];

export type CalendarsListData = {
    body?: never;
    path?: never;
    query?: {
        created_at__gte?: string;
        /**
         * The pagination cursor value.
         */
        cursor?: string;
        email?: string;
        /**
         * * `google_calendar` - Google Calendar
         * * `microsoft_outlook` - Microsoft Outlook
         */
        platform?: 'google_calendar' | 'microsoft_outlook';
        /**
         * * `connecting` - Connecting
         * * `connected` - Connected
         * * `disconnected` - Disconnected
         */
        status?: 'connected' | 'connecting' | 'disconnected';
    };
    url: '/api/v2/calendars/';
};

export type CalendarsListResponses = {
    200: PaginatedCalendarList;
};

export type CalendarsListResponse = CalendarsListResponses[keyof CalendarsListResponses];

export type CalendarsCreateData = {
    body: CalendarWritable;
    path?: never;
    query?: never;
    url: '/api/v2/calendars/';
};

export type CalendarsCreateResponses = {
    201: Calendar;
};

export type CalendarsCreateResponse = CalendarsCreateResponses[keyof CalendarsCreateResponses];

export type CalendarsDestroyData = {
    body?: never;
    path: {
        /**
         * A UUID string identifying this calendar.
         */
        id: string;
    };
    query?: never;
    url: '/api/v2/calendars/{id}/';
};

export type CalendarsDestroyResponses = {
    /**
     * No response body
     */
    204: void;
};

export type CalendarsDestroyResponse = CalendarsDestroyResponses[keyof CalendarsDestroyResponses];

export type CalendarsRetrieveData = {
    body?: never;
    path: {
        /**
         * A UUID string identifying this calendar.
         */
        id: string;
    };
    query?: never;
    url: '/api/v2/calendars/{id}/';
};

export type CalendarsRetrieveResponses = {
    200: Calendar;
};

export type CalendarsRetrieveResponse = CalendarsRetrieveResponses[keyof CalendarsRetrieveResponses];

export type CalendarsPartialUpdateData = {
    body?: PatchedCalendarWritable;
    path: {
        /**
         * A UUID string identifying this calendar.
         */
        id: string;
    };
    query?: never;
    url: '/api/v2/calendars/{id}/';
};

export type CalendarsPartialUpdateResponses = {
    200: Calendar;
};

export type CalendarsPartialUpdateResponse = CalendarsPartialUpdateResponses[keyof CalendarsPartialUpdateResponses];

export type CalendarsAccessTokenCreateData = {
    body?: never;
    path: {
        /**
         * A UUID string identifying this calendar.
         */
        id: string;
    };
    query?: never;
    url: '/api/v2/calendars/{id}/access-token/';
};

export type CalendarsAccessTokenCreateErrors = {
    400: CalendarAccountAccessTokenError;
};

export type CalendarsAccessTokenCreateError = CalendarsAccessTokenCreateErrors[keyof CalendarsAccessTokenCreateErrors];

export type CalendarsAccessTokenCreateResponses = {
    200: CalendarAccessToken;
};

export type CalendarsAccessTokenCreateResponse = CalendarsAccessTokenCreateResponses[keyof CalendarsAccessTokenCreateResponses];

export type RecordingListData = {
    body?: never;
    path?: never;
    query?: {
        bot_id?: string;
        created_at_after?: string;
        created_at_before?: string;
        /**
         * The pagination cursor value.
         */
        cursor?: string;
        desktop_sdk_upload_id?: string;
        /**
         * * `processing` - Processing
         * * `paused` - Paused
         * * `done` - Done
         * * `failed` - Failed
         */
        status_code?: 'done' | 'failed' | 'paused' | 'processing';
    };
    url: '/api/v1/recording/';
};

export type RecordingListResponses = {
    200: PaginatedRecordingList;
};

export type RecordingListResponse = RecordingListResponses[keyof RecordingListResponses];

export type RecordingDestroyData = {
    body?: never;
    path: {
        /**
         * A UUID string identifying this recording.
         */
        id: string;
    };
    query?: never;
    url: '/api/v1/recording/{id}/';
};

export type RecordingDestroyResponses = {
    /**
     * No response body
     */
    204: void;
};

export type RecordingDestroyResponse = RecordingDestroyResponses[keyof RecordingDestroyResponses];

export type RecordingRetrieveData = {
    body?: never;
    path: {
        /**
         * A UUID string identifying this recording.
         */
        id: string;
    };
    query?: never;
    url: '/api/v1/recording/{id}/';
};

export type RecordingRetrieveResponses = {
    200: Recording;
};

export type RecordingRetrieveResponse = RecordingRetrieveResponses[keyof RecordingRetrieveResponses];

export type RecordingCreateTranscriptCreateData = {
    body: RecordingCreateTranscriptArtifactWritable;
    path: {
        /**
         * A UUID string identifying this recording.
         */
        id: string;
    };
    query?: never;
    url: '/api/v1/recording/{id}/create_transcript/';
};

export type RecordingCreateTranscriptCreateResponses = {
    200: TranscriptArtifact;
};

export type RecordingCreateTranscriptCreateResponse = RecordingCreateTranscriptCreateResponses[keyof RecordingCreateTranscriptCreateResponses];

export type TranscriptListData = {
    body?: never;
    path?: never;
    query?: {
        created_at_after?: string;
        created_at_before?: string;
        /**
         * The pagination cursor value.
         */
        cursor?: string;
        recording_id?: string;
        /**
         * * `processing` - Processing
         * * `done` - Done
         * * `failed` - Failed
         */
        status_code?: 'done' | 'failed' | 'processing';
    };
    url: '/api/v1/transcript/';
};

export type TranscriptListResponses = {
    200: PaginatedTranscriptArtifactList;
};

export type TranscriptListResponse = TranscriptListResponses[keyof TranscriptListResponses];

export type TranscriptDestroyData = {
    body?: never;
    path: {
        /**
         * A UUID string identifying this transcript artifact.
         */
        id: string;
    };
    query?: never;
    url: '/api/v1/transcript/{id}/';
};

export type TranscriptDestroyResponses = {
    /**
     * No response body
     */
    204: void;
};

export type TranscriptDestroyResponse = TranscriptDestroyResponses[keyof TranscriptDestroyResponses];

export type TranscriptRetrieveData = {
    body?: never;
    path: {
        /**
         * A UUID string identifying this transcript artifact.
         */
        id: string;
    };
    query?: never;
    url: '/api/v1/transcript/{id}/';
};

export type TranscriptRetrieveResponses = {
    200: TranscriptArtifact;
};

export type TranscriptRetrieveResponse = TranscriptRetrieveResponses[keyof TranscriptRetrieveResponses];

export type TranscriptPartialUpdateData = {
    body?: PatchedTranscriptArtifactWritable;
    path: {
        /**
         * A UUID string identifying this transcript artifact.
         */
        id: string;
    };
    query?: never;
    url: '/api/v1/transcript/{id}/';
};

export type TranscriptPartialUpdateResponses = {
    200: TranscriptArtifact;
};

export type TranscriptPartialUpdateResponse = TranscriptPartialUpdateResponses[keyof TranscriptPartialUpdateResponses];

export type AudioMixedListData = {
    body?: never;
    path?: never;
    query?: {
        created_at_after?: string;
        created_at_before?: string;
        /**
         * The pagination cursor value.
         */
        cursor?: string;
        recording_id?: string;
        /**
         * * `processing` - Processing
         * * `done` - Done
         * * `failed` - Failed
         */
        status_code?: 'done' | 'failed' | 'processing';
    };
    url: '/api/v1/audio_mixed/';
};

export type AudioMixedListResponses = {
    200: PaginatedAudioMixedArtifactList;
};

export type AudioMixedListResponse = AudioMixedListResponses[keyof AudioMixedListResponses];

export type AudioMixedDestroyData = {
    body?: never;
    path: {
        /**
         * A UUID string identifying this audio mixed artifact.
         */
        id: string;
    };
    query?: never;
    url: '/api/v1/audio_mixed/{id}/';
};

export type AudioMixedDestroyResponses = {
    /**
     * No response body
     */
    204: void;
};

export type AudioMixedDestroyResponse = AudioMixedDestroyResponses[keyof AudioMixedDestroyResponses];

export type AudioMixedRetrieveData = {
    body?: never;
    path: {
        /**
         * A UUID string identifying this audio mixed artifact.
         */
        id: string;
    };
    query?: never;
    url: '/api/v1/audio_mixed/{id}/';
};

export type AudioMixedRetrieveResponses = {
    200: AudioMixedArtifact;
};

export type AudioMixedRetrieveResponse = AudioMixedRetrieveResponses[keyof AudioMixedRetrieveResponses];

export type AudioMixedPartialUpdateData = {
    body?: PatchedAudioMixedArtifactWritable;
    path: {
        /**
         * A UUID string identifying this audio mixed artifact.
         */
        id: string;
    };
    query?: never;
    url: '/api/v1/audio_mixed/{id}/';
};

export type AudioMixedPartialUpdateResponses = {
    200: AudioMixedArtifact;
};

export type AudioMixedPartialUpdateResponse = AudioMixedPartialUpdateResponses[keyof AudioMixedPartialUpdateResponses];

export type AudioSeparateListData = {
    body?: never;
    path?: never;
    query?: {
        created_at_after?: string;
        created_at_before?: string;
        /**
         * The pagination cursor value.
         */
        cursor?: string;
        recording_id?: string;
        /**
         * * `processing` - Processing
         * * `done` - Done
         * * `failed` - Failed
         */
        status_code?: 'done' | 'failed' | 'processing';
    };
    url: '/api/v1/audio_separate/';
};

export type AudioSeparateListResponses = {
    200: PaginatedAudioSeparateArtifactList;
};

export type AudioSeparateListResponse = AudioSeparateListResponses[keyof AudioSeparateListResponses];

export type AudioSeparateDestroyData = {
    body?: never;
    path: {
        /**
         * A UUID string identifying this audio separate artifact.
         */
        id: string;
    };
    query?: never;
    url: '/api/v1/audio_separate/{id}/';
};

export type AudioSeparateDestroyResponses = {
    /**
     * No response body
     */
    204: void;
};

export type AudioSeparateDestroyResponse = AudioSeparateDestroyResponses[keyof AudioSeparateDestroyResponses];

export type AudioSeparateRetrieveData = {
    body?: never;
    path: {
        /**
         * A UUID string identifying this audio separate artifact.
         */
        id: string;
    };
    query?: never;
    url: '/api/v1/audio_separate/{id}/';
};

export type AudioSeparateRetrieveResponses = {
    200: AudioSeparateArtifact;
};

export type AudioSeparateRetrieveResponse = AudioSeparateRetrieveResponses[keyof AudioSeparateRetrieveResponses];

export type AudioSeparatePartialUpdateData = {
    body?: PatchedAudioSeparateArtifactWritable;
    path: {
        /**
         * A UUID string identifying this audio separate artifact.
         */
        id: string;
    };
    query?: never;
    url: '/api/v1/audio_separate/{id}/';
};

export type AudioSeparatePartialUpdateResponses = {
    200: AudioSeparateArtifact;
};

export type AudioSeparatePartialUpdateResponse = AudioSeparatePartialUpdateResponses[keyof AudioSeparatePartialUpdateResponses];

export type VideoMixedListData = {
    body?: never;
    path?: never;
    query?: {
        created_at_after?: string;
        created_at_before?: string;
        /**
         * The pagination cursor value.
         */
        cursor?: string;
        recording_id?: string;
        /**
         * * `processing` - Processing
         * * `done` - Done
         * * `failed` - Failed
         */
        status_code?: 'done' | 'failed' | 'processing';
    };
    url: '/api/v1/video_mixed/';
};

export type VideoMixedListResponses = {
    200: PaginatedVideoMixedArtifactList;
};

export type VideoMixedListResponse = VideoMixedListResponses[keyof VideoMixedListResponses];

export type VideoMixedDestroyData = {
    body?: never;
    path: {
        /**
         * A UUID string identifying this video mixed artifact.
         */
        id: string;
    };
    query?: never;
    url: '/api/v1/video_mixed/{id}/';
};

export type VideoMixedDestroyResponses = {
    /**
     * No response body
     */
    204: void;
};

export type VideoMixedDestroyResponse = VideoMixedDestroyResponses[keyof VideoMixedDestroyResponses];

export type VideoMixedRetrieveData = {
    body?: never;
    path: {
        /**
         * A UUID string identifying this video mixed artifact.
         */
        id: string;
    };
    query?: never;
    url: '/api/v1/video_mixed/{id}/';
};

export type VideoMixedRetrieveResponses = {
    200: VideoMixedArtifact;
};

export type VideoMixedRetrieveResponse = VideoMixedRetrieveResponses[keyof VideoMixedRetrieveResponses];

export type VideoMixedPartialUpdateData = {
    body?: PatchedVideoMixedArtifactWritable;
    path: {
        /**
         * A UUID string identifying this video mixed artifact.
         */
        id: string;
    };
    query?: never;
    url: '/api/v1/video_mixed/{id}/';
};

export type VideoMixedPartialUpdateResponses = {
    200: VideoMixedArtifact;
};

export type VideoMixedPartialUpdateResponse = VideoMixedPartialUpdateResponses[keyof VideoMixedPartialUpdateResponses];

export type VideoSeparateListData = {
    body?: never;
    path?: never;
    query?: {
        created_at_after?: string;
        created_at_before?: string;
        /**
         * The pagination cursor value.
         */
        cursor?: string;
        recording_id?: string;
        /**
         * * `processing` - Processing
         * * `done` - Done
         * * `failed` - Failed
         */
        status_code?: 'done' | 'failed' | 'processing';
    };
    url: '/api/v1/video_separate/';
};

export type VideoSeparateListResponses = {
    200: PaginatedVideoSeparateArtifactList;
};

export type VideoSeparateListResponse = VideoSeparateListResponses[keyof VideoSeparateListResponses];

export type VideoSeparateDestroyData = {
    body?: never;
    path: {
        /**
         * A UUID string identifying this video separate artifact.
         */
        id: string;
    };
    query?: never;
    url: '/api/v1/video_separate/{id}/';
};

export type VideoSeparateDestroyResponses = {
    /**
     * No response body
     */
    204: void;
};

export type VideoSeparateDestroyResponse = VideoSeparateDestroyResponses[keyof VideoSeparateDestroyResponses];

export type VideoSeparateRetrieveData = {
    body?: never;
    path: {
        /**
         * A UUID string identifying this video separate artifact.
         */
        id: string;
    };
    query?: never;
    url: '/api/v1/video_separate/{id}/';
};

export type VideoSeparateRetrieveResponses = {
    200: VideoSeparateArtifact;
};

export type VideoSeparateRetrieveResponse = VideoSeparateRetrieveResponses[keyof VideoSeparateRetrieveResponses];

export type VideoSeparatePartialUpdateData = {
    body?: PatchedVideoSeparateArtifactWritable;
    path: {
        /**
         * A UUID string identifying this video separate artifact.
         */
        id: string;
    };
    query?: never;
    url: '/api/v1/video_separate/{id}/';
};

export type VideoSeparatePartialUpdateResponses = {
    200: VideoSeparateArtifact;
};

export type VideoSeparatePartialUpdateResponse = VideoSeparatePartialUpdateResponses[keyof VideoSeparatePartialUpdateResponses];
